{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT_embedding",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/luguzman/NLP/blob/main/BERT_embedding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1tadN4hSCP9p"
      },
      "source": [
        "# Fase 1: Importar las dependencias"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EUU4TlmoFMZ_"
      },
      "source": [
        "import numpy as np\n",
        "import math\n",
        "import re\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "import random\n",
        "\n",
        "from google.colab import drive"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lXj8lk3uGn4P",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 512
        },
        "outputId": "be8cecd8-9913-4f24-d479-11a5b140cb52"
      },
      "source": [
        "!pip install bert-for-tf2\n",
        "!pip install sentencepiece"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting bert-for-tf2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/87/df/ab6d927d6162657f30eb0ae3c534c723c28c191a9caf6ee68ec935df3d0b/bert-for-tf2-0.14.5.tar.gz (40kB)\n",
            "\r\u001b[K     |████████                        | 10kB 23.4MB/s eta 0:00:01\r\u001b[K     |████████████████                | 20kB 1.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 30kB 2.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 40kB 1.9MB/s \n",
            "\u001b[?25hCollecting py-params>=0.9.6\n",
            "  Downloading https://files.pythonhosted.org/packages/a4/bf/c1c70d5315a8677310ea10a41cfc41c5970d9b37c31f9c90d4ab98021fd1/py-params-0.9.7.tar.gz\n",
            "Collecting params-flow>=0.8.0\n",
            "  Downloading https://files.pythonhosted.org/packages/a9/95/ff49f5ebd501f142a6f0aaf42bcfd1c192dc54909d1d9eb84ab031d46056/params-flow-0.8.2.tar.gz\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from params-flow>=0.8.0->bert-for-tf2) (1.18.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from params-flow>=0.8.0->bert-for-tf2) (4.41.1)\n",
            "Building wheels for collected packages: bert-for-tf2, py-params, params-flow\n",
            "  Building wheel for bert-for-tf2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for bert-for-tf2: filename=bert_for_tf2-0.14.5-cp36-none-any.whl size=30315 sha256=721d23a8101be74893888b6b19fe57c1bfa557daa4a7700cf07f40b6d7f3a35a\n",
            "  Stored in directory: /root/.cache/pip/wheels/2e/70/a2/be357037dd2cbdcaeb0add1fdf083be6a600ca65ee1f68751c\n",
            "  Building wheel for py-params (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for py-params: filename=py_params-0.9.7-cp36-none-any.whl size=7302 sha256=c7ff307cc9ec3f96ce916c2bd3aa68ca80ab8d8687c9c7a863a5aeadf30cec89\n",
            "  Stored in directory: /root/.cache/pip/wheels/67/f5/19/b461849a50aefdf4bab47c4756596e82ee2118b8278e5a1980\n",
            "  Building wheel for params-flow (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for params-flow: filename=params_flow-0.8.2-cp36-none-any.whl size=19473 sha256=5344cf54ef193276d15af066b214cb9f07d962779386602f6bac402291943b14\n",
            "  Stored in directory: /root/.cache/pip/wheels/08/c8/7f/81c86b9ff2b86e2c477e3914175be03e679e596067dc630c06\n",
            "Successfully built bert-for-tf2 py-params params-flow\n",
            "Installing collected packages: py-params, params-flow, bert-for-tf2\n",
            "Successfully installed bert-for-tf2-0.14.5 params-flow-0.8.2 py-params-0.9.7\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 2.8MB/s \n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.91\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fOfuPdFHFpfC"
      },
      "source": [
        "try:\n",
        "    %tensorflow_version 2.x\n",
        "except Exception:\n",
        "    pass\n",
        "import tensorflow as tf\n",
        "\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "from tensorflow.keras import layers\n",
        "import bert"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j6ZbE2lPDIFL"
      },
      "source": [
        "# Fase 2: Pre Procesado de Datos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T9S77lewDNE1"
      },
      "source": [
        "## Carga de los ficheros"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J7GET0xsDSDc"
      },
      "source": [
        "Cargamos los ficheros de nuestro Google Drive personal"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5hABc0h8GdTe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "outputId": "bda03036-2c2b-4aaa-e207-be77b25f2b6a"
      },
      "source": [
        "drive.mount(\"/content/drive\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "slnILsqwGxTX"
      },
      "source": [
        "cols = [\"sentiment\", \"id\", \"date\", \"query\", \"user\", \"text\"]\n",
        "data = pd.read_csv(\n",
        "    \"/content/drive/My Drive/Curso de NLP/BERT/sentiment_data/training.csv\",\n",
        "    header=None,\n",
        "    names=cols,\n",
        "    engine=\"python\",\n",
        "    encoding=\"latin1\"\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "REdK4z4YG9kZ"
      },
      "source": [
        "data.drop([\"id\", \"date\", \"query\", \"user\"],\n",
        "          axis=1,\n",
        "          inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lz2g61evDZb4"
      },
      "source": [
        "## Pre Procesado"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LCyy4babDrI8"
      },
      "source": [
        "### Limpieza"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UEyorQS_HArn"
      },
      "source": [
        "def clean_tweet(tweet):\n",
        "    tweet = BeautifulSoup(tweet, \"lxml\").get_text()\n",
        "    # Removing the @\n",
        "    tweet = re.sub(r\"@[A-Za-z0-9]+\", ' ', tweet)\n",
        "    # Removing the URL links\n",
        "    tweet = re.sub(r\"https?://[A-Za-z0-9./]+\", ' ', tweet)\n",
        "    # Keeping only letters\n",
        "    tweet = re.sub(r\"[^a-zA-Z.!?']\", ' ', tweet)\n",
        "    # Removing additional whitespaces\n",
        "    tweet = re.sub(r\" +\", ' ', tweet)\n",
        "    return tweet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3BlbZpy0HHiV"
      },
      "source": [
        "data_clean = [clean_tweet(tweet) for tweet in data.text]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H6SOj46BHKEk"
      },
      "source": [
        "data_labels = data.sentiment.values\n",
        "data_labels[data_labels == 4] = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MJa3YWeJD1gM"
      },
      "source": [
        "### Tokenización"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MUaCPqqBD7kQ"
      },
      "source": [
        "Necesitamos crear una capa BERT para tener acceso a los metadatos del tokenizador (como el tamaño del vocabulario)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0wry-st-HMN0"
      },
      "source": [
        "FullTokenizer = bert.bert_tokenization.FullTokenizer # Instacia que nos permitirá convertir los textos a token\n",
        "# Una vez teniendo este tokenizador necesitamos info adicional de este como cual es el tamaño del vocabulario,\n",
        "# convertir todo a minúsculas, etc y por lo tanto aplicaremos la siguiente capa:\n",
        "bert_layer = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\",   \n",
        "                            trainable=False)\n",
        "# en_uncased: english en minúscula\n",
        "# L-12: Es la versión sencilla de BERT, para que sea rápido de entrenar vs el L-24\n",
        "# trainable: Se utiliza para indicar si haremos find tuning, si hay que entrenar algo adicional de los propios pesos\n",
        "\n",
        "# Extraemos el tamaño del vocabulario(file temporal que genera el hub) a partir del tokenizer. \n",
        "# Es más que el tamaño es el diccionario completo.\n",
        "vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
        "# Del mismo modo necesitamos la info de la conversión a minúsculas por parte de BERT.\n",
        "do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
        "tokenizer = FullTokenizer(vocab_file, do_lower_case)\n",
        "# Esto es normal que tarde pues con esta versión baja la arquitectura junto con los 110 M de pesos"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HMVarTJpELyK"
      },
      "source": [
        "Solo usamos la primera oración para las entradas BERT, por lo que agregamos el token CLS al principio y el token SEP al final de cada oración."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v-JkZt9NduoC"
      },
      "source": [
        "# Ahora aparecerán dos token uno \"CLS\" al inicio que se utiliza para problemas de clasificación\n",
        "# y un token \"SEP\" token de separación entre las frases . Por supuesto, en nuestro caso solo vamos a \n",
        "# tener una frase  porque estamos llevando a cabo una tarea de clasificación. Por lo que en este caso solo\n",
        "# rendremos que añadir el token de sepración al final y nada más.\n",
        "def encode_sentence(sent):\n",
        "    return [\"[CLS]\"] + tokenizer.tokenize(sent) + [\"[SEP]\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pel_Uk6Ic4xB"
      },
      "source": [
        "data_inputs = [encode_sentence(sentence) for sentence in data_clean]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z32MeEwnkCB8"
      },
      "source": [
        "### Creación del dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cUVc83VNEcW9"
      },
      "source": [
        "Necesitamos crear las 3 entradas diferentes para cada oración."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wmW9JZLJaxww"
      },
      "source": [
        "# Una vez tokenizada la frase necesitamos crear 3 tokens para las 3 entradas\n",
        "\n",
        "# La siguiente función a partir de una lista de palabras tokenizadas con el cls + el separador el \n",
        "# tokenizer se encarga de convertir los tokens a id's.\n",
        "def get_ids(tokens):\n",
        "    return tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "# La siguiente función aplicará la máscara correspondiente a los tokens de padding. Basicamente lo que \n",
        "# hará es buscar dentro de la lista de tokens la existencia del token 'PAD'. Cuando nosotros nos encontremos \n",
        "# con ese token 'PAD' significa que ese es un elemento que no nos interesa. Cunado el token en cuestión\n",
        "# en una posición no sea el de PAD nos devolverá un 0 y 1's donde no.\n",
        "def get_mask(tokens):\n",
        "    return np.char.not_equal(tokens, \"[PAD]\").astype(int)\n",
        "\n",
        "# La siguiente función indicará si un token pertenece a la primera frase o a la segunda frase. Esto lo \n",
        "# haremos gracias a ubicar el token de separación 'SEP'. Entonces, hasta encontrar el token SEP utilizaremos\n",
        "# 0´s para indicar que el segmento de frase pertence al primer trozo y posteriormente colocaremos 1´s\n",
        "def get_segments(tokens):\n",
        "    seg_ids = []\n",
        "    current_seg_id = 0\n",
        "    for tok in tokens:\n",
        "        seg_ids.append(current_seg_id)\n",
        "        if tok == \"[SEP]\":\n",
        "            current_seg_id = 1-current_seg_id # convierte los 1 en 0 y vice versa\n",
        "    return seg_ids"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x06fFPFtFqVK"
      },
      "source": [
        "Crearemos padded batches (por lo que rellenamos las frases para cada lote de forma independiente), de esta forma añadimos el mínimo número de tokens de padding posible. Para eso, ordenamos las frases por longitud, aplicamos padded_batches y luego las mezclamos."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hjAVGCwlb6F8"
      },
      "source": [
        "data_with_len = [[sent, data_labels[i], len(sent)]\n",
        "                 for i, sent in enumerate(data_inputs)]\n",
        "random.shuffle(data_with_len)\n",
        "data_with_len.sort(key=lambda x: x[2])\n",
        "# Hasta ahora tomabamos como una entrada una secuencia de tokens numéricos. Ahora necesitamos 3\n",
        "# secuencias de tokens. \n",
        "sorted_all = [([get_ids(sent_lab[0]),   # idetificadores para la frase\n",
        "                get_mask(sent_lab[0]),  # máscara para la isma frase \n",
        "                get_segments(sent_lab[0])], # segmentos para la propia frase\n",
        "               sent_lab[1])         # etiqueta para esta frase que acabos de tripiclar su tamaño\n",
        "              for sent_lab in data_with_len if sent_lab[2] > 7] # Unicamente si la frase tiene más de 7 palabras"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZkMiqmzsfo6a"
      },
      "source": [
        "# Una lista es un tipo de iterador de modo que se puede usar como generador de un dataset.\n",
        "\n",
        "# En este punto nuestras aun no tiene todas la misma longitud por lo que utilizaremos un generator,\n",
        "# para que se encargue de arreglarlas. Un generedor como funciona es simplemente le das un elemnto \n",
        "# y te duevuelve otro. \n",
        "all_dataset = tf.data.Dataset.from_generator(lambda: sorted_all,\n",
        "                                             output_types=(tf.int32, tf.int32)) # Tipo de dato de la salida (enteros en este caso)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gkGWlzeOfos6"
      },
      "source": [
        "# Ahora haremos el proceso de Padding pero recordemos que será por bloques así reduciremos el entrenamiento\n",
        "BATCH_SIZE = 32\n",
        "all_batched = all_dataset.padded_batch(BATCH_SIZE,\n",
        "                                       padded_shapes=((3, None), ()),\n",
        "                                       padding_values=(0, 0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5aA7it--hHl4"
      },
      "source": [
        "# Generamos nustro conjunto de entrenamiento y testing por lotes.\n",
        "\n",
        "# Declaramos el número total de lotes que va a haber.\n",
        "NB_BATCHES = math.ceil(len(sorted_all) / BATCH_SIZE)\n",
        "# Declaramos el número total de lotes de test que va a haber.\n",
        "NB_BATCHES_TEST = NB_BATCHES // 10  # Nos quedamos con el 10%\n",
        "# Solo recordemos que los lotes los tenemos ordenados por lo que si nos tomamos el 90% priemro para \n",
        "# entrenar y el 10% restante para testing me van a quedar el 10% más largo. Por lo que volveremos a \n",
        "# mezclar cada uno de los lotes.\n",
        "all_batched.shuffle(NB_BATCHES)\n",
        "test_dataset = all_batched.take(NB_BATCHES_TEST)\n",
        "train_dataset = all_batched.skip(NB_BATCHES_TEST)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X4QCPok7aEM_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e249c531-96f7-475e-e156-5c2189cecbc3"
      },
      "source": [
        "next(iter(train_dataset))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<tf.Tensor: shape=(32, 3, 10), dtype=int32, numpy=\n",
              " array([[[  101, 27427,  2229, 23773,  2003,  2025,  2652,  3835,  2651,\n",
              "            102],\n",
              "         [    1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "              1],\n",
              "         [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "              0]],\n",
              " \n",
              "        [[  101,  1005,  1055,  4439,  9450,  2003,  5881,  1999,  4026,\n",
              "            102],\n",
              "         [    1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "              1],\n",
              "         [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "              0]],\n",
              " \n",
              "        [[  101,  6289, 23644,  2085,  1045,  2131,  2009,   999,   999,\n",
              "            102],\n",
              "         [    1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "              1],\n",
              "         [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "              0]],\n",
              " \n",
              "        [[  101, 24471,  2290,   999,  1045,  3335,  4757,  2017,  1012,\n",
              "            102],\n",
              "         [    1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "              1],\n",
              "         [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "              0]],\n",
              " \n",
              "        [[  101,  7929,   999,  1045,  2074,  2741,  2017,  2070,   999,\n",
              "            102],\n",
              "         [    1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "              1],\n",
              "         [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "              0]],\n",
              " \n",
              "        [[  101, 10651,  2134,  1005,  1056,  2421,  2678,  3405,  9808,\n",
              "            102],\n",
              "         [    1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "              1],\n",
              "         [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "              0]],\n",
              " \n",
              "        [[  101,  2183,  2000,  2793,  1012,  2009,  1005,  1055,  3147,\n",
              "            102],\n",
              "         [    1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "              1],\n",
              "         [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "              0]],\n",
              " \n",
              "        [[  101,  2017,  2024,  1037, 10392,  1998, 17160,  2711,  1012,\n",
              "            102],\n",
              "         [    1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "              1],\n",
              "         [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "              0]],\n",
              " \n",
              "        [[  101,  2821,  2053,  1045, 18358,  2039,  1996,  8785,  2015,\n",
              "            102],\n",
              "         [    1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "              1],\n",
              "         [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "              0]],\n",
              " \n",
              "        [[  101,  2215,  2000,  2031,  3256,  6949,  1012,  1012,  1012,\n",
              "            102],\n",
              "         [    1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "              1],\n",
              "         [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "              0]],\n",
              " \n",
              "        [[  101,  6732,  2008,  2023,  2086,  3694, 14108,  2003, 11771,\n",
              "            102],\n",
              "         [    1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "              1],\n",
              "         [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "              0]],\n",
              " \n",
              "        [[  101,  3564,  1999,  1996,  2282,  1012,  3793,  2075,  4202,\n",
              "            102],\n",
              "         [    1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "              1],\n",
              "         [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "              0]],\n",
              " \n",
              "        [[  101,  1047, 21638,  1999, 15849,  2073,  2003,  5034,  2243,\n",
              "            102],\n",
              "         [    1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "              1],\n",
              "         [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "              0]],\n",
              " \n",
              "        [[  101,  9267,  1051, 10085,  3374,  2000,  2963,  2008,  1012,\n",
              "            102],\n",
              "         [    1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "              1],\n",
              "         [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "              0]],\n",
              " \n",
              "        [[  101, 21534,  2035,  2026,  9781,  2015,  1999,  3888,  2237,\n",
              "            102],\n",
              "         [    1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "              1],\n",
              "         [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "              0]],\n",
              " \n",
              "        [[  101,  2589,  2007, 19453,  1012,  1012,  1012,  2005,  2085,\n",
              "            102],\n",
              "         [    1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "              1],\n",
              "         [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "              0]],\n",
              " \n",
              "        [[  101,  2009,  1005,  1055,  2055,  1996, 12476,  2791,  1012,\n",
              "            102],\n",
              "         [    1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "              1],\n",
              "         [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "              0]],\n",
              " \n",
              "        [[  101,  8840,  2140,  4283,  2025,  2919,  2005,  2055,  2420,\n",
              "            102],\n",
              "         [    1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "              1],\n",
              "         [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "              0]],\n",
              " \n",
              "        [[  101,  2814,  2256,  4715,  3790,  2008,  2437,  2033,  6114,\n",
              "            102],\n",
              "         [    1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "              1],\n",
              "         [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "              0]],\n",
              " \n",
              "        [[  101, 25636,  2024,  2091,  2028,  2062,  2558,  2000,  2175,\n",
              "            102],\n",
              "         [    1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "              1],\n",
              "         [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "              0]],\n",
              " \n",
              "        [[  101,  1045,  2245,  2017,  1005,  1040,  2066,  2009,   999,\n",
              "            102],\n",
              "         [    1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "              1],\n",
              "         [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "              0]],\n",
              " \n",
              "        [[  101,  3110,  2204,  4142,  1998,  9499,  2005,  1037,  2689,\n",
              "            102],\n",
              "         [    1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "              1],\n",
              "         [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "              0]],\n",
              " \n",
              "        [[  101,  8477,  4283,  2017,  1005,  2128,  2006,  1996,  2862,\n",
              "            102],\n",
              "         [    1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "              1],\n",
              "         [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "              0]],\n",
              " \n",
              "        [[  101,  2017,  2113,  1045,  2052,  2065,  1045,  2071, 11561,\n",
              "            102],\n",
              "         [    1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "              1],\n",
              "         [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "              0]],\n",
              " \n",
              "        [[  101, 13072,  3374,  2000,  2963,  2008,  2129,  2272,  1029,\n",
              "            102],\n",
              "         [    1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "              1],\n",
              "         [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "              0]],\n",
              " \n",
              "        [[  101,  1045,  2074, 13017,  2026,  2373,  2604, 11829,  4590,\n",
              "            102],\n",
              "         [    1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "              1],\n",
              "         [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "              0]],\n",
              " \n",
              "        [[  101,  2183,  2000, 13875,  1005,  1055,  2651, 28939,  2546,\n",
              "            102],\n",
              "         [    1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "              1],\n",
              "         [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "              0]],\n",
              " \n",
              "        [[  101,  1045,  1049,  1999,  2293,  2007,  4456,  7366,   999,\n",
              "            102],\n",
              "         [    1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "              1],\n",
              "         [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "              0]],\n",
              " \n",
              "        [[  101,  2633,  1045,  2514,  4012, 12031,  2007,  2026, 10474,\n",
              "            102],\n",
              "         [    1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "              1],\n",
              "         [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "              0]],\n",
              " \n",
              "        [[  101,  1996, 20920, 27498,  4615, 27498,  1998,  4615, 23863,\n",
              "            102],\n",
              "         [    1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "              1],\n",
              "         [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "              0]],\n",
              " \n",
              "        [[  101, 10094,  2000,  2377, 19456,  1997,  2162,  2006,  5282,\n",
              "            102],\n",
              "         [    1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "              1],\n",
              "         [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "              0]],\n",
              " \n",
              "        [[  101,  4067,  2017, 13008,  5076,  2005,  1996, 10651,   999,\n",
              "            102],\n",
              "         [    1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "              1],\n",
              "         [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "              0]]], dtype=int32)>, <tf.Tensor: shape=(32,), dtype=int32, numpy=\n",
              " array([0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1,\n",
              "        1, 0, 0, 0, 1, 1, 1, 1, 0, 1], dtype=int32)>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cbR6oHocxZvh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e7feb8a-f8f6-449a-a9ab-dd93abc1a93e"
      },
      "source": [
        "# Ejemplo\n",
        "my_sent = [\"[CLS]\"] + tokenizer.tokenize(\"Roses are red.\") + [\"[SEP]\"]\n",
        "print(my_sent)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-352e4be88122>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmy_sent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"[CLS]\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Roses are red.\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"[SEP]\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmy_sent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'tokenizer' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m9uVFx3Sxzva",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f158390-3146-43e4-d993-a4cd46c863aa"
      },
      "source": [
        "# Ejemplo de lo que le pasaríamos a la capa de BERT\n",
        "# necesitamos que se un tensor por lo que expandimos la 1°,2° Y 3° capa de BERT\n",
        "bert_layer([tf.expand_dims(tf.cast(get_ids(my_sent), tf.int32), 0), \n",
        "            tf.expand_dims(tf.cast(get_mask(my_sent), tf.int32), 0),\n",
        "            tf.expand_dims(tf.cast(get_segments(my_sent), tf.int32), 0)])\n",
        "\n",
        "# NOTA: cuando necesitemos utilizar la representación vectorial de una frase, usaremos el primer tensor\n",
        "# y cuando necesitemos la representación vectorial de cada una de las palabras de la frase utilizaremos\n",
        "# el segundo tensor."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-5c4100abbaf2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m bert_layer([tf.expand_dims(tf.cast(get_ids(my_sent), tf.int32), 0),\n\u001b[0m\u001b[1;32m      2\u001b[0m             \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmy_sent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m             tf.expand_dims(tf.cast(get_segments(my_sent), tf.int32), 0)])\n",
            "\u001b[0;31mNameError\u001b[0m: name 'bert_layer' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N2pxAPFxGe8r"
      },
      "source": [
        "# Fase 3: Construcción del modelo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L6DD3k3qPLDQ"
      },
      "source": [
        "class DCNNBERTEmbedding(tf.keras.Model):\n",
        "    \n",
        "    # En este caso la capa de BERT ya sabe cual es la dimensión del embedding dependiendo de cual llamemos y\n",
        "    # también sabe el tamaño del vocabulario porque utiliza su propio tokenizador por lo que ya no usaremos\n",
        "    # vocab_size ni emb_dim.\n",
        "    def __init__(self,\n",
        "                 nb_filters=50, # Número de filtros que le aplicaremos a RNC por cada vector de características\n",
        "                 FFN_units=512, # Número de neuronas en la capa oculta. En este caso la penultima capa densa\n",
        "                 nb_classes=2,  # Ya que es un proceso de clasificación binario\n",
        "                 dropout_rate=0.1,  # Apaga el 10% de las neuronas de manera aleatoria en cada epoch para evitar el overfitting\n",
        "                 name=\"dcnn\"):\n",
        "        # Para poder utlizar todas las varibales de tk.keras.Model lo primero y más importante tenemos que \n",
        "        # llamar a la super clase, ya que como hemos hecho que el método herede de tf.keras.Model. estamos \n",
        "        # obligados a llamar a :\n",
        "        super(DCNNBERTEmbedding, self).__init__(name=name)\n",
        "        \n",
        "        # Ahora la 1° capa de embeddings ya no es la de tensorflow, sino que en este caso utilizamos al\n",
        "        # muy parecido a lo que utilizamos con la capa de BERT para el tokenizador. Usamos nuevamente\n",
        "        # hub.KerasLayer y vovlemos a utilzar el modelos simple de 12 capas incrustadas en un espacio de \n",
        "        # dimensión 768\n",
        "        self.bert_layer = hub.KerasLayer(\n",
        "            \"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\",\n",
        "            trainable=False)\n",
        "        # trainable = False: Porque en este caso quiero utilizar el embedding ya entrenado por Google y\n",
        "        # no quiero hacer find tuning.\n",
        "\n",
        "        self.bigram = layers.Conv1D(filters=nb_filters,\n",
        "                                    kernel_size=2,      # analizamos las palabras cuando aparecen juntas de dos en dos, ie, filtrará palabras de 2 en 2.\n",
        "                                    padding=\"valid\",    # aqui no toma mucha importancia, lo que hará es añadir ceros en el caso de las primeras y ultimas convoluciones en las que ya no haya elementos. Pero como no hemos puesto el parametro de stride es = 1 por lo que no se podría salir\n",
        "                                    activation=\"relu\")  # trabaja muy bien en este tipo de análisis. La aplicamos para romper la linealidad y que busque más allá de valores lineales.  \n",
        "        self.trigram = layers.Conv1D(filters=nb_filters,\n",
        "                                     kernel_size=3,\n",
        "                                     padding=\"valid\",\n",
        "                                     activation=\"relu\")\n",
        "        self.fourgram = layers.Conv1D(filters=nb_filters,\n",
        "                                      kernel_size=4,\n",
        "                                      padding=\"valid\",\n",
        "                                      activation=\"relu\")\n",
        "        # Creamos una capa la cual se encarga de solo quedarse con el máximo de todo estos valores. El \n",
        "        # máximo de cada bi-tri-cuatrigrama\n",
        "        self.pool = layers.GlobalMaxPool1D()\n",
        "\n",
        "        # Pasamos estos maximos a la primera capa densa\n",
        "        self.dense_1 = layers.Dense(units=FFN_units, activation=\"relu\")\n",
        "        self.dropout = layers.Dropout(rate=dropout_rate)\n",
        "        if nb_classes == 2:\n",
        "            self.last_dense = layers.Dense(units=1,\n",
        "                                           activation=\"sigmoid\")\n",
        "        else:\n",
        "            self.last_dense = layers.Dense(units=nb_classes,\n",
        "                                           activation=\"softmax\")\n",
        "    # En la sig función recibe el token de entrada, recordemos que vamos a usar bloques ded tokens y por\n",
        "    # tanto la capa bert_layer que declaramos en el __init__ la aplicamos aqui y le damos la forma\n",
        "    # correcta, porque para cada batch de tokens hay obtener los identificadores, las máscaras y el token\n",
        "    # de frase en el orden adecuado. \n",
        "    def embed_with_bert(self, all_tokens):\n",
        "        _, embs = self.bert_layer([all_tokens[:, 0, :], # A todas las frases que vengan en el batch a todas y cada una de las palabras (están en la posición 0 del tensor) en dicha frase \n",
        "                                   all_tokens[:, 1, :],\n",
        "                                   all_tokens[:, 2, :]])\n",
        "        # Recordemos que la respuesta el 1° tensor es para cuando queremos un proceso de análisis de la frase \n",
        "        # en global (vector de dim 768 que represnta la frase en todo su contexto) y la 2° corresponde a los \n",
        "        # tokens, las palabras individuales mapeadas a ese espacio de 768. Lo que quiero es analizar palabra\n",
        "        # por palabra.  \n",
        "        return embs\n",
        "\n",
        "    def call(self, inputs, training):\n",
        "        x = self.embed_with_bert(inputs)    # Se ecarga de crear la lista larga con cada uno de los tokens que corresponda, uno detrás del otro\n",
        "\n",
        "        x_1 = self.bigram(x)    # (batch_size, nb_filters, seq_len-1) Aplicará la transformación de bigramas a x\n",
        "        x_1 = self.pool(x_1)    # (batch_size, nb_filters) Ya que queremos quedarnos con el más grande, el más importante de esos valores de cada uno de los 50 de los mapas de características\n",
        "        x_2 = self.trigram(x)   # (batch_size, nb_filters, seq_len-2)\n",
        "        x_2 = self.pool(x_2)    # (batch_size, nb_filters)\n",
        "        x_3 = self.fourgram(x)  # (batch_size, nb_filters, seq_len-3)\n",
        "        x_3 = self.pool(x_3)    # (batch_size, nb_filters)\n",
        "        \n",
        "        # Concatenamos la lista formada por x1,x2 y x3 para formar nuestra macro entrada que pasará a nuestra red neuronal\n",
        "        merged = tf.concat([x_1, x_2, x_3], axis=-1) # (batch_size, 3 * nb_filters)\n",
        "        # -1: representa el último de los ejes de combinacion. En este caso tenemos 2 ejes.\n",
        "        # El primer eje es el de los bloques:\n",
        "        # Recordemos que le damos batches de palabras. Por lo tanto el primer eje sería cada uno de los\n",
        "        # objetos es un comentario.El segundo eje representa el propio valor que acabamos de sacar después\n",
        "        # de cada uno de los max pooling.\n",
        "        # Lo que se obtiene en la variable merge será un tensor cuyo tamaño será el número de filas, el\n",
        "        # mismo que el del tamaño del bloque (batchsize), y el número de columnas será 3 veces * el numero filtros\n",
        "        # 3 veces se debe a que habrá el número de filtros en bigramas, el número de filtros de trigramas, etc\n",
        "        \n",
        "        # Aplicamos la tranformación de la capa densa número 1 a toda esta info ya combinada\n",
        "        merged = self.dense_1(merged)\n",
        "        # Aplicamos la capa de Dropout unica y exclusivamente en fase de training \n",
        "        merged = self.dropout(merged, training)\n",
        "        # Por último aplicamos la capa de salida \n",
        "        output = self.last_dense(merged)\n",
        "        \n",
        "        return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hsWpzQz2IQvJ"
      },
      "source": [
        "# Fase 4: Entrenamiento"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lhfUFvWEPOIf"
      },
      "source": [
        "NB_FILTERS = 100    # Número de filtros de la red neuronal convolucional\n",
        "FFN_UNITS = 256     # Número de unidades que la capa de Feed Forward tendrá en la capa oculta\n",
        "NB_CLASSES = len(set(train_labels))      \n",
        "\n",
        "DROPOUT_RATE = 0.2  # Definimos la tasa de olvido\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "NB_EPOCHS = 5       # Número de veces que pasarémos por todo el conjunto de entrenamiento"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5HPbZ72KPPnX"
      },
      "source": [
        "# Creamos la instancia del objeto DCNN\n",
        "Dcnn = DCNNBERTEmbedding(nb_filters=NB_FILTERS,\n",
        "                         FFN_units=FFN_UNITS,\n",
        "                         nb_classes=NB_CLASSES,\n",
        "                         dropout_rate=DROPOUT_RATE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JpHDseF0QLl3"
      },
      "source": [
        "if NB_CLASSES == 2:\n",
        "    Dcnn.compile(loss=\"binary_crossentropy\",\n",
        "                 optimizer=\"adam\",\n",
        "                 metrics=[\"accuracy\"])\n",
        "else:\n",
        "    Dcnn.compile(loss=\"sparse_categorical_crossentropy\",\n",
        "                 optimizer=\"adam\",\n",
        "                 metrics=[\"sparse_categorical_accuracy\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K1hdT_JT2Rfi"
      },
      "source": [
        "# Crearemos un sistema de checkpoint en google colab que en caso de que se reinicie la sesión podemos reanudar\n",
        "# desde elúltimo checkpoint que se haya guardado o incluso más adelante , seguir entrenando con más textos \n",
        "# desde donde lo habíamos dejado en lugar de tener que crear la red neuronal desde el inicio.\n",
        "\n",
        "# Definimos una ruta dentro de mi Google Drive donde se guardarán los checkpoints\n",
        "checkpoint_path = \"./drive/My Drive/Curso NLP/BERT/ckpt_bert_embedding/\"\n",
        "\n",
        "# Guardamos el modelo que queremos guradar autoamticamente siempre que sea posible\n",
        "ckpt = tf.train.Checkpoint(Dcnn=Dcnn)\n",
        "\n",
        "# Definimos el manager que será el encargado de guardar los checkpoints en la ruta establecida\n",
        "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=1)\n",
        "# max_to_keep: Es para definir que queremos que siempre se guarden los últimos 5 checkpoints.\n",
        "# En este caso si quisieramso guardar todos bastaría con poner max_to_keep = 5 pues son 5 epochs\n",
        "\n",
        "# Las siguientes líneas de código lo que hacen es preguntarle al Checkpoint Manager si hay o no hay \n",
        "# último checkpoint\n",
        "if ckpt_manager.latest_checkpoint:  # esta linea devuelve un None si no hay checkpoint previo (If None:)\n",
        "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
        "    print(\"Último checkpoint restaurado!!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e8LHztku2cjl"
      },
      "source": [
        "# Función que entrega al método fit. Como se puede apreciar se le pueden entregar uno o más callbacks \n",
        "# y el objetivo es que se ejecuten algunas líneas de código entre cada epoch, al inicio de un epoch,\n",
        "# al final, etc. Hay una serie de metodo que podemos sobreescribir. En este caso on_epoch_end, cuando \n",
        "# termine un epoch lo que quiero es que se ejecute el guardado que hemos dicho en el checkpoint manager.\n",
        "\n",
        "# En otras palabras para que cada que fianlice un epoch haya un guardado de los pesos, de esos parámetros\n",
        "class MyCustomCallback(tf.keras.callbacks.Callback):\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        ckpt_manager.save()\n",
        "        print(\"Checkpoint guardado en {}.\".format(checkpoint_path))\n",
        "# logs=None: No queremos que se muestre ningún log en particular "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V0C5lNxFTMrA"
      },
      "source": [
        "## Result"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WrT8oWZzQNmW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 319
        },
        "outputId": "cadfe18c-0326-4625-f33e-a03ad084b1ac"
      },
      "source": [
        "Dcnn.fit(train_dataset,\n",
        "         epochs=NB_EPOCHS,\n",
        "         callbacks=[MyCustomCallback()])    # En este caso solo llamamos un callback, sin embargo, en esta lista podríamos llamar a 2,3, etc."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "  40623/Unknown - 1579s 39ms/step - loss: 0.3969 - accuracy: 0.8222Checkpoint guardado en ./drive/My Drive/Curso de NLP/BERT/ckpt_bert_embedding/.\n",
            "40623/40623 [==============================] - 1582s 39ms/step - loss: 0.3969 - accuracy: 0.8222\n",
            "Epoch 2/5\n",
            "40623/40623 [==============================] - ETA: 0s - loss: 0.3744 - accuracy: 0.8344Checkpoint guardado en ./drive/My Drive/Curso de NLP/BERT/ckpt_bert_embedding/.\n",
            "40623/40623 [==============================] - 1588s 39ms/step - loss: 0.3744 - accuracy: 0.8344\n",
            "Epoch 3/5\n",
            "40623/40623 [==============================] - ETA: 0s - loss: 0.3642 - accuracy: 0.8398Checkpoint guardado en ./drive/My Drive/Curso de NLP/BERT/ckpt_bert_embedding/.\n",
            "40623/40623 [==============================] - 1613s 40ms/step - loss: 0.3642 - accuracy: 0.8398\n",
            "Epoch 4/5\n",
            "40623/40623 [==============================] - ETA: 0s - loss: 0.3565 - accuracy: 0.8435Checkpoint guardado en ./drive/My Drive/Curso de NLP/BERT/ckpt_bert_embedding/.\n",
            "40623/40623 [==============================] - 1611s 40ms/step - loss: 0.3565 - accuracy: 0.8435\n",
            "Epoch 5/5\n",
            "40623/40623 [==============================] - ETA: 0s - loss: 0.3502 - accuracy: 0.8464Checkpoint guardado en ./drive/My Drive/Curso de NLP/BERT/ckpt_bert_embedding/.\n",
            "40623/40623 [==============================] - 1608s 40ms/step - loss: 0.3502 - accuracy: 0.8464\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f75cbf3d358>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MAb_ijA5Idmz"
      },
      "source": [
        "# Stage 5: Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gQN-Y99WIf6m",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "bbec585f-6253-4704-b2af-d768724d6b97"
      },
      "source": [
        "results = Dcnn.evaluate(test_dataset)\n",
        "print(results)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4513/4513 [==============================] - 99s 22ms/step - loss: 0.3351 - accuracy: 0.8573\n",
            "[0.33505797386169434, 0.857308030128479]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "12Mz1tT5ddFm"
      },
      "source": [
        "Hay que pensar que hay una gran parte de la red neuronal que ahora mismo no ha sido optimizada para absolutamente nada. La red neuronal anteiror, optimizaba los valores edel embedding para que fueran los mejores con base al data set, al diccionario de palabras del que formaba parte. Ahora no hemos congelado esa parte y aún así hemos visto uno mejora no muy significativa pero interesante ya que se ocupo mitad de tiempo, y también perdimos overfitting. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rj98dgxnmhak"
      },
      "source": [
        "def get_prediction(sentence):\n",
        "    tokens = encode_sentence(sentence)\n",
        "    # Recordemos que cuando entrenamos el modelo nostros le pasabamos bloques de frase y si me quedo \n",
        "    # unicamente con con lo de arriba no sería un bloque por lo añadimos una dimesnion adiconal\n",
        "\n",
        "    input_ids = get_ids(tokens)\n",
        "    input_mask = get_mask(tokens)\n",
        "    segment_ids = get_segments(tokens)\n",
        "\n",
        "    inputs = tf.stack(\n",
        "        [tf.cast(input_ids, dtype=tf.int32),\n",
        "         tf.cast(input_mask, dtype=tf.int32),\n",
        "         tf.cast(segment_ids, dtype=tf.int32)],\n",
        "         axis=0)\n",
        "    inputs = tf.expand_dims(inputs, 0) # simula un lote\n",
        "\n",
        "    output = Dcnn(inputs, training=False)\n",
        "\n",
        "    # El sentimiento lo obtendremos de redondear a la baja el output * 2. Esto ya que el número esta entre 0\n",
        "    # y 1 y este al multiplicarlo * 2 estará entre 0 y 2. Al redondear a la baja toda cosa entre 0 y .999\n",
        "    # será redondeado a 0 y todo valor entre 1 y 1.999 será 1 es imposible obtener 2 así que no hay problema\n",
        "    sentiment = math.floor(output*2)\n",
        "\n",
        "    if sentiment == 0:\n",
        "        print(\"Salida del modelo: {}\\nSentimiento predicho: Negativo.\".format(\n",
        "            output))\n",
        "    elif sentiment == 1:\n",
        "        print(\"Salida del modelo: {}\\nSentimiento predicho: Positivo.\".format(\n",
        "            output))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J9jC8UnJgOjS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "44442f2b-6e82-4d46-e076-6858c9533efc"
      },
      "source": [
        "get_prediction(\"This actor is a deception.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Salida del modelo: [[0.21923825]]\n",
            "Sentimiento predicho: Negativo.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0aMrBVRbeM29"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}