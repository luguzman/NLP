{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT_squad",
      "provenance": [],
      "collapsed_sections": [
        "qDIlHd5Tos6C"
      ],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/luguzman/NLP/blob/main/BERT_squad.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aAUq7duyG5J0"
      },
      "source": [
        "# Fase 1: Importar dependencias"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9XYlThAmGZg1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "921dd8cc-b788-4a3c-94b8-295f7edaf6cf"
      },
      "source": [
        "!pip install sentencepiece\n",
        "#!pip install tf-models-official\n",
        "!pip install tf-models-nightly # mejor instalar la versión en desarrollo\n",
        "!pip install tf-nightly"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e5/2d/6d4ca4bef9a67070fa1cac508606328329152b1df10bdf31fb6e4e727894/sentencepiece-0.1.94-cp36-cp36m-manylinux2014_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 7.0MB/s \n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.94\n",
            "Collecting tf-models-nightly\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/cf/28/db143af309ec74c62781b325e2538e85d4f10acb948bed03493eb12e778a/tf_models_nightly-2.3.0.dev20210103-py2.py3-none-any.whl (1.2MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2MB 6.5MB/s \n",
            "\u001b[?25hCollecting tensorflow-model-optimization>=0.4.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/55/38/4fd48ea1bfcb0b6e36d949025200426fe9c3a8bfae029f0973d85518fa5a/tensorflow_model_optimization-0.5.0-py2.py3-none-any.whl (172kB)\n",
            "\u001b[K     |████████████████████████████████| 174kB 16.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: kaggle>=1.3.9 in /usr/local/lib/python3.6/dist-packages (from tf-models-nightly) (1.5.10)\n",
            "Collecting tf-nightly\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/0a/542c41c3d116beae73af193ad1665482bfc16be3fe7a116e87d58f92e571/tf_nightly-2.5.0.dev20210103-cp36-cp36m-manylinux2010_x86_64.whl (399.4MB)\n",
            "\u001b[K     |████████████████████████████████| 399.4MB 33kB/s \n",
            "\u001b[?25hRequirement already satisfied: Cython in /usr/local/lib/python3.6/dist-packages (from tf-models-nightly) (0.29.21)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from tf-models-nightly) (3.2.2)\n",
            "Requirement already satisfied: pandas>=0.22.0 in /usr/local/lib/python3.6/dist-packages (from tf-models-nightly) (1.1.5)\n",
            "Requirement already satisfied: tensorflow-addons in /usr/local/lib/python3.6/dist-packages (from tf-models-nightly) (0.8.3)\n",
            "Requirement already satisfied: pycocotools in /usr/local/lib/python3.6/dist-packages (from tf-models-nightly) (2.0.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from tf-models-nightly) (1.15.0)\n",
            "Collecting tf-slim>=1.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/02/97/b0f4a64df018ca018cc035d44f2ef08f91e2e8aa67271f6f19633a015ff7/tf_slim-1.1.0-py2.py3-none-any.whl (352kB)\n",
            "\u001b[K     |████████████████████████████████| 358kB 58.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-api-python-client>=1.6.7 in /usr/local/lib/python3.6/dist-packages (from tf-models-nightly) (1.7.12)\n",
            "Collecting seqeval\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9d/2d/233c79d5b4e5ab1dbf111242299153f3caddddbb691219f363ad55ce783d/seqeval-1.2.2.tar.gz (43kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 6.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.6/dist-packages (from tf-models-nightly) (1.4.1)\n",
            "Requirement already satisfied: psutil>=5.4.3 in /usr/local/lib/python3.6/dist-packages (from tf-models-nightly) (5.4.8)\n",
            "Collecting py-cpuinfo>=3.3.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f6/f5/8e6e85ce2e9f6e05040cf0d4e26f43a4718bcc4bce988b433276d4b1a5c1/py-cpuinfo-7.0.0.tar.gz (95kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 11.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: Pillow in /usr/local/lib/python3.6/dist-packages (from tf-models-nightly) (7.0.0)\n",
            "Collecting pyyaml>=5.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/64/c2/b80047c7ac2478f9501676c988a5411ed5572f35d1beff9cae07d321512c/PyYAML-5.3.1.tar.gz (269kB)\n",
            "\u001b[K     |████████████████████████████████| 276kB 55.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorflow-datasets in /usr/local/lib/python3.6/dist-packages (from tf-models-nightly) (4.0.1)\n",
            "Requirement already satisfied: oauth2client in /usr/local/lib/python3.6/dist-packages (from tf-models-nightly) (4.1.3)\n",
            "Requirement already satisfied: gin-config in /usr/local/lib/python3.6/dist-packages (from tf-models-nightly) (0.4.0)\n",
            "Requirement already satisfied: google-cloud-bigquery>=0.31.0 in /usr/local/lib/python3.6/dist-packages (from tf-models-nightly) (1.21.0)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from tf-models-nightly) (0.8)\n",
            "Requirement already satisfied: tensorflow-hub>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tf-models-nightly) (0.10.0)\n",
            "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.6/dist-packages (from tf-models-nightly) (1.19.4)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from tf-models-nightly) (0.1.94)\n",
            "Collecting opencv-python-headless\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/96/fc/4da675cc522a749ebbcf85c5a63fba844b2d44c87e6f24e3fdb147df3270/opencv_python_headless-4.5.1.48-cp36-cp36m-manylinux2014_x86_64.whl (37.6MB)\n",
            "\u001b[K     |████████████████████████████████| 37.6MB 213kB/s \n",
            "\u001b[?25hRequirement already satisfied: dm-tree~=0.1.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-model-optimization>=0.4.1->tf-models-nightly) (0.1.5)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.6/dist-packages (from kaggle>=1.3.9->tf-models-nightly) (2020.12.5)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.6/dist-packages (from kaggle>=1.3.9->tf-models-nightly) (4.0.1)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.6/dist-packages (from kaggle>=1.3.9->tf-models-nightly) (2.8.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from kaggle>=1.3.9->tf-models-nightly) (4.41.1)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.6/dist-packages (from kaggle>=1.3.9->tf-models-nightly) (1.24.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from kaggle>=1.3.9->tf-models-nightly) (2.23.0)\n",
            "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly->tf-models-nightly) (3.3.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.6/dist-packages (from tf-nightly->tf-models-nightly) (3.12.4)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.6/dist-packages (from tf-nightly->tf-models-nightly) (1.1.2)\n",
            "Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly->tf-models-nightly) (1.1.0)\n",
            "Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.6/dist-packages (from tf-nightly->tf-models-nightly) (1.12.1)\n",
            "Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.6/dist-packages (from tf-nightly->tf-models-nightly) (0.10.0)\n",
            "Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.6/dist-packages (from tf-nightly->tf-models-nightly) (3.7.4.3)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tf-nightly->tf-models-nightly) (0.3.3)\n",
            "Collecting h5py~=3.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/70/7a/e53e500335afb6b1aade11227cdf107fca54106a1dca5c9d13242a043f3b/h5py-3.1.0-cp36-cp36m-manylinux1_x86_64.whl (4.0MB)\n",
            "\u001b[K     |████████████████████████████████| 4.0MB 59.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tf-nightly->tf-models-nightly) (1.6.3)\n",
            "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.6/dist-packages (from tf-nightly->tf-models-nightly) (0.2.0)\n",
            "Collecting tb-nightly~=2.5.0.a\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/85/94/a3e52503613b72a48d589832f1a0791f4266d07b0d9e337de40eb15cebd3/tb_nightly-2.5.0a20210102-py3-none-any.whl (12.2MB)\n",
            "\u001b[K     |████████████████████████████████| 12.2MB 57.7MB/s \n",
            "\u001b[?25hCollecting tf-estimator-nightly~=2.5.0.dev\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5d/ac/8df7c6567cfa0e50260489911b2fb7b08c119e77983fe014701568aff32a/tf_estimator_nightly-2.5.0.dev2021010301-py2.py3-none-any.whl (462kB)\n",
            "\u001b[K     |████████████████████████████████| 471kB 61.4MB/s \n",
            "\u001b[?25hCollecting grpcio~=1.34.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/91/ab/8a7b37278fb59f3688af01cd069a5a4d2f3383eea2a2f78ddea4c7be047a/grpcio-1.34.0-cp36-cp36m-manylinux2014_x86_64.whl (4.0MB)\n",
            "\u001b[K     |████████████████████████████████| 4.0MB 61.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.6/dist-packages (from tf-nightly->tf-models-nightly) (0.36.2)\n",
            "Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly->tf-models-nightly) (1.12)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->tf-models-nightly) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->tf-models-nightly) (0.10.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->tf-models-nightly) (2.4.7)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.22.0->tf-models-nightly) (2018.9)\n",
            "Requirement already satisfied: typeguard in /usr/local/lib/python3.6/dist-packages (from tensorflow-addons->tf-models-nightly) (2.7.1)\n",
            "Requirement already satisfied: setuptools>=18.0 in /usr/local/lib/python3.6/dist-packages (from pycocotools->tf-models-nightly) (51.0.0)\n",
            "Requirement already satisfied: google-auth>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.6.7->tf-models-nightly) (1.17.2)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.6.7->tf-models-nightly) (0.0.4)\n",
            "Requirement already satisfied: httplib2<1dev,>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.6.7->tf-models-nightly) (0.17.4)\n",
            "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.6.7->tf-models-nightly) (3.0.1)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.6/dist-packages (from seqeval->tf-models-nightly) (0.22.2.post1)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->tf-models-nightly) (0.26.0)\n",
            "Requirement already satisfied: attrs>=18.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->tf-models-nightly) (20.3.0)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->tf-models-nightly) (2.3)\n",
            "Requirement already satisfied: importlib-resources; python_version < \"3.9\" in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->tf-models-nightly) (3.3.0)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->tf-models-nightly) (0.3.3)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->tf-models-nightly) (0.16.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.6/dist-packages (from oauth2client->tf-models-nightly) (0.2.8)\n",
            "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from oauth2client->tf-models-nightly) (4.6)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.6/dist-packages (from oauth2client->tf-models-nightly) (0.4.8)\n",
            "Requirement already satisfied: google-cloud-core<2.0dev,>=1.0.3 in /usr/local/lib/python3.6/dist-packages (from google-cloud-bigquery>=0.31.0->tf-models-nightly) (1.0.3)\n",
            "Requirement already satisfied: google-resumable-media!=0.4.0,<0.5.0dev,>=0.3.1 in /usr/local/lib/python3.6/dist-packages (from google-cloud-bigquery>=0.31.0->tf-models-nightly) (0.4.1)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.6/dist-packages (from python-slugify->kaggle>=1.3.9->tf-models-nightly) (1.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->kaggle>=1.3.9->tf-models-nightly) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->kaggle>=1.3.9->tf-models-nightly) (2.10)\n",
            "Collecting cached-property; python_version < \"3.8\"\n",
            "  Downloading https://files.pythonhosted.org/packages/48/19/f2090f7dad41e225c7f2326e4cfe6fff49e57dedb5b53636c9551f86b069/cached_property-1.5.2-py2.py3-none-any.whl\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tb-nightly~=2.5.0.a->tf-nightly->tf-models-nightly) (0.4.2)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tb-nightly~=2.5.0.a->tf-nightly->tf-models-nightly) (1.7.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tb-nightly~=2.5.0.a->tf-nightly->tf-models-nightly) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tb-nightly~=2.5.0.a->tf-nightly->tf-models-nightly) (3.3.3)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.4.1->google-api-python-client>=1.6.7->tf-models-nightly) (4.2.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.21.3->seqeval->tf-models-nightly) (1.0.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-metadata->tensorflow-datasets->tf-models-nightly) (1.52.0)\n",
            "Requirement already satisfied: zipp>=0.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-resources; python_version < \"3.9\"->tensorflow-datasets->tf-models-nightly) (3.4.0)\n",
            "Requirement already satisfied: google-api-core<2.0.0dev,>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from google-cloud-core<2.0dev,>=1.0.3->google-cloud-bigquery>=0.31.0->tf-models-nightly) (1.16.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tb-nightly~=2.5.0.a->tf-nightly->tf-models-nightly) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tb-nightly~=2.5.0.a->tf-nightly->tf-models-nightly) (3.3.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tb-nightly~=2.5.0.a->tf-nightly->tf-models-nightly) (3.1.0)\n",
            "Building wheels for collected packages: seqeval, py-cpuinfo, pyyaml\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for seqeval: filename=seqeval-1.2.2-cp36-none-any.whl size=16171 sha256=d16b6cf6fbce25d5f31396c28378b5b17578992a8b53fdcd2120ed4df9534b6c\n",
            "  Stored in directory: /root/.cache/pip/wheels/52/df/1b/45d75646c37428f7e626214704a0e35bd3cfc32eda37e59e5f\n",
            "  Building wheel for py-cpuinfo (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for py-cpuinfo: filename=py_cpuinfo-7.0.0-cp36-none-any.whl size=20072 sha256=88f7fe917bac27fa18a2dfd69ba8e09b1b93f5b2ce60672b047cd4e06c45c6e2\n",
            "  Stored in directory: /root/.cache/pip/wheels/f1/93/7b/127daf0c3a5a49feb2fecd468d508067c733fba5192f726ad1\n",
            "  Building wheel for pyyaml (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyyaml: filename=PyYAML-5.3.1-cp36-cp36m-linux_x86_64.whl size=44621 sha256=f4708dee8a7d120d8153ea79de7c9b93d127cde3f1cecf72dc6298fe9c583fa0\n",
            "  Stored in directory: /root/.cache/pip/wheels/a7/c1/ea/cf5bd31012e735dc1dfea3131a2d5eae7978b251083d6247bd\n",
            "Successfully built seqeval py-cpuinfo pyyaml\n",
            "\u001b[31mERROR: tensorflow 2.4.0 has requirement grpcio~=1.32.0, but you'll have grpcio 1.34.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow 2.4.0 has requirement h5py~=2.10.0, but you'll have h5py 3.1.0 which is incompatible.\u001b[0m\n",
            "Installing collected packages: tensorflow-model-optimization, cached-property, h5py, grpcio, tb-nightly, tf-estimator-nightly, tf-nightly, tf-slim, seqeval, py-cpuinfo, pyyaml, opencv-python-headless, tf-models-nightly\n",
            "  Found existing installation: h5py 2.10.0\n",
            "    Uninstalling h5py-2.10.0:\n",
            "      Successfully uninstalled h5py-2.10.0\n",
            "  Found existing installation: grpcio 1.32.0\n",
            "    Uninstalling grpcio-1.32.0:\n",
            "      Successfully uninstalled grpcio-1.32.0\n",
            "  Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed cached-property-1.5.2 grpcio-1.34.0 h5py-3.1.0 opencv-python-headless-4.5.1.48 py-cpuinfo-7.0.0 pyyaml-5.3.1 seqeval-1.2.2 tb-nightly-2.5.0a20210102 tensorflow-model-optimization-0.5.0 tf-estimator-nightly-2.5.0.dev2021010301 tf-models-nightly-2.3.0.dev20210103 tf-nightly-2.5.0.dev20210103 tf-slim-1.1.0\n",
            "Requirement already satisfied: tf-nightly in /usr/local/lib/python3.6/dist-packages (2.5.0.dev20210103)\n",
            "Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (0.10.0)\n",
            "Requirement already satisfied: tb-nightly~=2.5.0.a in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (2.5.0a20210102)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (1.1.2)\n",
            "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (0.2.0)\n",
            "Requirement already satisfied: numpy~=1.19.2 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (1.19.4)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (1.6.3)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (3.12.4)\n",
            "Requirement already satisfied: six~=1.15.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (1.15.0)\n",
            "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (0.36.2)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (0.3.3)\n",
            "Requirement already satisfied: grpcio~=1.34.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (1.34.0)\n",
            "Requirement already satisfied: h5py~=3.1.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (3.7.4.3)\n",
            "Requirement already satisfied: tf-estimator-nightly~=2.5.0.dev in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (2.5.0.dev2021010301)\n",
            "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (3.3.0)\n",
            "Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (1.1.0)\n",
            "Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (1.12)\n",
            "Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (1.12.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tb-nightly~=2.5.0.a->tf-nightly) (3.3.3)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tb-nightly~=2.5.0.a->tf-nightly) (51.0.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tb-nightly~=2.5.0.a->tf-nightly) (1.17.2)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tb-nightly~=2.5.0.a->tf-nightly) (0.4.2)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tb-nightly~=2.5.0.a->tf-nightly) (2.23.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tb-nightly~=2.5.0.a->tf-nightly) (1.0.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tb-nightly~=2.5.0.a->tf-nightly) (1.7.0)\n",
            "Requirement already satisfied: cached-property; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from h5py~=3.1.0->tf-nightly) (1.5.2)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tb-nightly~=2.5.0.a->tf-nightly) (3.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tb-nightly~=2.5.0.a->tf-nightly) (4.6)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tb-nightly~=2.5.0.a->tf-nightly) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tb-nightly~=2.5.0.a->tf-nightly) (4.2.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tb-nightly~=2.5.0.a->tf-nightly) (1.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tb-nightly~=2.5.0.a->tf-nightly) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tb-nightly~=2.5.0.a->tf-nightly) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tb-nightly~=2.5.0.a->tf-nightly) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tb-nightly~=2.5.0.a->tf-nightly) (3.0.4)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tb-nightly~=2.5.0.a->tf-nightly) (3.4.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3\"->google-auth<2,>=1.6.3->tb-nightly~=2.5.0.a->tf-nightly) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tb-nightly~=2.5.0.a->tf-nightly) (3.1.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eDdYqvxPHnI8"
      },
      "source": [
        "import tensorflow as tf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zFS9XSASHvQi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 57
        },
        "outputId": "f9859e9f-5200-4999-ed0d-026c3f45c3cd"
      },
      "source": [
        "tf.__version__"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2.4.0-dev20200822'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fv38eJzSH00S"
      },
      "source": [
        "import tensorflow_hub as hub        # librería que nos ayuda a importar modelos con pesos ya almacenados\n",
        "\n",
        "from official.nlp.bert.tokenization import FullTokenizer        # Es doonde esta todo lo del módulo de desarrollo de tensorflow\n",
        "from official.nlp.bert.input_pipeline import create_squad_dataset    # Nos va a peritir crear el dataset de squad a partir de importar esos ficheros que descargue y coloque en mi google drive\n",
        "from official.nlp.data.squad_lib import generate_tf_record_from_json_file   # Creará un fichero intermedio a partir del dataset, que es el llamada tf_record\n",
        "\n",
        "from official.nlp import optimization   # Mejora del optimizador Adam que encaja perfecto con BERT\n",
        "\n",
        "from official.nlp.data.squad_lib import read_squad_examples     # Basicamente es para leer los ficheros de evaluación y que podamos ver que también ha sido nuestro modole para resolver los problemas de squad\n",
        "from official.nlp.data.squad_lib import FeatureWriter           # Herramienta que nos ayudará a tarabajar la salida de BERT y validarlo como parte del proceso final \n",
        "from official.nlp.data.squad_lib import convert_examples_to_features    # Herramienta que nos ayudará a tarabajar la salida de BERT y validarlo como parte del proceso final\n",
        "from official.nlp.data.squad_lib import write_predictions       # nos permitirá escribir las predicciones en un formato json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WjzrCZJMJN1N"
      },
      "source": [
        "import numpy as np\n",
        "import math\n",
        "import random\n",
        "import time\n",
        "import json\n",
        "import collections\n",
        "import os\n",
        "\n",
        "from google.colab import drive"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iSGw1I_zHAb5"
      },
      "source": [
        "# Fase 2: Preprocesado de Datos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SfdTEReCKFky",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "82d941bc-9989-4350-d331-ad08e0f2eaec"
      },
      "source": [
        "drive.mount(\"/content/drive\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E_PcOeemKcYq"
      },
      "source": [
        "# Creamos un fichero  json intermedio que contendra el conjunto de entrenamiento junto con el vocabulario \n",
        "input_meta_data = generate_tf_record_from_json_file(\n",
        "    \"/content/drive/My Drive/Curso de NLP/BERT/squad_data/train-v1.1.json\", # Acceso al fichero de entrenamiento\n",
        "    \"/content/drive/My Drive/Curso de NLP/BERT/squad_data/vocab.txt\",       # La ruta al fichero de vocabulario \n",
        "    \"/content/drive/My Drive/Curso de NLP/BERT/squad_data/train-v1.1.tf_record\") # Indicamos el path donde queremos que se guarde este fichero tensorflow record"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7XQGdnANLziM"
      },
      "source": [
        "# Almacenamos los metadatos para entrenar \n",
        "with tf.io.gfile.GFile(\"/content/drive/My Drive/Curso de NLP/BERT/squad_data/train_meta_data\", \"w\") as writer:\n",
        "    writer.write(json.dumps(input_meta_data, indent=4) + \"\\n\") \n",
        "# \\n: es simplemente para no tener problemas de lectura pues los ficheros deberían acabar siempre con un intro al final  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "295nG2zQMYSU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c4465981-dde3-46d3-c2f1-ee873caa2f32"
      },
      "source": [
        "# Creamos el dataset a partir de todos estos datos volcados, de estos tf_records\n",
        "BATCH_SIZE = 4  # Notemos que aquí tenemos un pequeño problema y es que al crear este dataset yo no puedo indicar un tamaño de lote demasiado\n",
        "# grande y es que aparentemente Google dice que utilizar un un batch_size demasiado grande sería bastante complicado. La razón es que las entradas\n",
        "# son mucho más grandes, mucho más largas que las que hemos utilizado en formato de tweet. Recordemos que un tweet caben 140 caracteres. Aquí\n",
        "# podriamos tener frases realmente largas para la tarea de preguntas y respuestas. Recordemos también que la primera entrada es todo un texto, \n",
        "# podría ser la página entra de un libro que contenga información de un tema.\n",
        "\n",
        "train_dataset = create_squad_dataset(\n",
        "    \"/content/drive/My Drive/Curso de NLP/BERT/squad_data/train-v1.1.tf_record\",\n",
        "    input_meta_data['max_seq_length'], # sequencia más larga con la que nos encontramos que en este caso es de 384\n",
        "    BATCH_SIZE,\n",
        "    is_training=True)  # A la hr de crear el conjunto squad el de entrenamiento y el de testing es diferente"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:AutoGraph could not transform <function create_squad_dataset.<locals>._select_data_from_record at 0x7f77d552b0d0> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: '<' not supported between instances of 'str' and 'Literal'\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:AutoGraph could not transform <function create_squad_dataset.<locals>._select_data_from_record at 0x7f77d552b0d0> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: '<' not supported between instances of 'str' and 'Literal'\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING: AutoGraph could not transform <function create_squad_dataset.<locals>._select_data_from_record at 0x7f77d552b0d0> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: '<' not supported between instances of 'str' and 'Literal'\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F4939RJqHRUs"
      },
      "source": [
        "# Fase 3: Construcción del modelo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lcQcFi8kOc6K"
      },
      "source": [
        "## Capa Squad"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iynuYxtixV_w"
      },
      "source": [
        "Para cada uno de los vectores de salida que me devuelve BERT vamos a aplicarle una capa densa que se conectará con dos unidades(neuronas) en la capa de salida. Que van a ser para cada token el 1° número, qué tan problable es que la respuesta empiece en dicho token y el segundo número nos dirá lo mismo: la verosimilitud de que ese token sea el final de frase que responde a esa pregunta. Dicho esto habrá que re-dimensionarlo y colocarlo de modo que si tenemos un párrafo de 200 palabras en lugar de tener 200 pares de valores. Score de ser el inicio y score de ser el final, voy preferir tener 200 valores en dos listas separadas, ie, una lista en donde vendrá el score por palabra de ser el inicio de frase y de la misma manera la segunda pero considerando el score de que se el final de la frase."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gjmLeQjiaOo5"
      },
      "source": [
        "class BertSquadLayer(tf.keras.layers.Layer):\n",
        "\n",
        "  def __init__(self):\n",
        "    # Inicializamos la super clase. de este modo ya estarán creadas las variables y los metodos pertenecientes a la super clase.\n",
        "    super(BertSquadLayer, self).__init__()\n",
        "    # Creamos la capa densa de la que hemos platicado.\n",
        "    self.final_dense = tf.keras.layers.Dense(\n",
        "        units=2,\n",
        "        kernel_initializer=tf.keras.initializers.TruncatedNormal(stddev=0.02))  # Google recomiendo que a la hr de hacer find tuning con BERT utilicemos una normal truncada con un valor pequeño de la desviación estándar\n",
        "\n",
        "    # inputs: son los valores que saldran de la capa de BERT\n",
        "  def call(self, inputs):\n",
        "    logits = self.final_dense(inputs) # (batch_size, seq_len, 2)\n",
        "\n",
        "    # Quiero tener un par de listas dodne cada una de ellas tenga esos 200 valores pro separado, por lo que tenemos que re-dimensionar y cortar el tensor\n",
        "    logits = tf.transpose(logits, [2, 0, 1]) # (2, batch_size, seq_len) pos 2: que tenía el score de ser inicio y final será lo primero, pos 0: lote de entrenamiento pos1:dimensión que se corresponde con las palabras (longitud de la secuencia)\n",
        "    unstacked_logits = tf.unstack(logits, axis=0) # [(batch_size, seq_len), (batch_size, seq_len)] Como la primer entrada (axis=0; eje número 0) tiene el score de ser inicio y final lo desapilamos\n",
        "    return unstacked_logits[0], unstacked_logits[1] \n",
        "    # La 1° me devolverá para cada frase del lote y para cada palabra de la frase el score de ser esa palabra de ese lote el incio de la respuesta\n",
        "    # El 2° me devolverá para cada frase del lote y para cada palabra de la frase el score de ser esa palabra de ese lote el final de la respuesta\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQbQFKjUOeyf"
      },
      "source": [
        "## Modelo completo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9geXlgpYE-5c"
      },
      "source": [
        "Construimos una clase que será todo el modelo que juntará la primera parte de BERT con la capa de squad."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KgkIFb1GMT81"
      },
      "source": [
        "class BERTSquad(tf.keras.Model):\n",
        "    \n",
        "    def __init__(self,\n",
        "                 name=\"bert_squad\"):\n",
        "        super(BERTSquad, self).__init__(name=name)\n",
        "        \n",
        "        self.bert_layer = hub.KerasLayer(\n",
        "            \"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\",\n",
        "            trainable=True)\n",
        "        \n",
        "        self.squad_layer = BertSquadLayer()\n",
        "    \n",
        "    # Definimos una función de ayuda que lo que hará será formatear la entrada de BERT porque recordemos que necesitamos tener los id's, las \n",
        "    # máscaras y los tokens de frase en ese orden correctamente para la entrada del algoritmo BERT.\n",
        "    def apply_bert(self, inputs):\n",
        "        _ , sequence_output = self.bert_layer([inputs[\"input_word_ids\"],    # Clave con la que se accede a los identificadores de cada palabra\n",
        "                                               inputs[\"input_mask\"],        # Es con el que se accede a la máscara de aleatorización.   \n",
        "                                               inputs[\"input_type_ids\"]])   # Nos indica si la frase es la número 1 o la número 2\n",
        "        # Recordemos que la capa de BERT devuelve el vector se corresponde a toda la frase en 1° posición y el que corresponde a cada una de\n",
        "        # las palabras tokenizadas en 2° posición  \n",
        "        return sequence_output\n",
        "\n",
        "    def call(self, inputs):\n",
        "        seq_output = self.apply_bert(inputs)\n",
        "\n",
        "        start_logits, end_logits = self.squad_layer(seq_output)\n",
        "        \n",
        "        return start_logits, end_logits"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lBmSU2RnHV_a"
      },
      "source": [
        "# Fase 4: Entrenamiento"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WnA3WHwRIHAZ"
      },
      "source": [
        "## Creación de la IA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JEUxomxENRoJ"
      },
      "source": [
        "TRAIN_DATA_SIZE = 88641     # Tamaño/longitud de nuestro conjunto de entrenamiento.\n",
        "# Este es un dataset bastante pesado para entrenar y para poder entrenarlo en google colab sin que nos eche es hacer un pequeño truco y es no \n",
        "# utilizar todo el data set para entrenar sino que queremos ver BERT funciona correctamente por lo que definimos:\n",
        "NB_BATCHES_TRAIN = 2000     # Esto representa aprox un 10% del total de lotes que tenemos 88641/4*.10\n",
        "BATCH_SIZE = 4              \n",
        "NB_EPOCHS = 3               # Inclusive 2 podría ser más que suficiente\n",
        "# Versión modificada del optimizador Adam por Google creado explictamemnte para esta tarea:\n",
        "INIT_LR = 5e-5              # Ratio de aprendizaje al inicio. Se define con el propósito de que el ratio de aprendizaje sea pequeño al inicio para que pueda ir experimentando y vaya incrementando durante un ratio determinado de pasos:\n",
        "WARMUP_STEPS = int(NB_BATCHES_TRAIN * 0.1)  # Ratio determinado de pasos que será el 10% del lote 2000*10% = 200"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Pg6EKe2daFy"
      },
      "source": [
        "train_dataset_light = train_dataset.take(NB_BATCHES_TRAIN)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CXnH3CQWH5tt"
      },
      "source": [
        "Creamos y compilamos nuestro modelo BERTSquad"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eHd5MzTdNIZq"
      },
      "source": [
        "bert_squad = BERTSquad()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dQrfVYTrIGMN"
      },
      "source": [
        "Creamos nuestro optimizador Adam modificado por Google."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G0cvDjBm_KXT"
      },
      "source": [
        "optimizer = optimization.create_optimizer(\n",
        "    init_lr=INIT_LR,\n",
        "    num_train_steps=NB_BATCHES_TRAIN,\n",
        "    num_warmup_steps=WARMUP_STEPS)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "96ns-oGSdRXz"
      },
      "source": [
        "Creamos nuestra función de perdidas personalizada. Recordemos que nuestra capa final nos devolverá 2 vectores uno en donde nos devuelve la verosimilitud de que un token sea el token de incio y otro considerando cual es el token final. Al final de cuentas suena como que hay que resolver un problema de clasificación pues hay que decidir si es o no el token que buscamos. Por lo tanto, utilizaremos sparse_categorical_crossentropy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I6kG-HpzVK7v"
      },
      "source": [
        "# labels: etiquetas reales\n",
        "# model_outputs: salidas del modelo\n",
        "def squad_loss_fn(labels, model_outputs):\n",
        "    start_positions = labels['start_positions']\n",
        "    end_positions = labels['end_positions']\n",
        "    start_logits, end_logits = model_outputs\n",
        "\n",
        "    start_loss = tf.keras.backend.sparse_categorical_crossentropy(\n",
        "        start_positions, start_logits, from_logits=True) \n",
        "# from_logits=True: Significa que la respuesta de nuestro modelo, en este caso el segundo parámetro, no es reralmente una probabilidad y es que en \n",
        "# ningún momento hemos aplicado una función softmax o algo así que se encargue de que todos los valores esten entre 0 y 1 y la suma de todos sea\n",
        "# igual a 1. Ahora solo tenemos números de medidas de verosimilitud, pero que en principio serán números cualesquiera, podrían ser positivos \n",
        "# o negativos y al aplicar from_logits=True, la función sparse_categorical_crossentropy se encargará de hacer las modificaciones adecuadas \n",
        "# y de comparar la respuesta para devovlernos esa pérdida. \n",
        "\n",
        "    end_loss = tf.keras.backend.sparse_categorical_crossentropy(\n",
        "        end_positions, end_logits, from_logits=True)\n",
        "    \n",
        "    # Obtenemos la media de ambas perdidas\n",
        "    total_loss = (tf.reduce_mean(start_loss) + tf.reduce_mean(end_loss)) / 2    \n",
        "    # reduce_mean: ya que queremos obtener la pérdida de todo el lote. En un principio obtendríamos 4 valores ya que recordemos que son 4 frases \n",
        "    # (dado que BATCH_SIZE = 4)\n",
        "\n",
        "    return total_loss\n",
        "\n",
        "# Finalmente, como queremos construir nuestro propio bucle de entrenamiento personalizado, nos va a intersar tener un tracking de la función \n",
        "# de perdidas (global duarante toda la fase de training) que acabamos de crear y que no es una de las estándar que utilizamos de las de tf.\n",
        "train_loss = tf.keras.metrics.Mean(name=\"train_loss\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HN_C_WA5R_Cb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "783e1376-0065-4189-f959-8d3f62855e25"
      },
      "source": [
        "next(iter(train_dataset_light))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "({'input_mask': <tf.Tensor: shape=(4, 384), dtype=int32, numpy=\n",
              "  array([[1, 1, 1, ..., 0, 0, 0],\n",
              "         [1, 1, 1, ..., 0, 0, 0],\n",
              "         [1, 1, 1, ..., 0, 0, 0],\n",
              "         [1, 1, 1, ..., 0, 0, 0]], dtype=int32)>,\n",
              "  'input_type_ids': <tf.Tensor: shape=(4, 384), dtype=int32, numpy=\n",
              "  array([[0, 0, 0, ..., 0, 0, 0],\n",
              "         [0, 0, 0, ..., 0, 0, 0],\n",
              "         [0, 0, 0, ..., 0, 0, 0],\n",
              "         [0, 0, 0, ..., 0, 0, 0]], dtype=int32)>,\n",
              "  'input_word_ids': <tf.Tensor: shape=(4, 384), dtype=int32, numpy=\n",
              "  array([[ 101, 2216, 2040, ...,    0,    0,    0],\n",
              "         [ 101, 2054, 7017, ...,    0,    0,    0],\n",
              "         [ 101, 2054, 2001, ...,    0,    0,    0],\n",
              "         [ 101, 2054, 2565, ...,    0,    0,    0]], dtype=int32)>},\n",
              " {'end_positions': <tf.Tensor: shape=(4,), dtype=int32, numpy=array([111,  74,  82, 102], dtype=int32)>,\n",
              "  'start_positions': <tf.Tensor: shape=(4,), dtype=int32, numpy=array([109,  71,  80, 100], dtype=int32)>})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lLZ2wEgzjzKF"
      },
      "source": [
        "Compilamos el modelo:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V-iE2QFC_KRI"
      },
      "source": [
        "bert_squad.compile(optimizer,\n",
        "                   squad_loss_fn)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hjp-_4OyTbuK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "f85640d4-9adf-44f9-b612-5c414e085077"
      },
      "source": [
        "# Crearemos un sistema de checkpoint en google colab que en caso de que se reinicie la sesión podemos reanudar\n",
        "# desde elúltimo checkpoint que se haya guardado o incluso más adelante , seguir entrenando con más textos \n",
        "# desde donde lo habíamos dejado en lugar de tener que crear la red neuronal desde el inicio.\n",
        "\n",
        "# Definimos una ruta dentro de mi Google Drive donde se guardarán los checkpoints\n",
        "checkpoint_path = \"./drive/My Drive/Curso de NLP/BERT/ckpt_bert_squad/\"\n",
        "\n",
        "# Guardamos el modelo que queremos guardar autoamticamente siempre que sea posible\n",
        "ckpt = tf.train.Checkpoint(bert_squad=bert_squad)\n",
        "\n",
        "# Definimos el manager que será el encargado de guardar los checkpoints en la ruta establecida\n",
        "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=1)\n",
        "# max_to_keep: Es para definir que queremos que siempre se guarden los últimos 5 checkpoints.\n",
        "# En este caso si quisieramos guardar todos bastaría con poner max_to_keep = 3 pues son 3 epochs\n",
        "\n",
        "# Las siguientes líneas de código lo que hacen es preguntarle al Checkpoint Manager si hay o no hay \n",
        "# último checkpoint\n",
        "if ckpt_manager.latest_checkpoint:      # esta linea devuelve un None si no hay checkpoint previo (If None:)\n",
        "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
        "    print(\"Último checkpoint restaurado!!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Último checkpoint restaurado!!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bDgEq09xIOOl"
      },
      "source": [
        "## Entrenamiento personalizado"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ywelW3uaSbT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "1c27881e-7ebe-438e-b6dd-6830bd4bcc79"
      },
      "source": [
        "for epoch in range(NB_EPOCHS):\n",
        "    print(\"Inicio del Epoch {}\".format(epoch+1))\n",
        "    start = time.time()\n",
        "    \n",
        "    train_loss.reset_states()   # Devuelve toda la función de pérdidas a 0 y así durante un epoch llevar traking solo de este\n",
        "    \n",
        "    for (batch, (inputs, targets)) in enumerate(train_dataset_light):\n",
        "        with tf.GradientTape() as tape:\n",
        "            model_outputs = bert_squad(inputs)\n",
        "            loss = squad_loss_fn(targets, model_outputs)\n",
        "        \n",
        "        gradients = tape.gradient(loss, bert_squad.trainable_variables)     # Se obtiene el gradiente solo para las variables entrenables de BERT\n",
        "        optimizer.apply_gradients(zip(gradients, bert_squad.trainable_variables))   # juntamos la info de los gradientes y a que variables pertenecen\n",
        "        \n",
        "        train_loss(loss)\n",
        "        \n",
        "        # Si el lote es múltiplo de 50 que se impriman los siguientes valores:\n",
        "        if batch % 50 == 0:\n",
        "            print(\"Epoch {} Lote {} Pérdida {:.4f}\".format(\n",
        "                epoch+1, batch, train_loss.result()))\n",
        "        \n",
        "        # Si el lote es múltiplo de 500 se hará un checkpoint\n",
        "        if batch % 500 == 0:\n",
        "            ckpt_save_path = ckpt_manager.save()\n",
        "            print(\"Guardando checkpoint para el epoch {} en el directorio {}\".format(epoch+1,\n",
        "                                                                ckpt_save_path))\n",
        "    print(\"Tiempo total para entrenar 1 epoch: {} segs\\n\".format(time.time() - start))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Inicio del Epoch 1\n",
            "Epoch 1 Lote 0 Pérdida 6.0621\n",
            "Guardando checkpoint para el epoch 1 en el directorio ./drive/My Drive/Curso de NLP/BERT/ckpt_bert_squad/ckpt-1\n",
            "Epoch 1 Lote 50 Pérdida 5.8209\n",
            "Epoch 1 Lote 100 Pérdida 5.2858\n",
            "Epoch 1 Lote 150 Pérdida 4.6081\n",
            "Epoch 1 Lote 200 Pérdida 4.1292\n",
            "Epoch 1 Lote 250 Pérdida 3.7406\n",
            "Epoch 1 Lote 300 Pérdida 3.4795\n",
            "Epoch 1 Lote 350 Pérdida 3.3128\n",
            "Epoch 1 Lote 400 Pérdida 3.1128\n",
            "Epoch 1 Lote 450 Pérdida 2.9574\n",
            "Epoch 1 Lote 500 Pérdida 2.8129\n",
            "Guardando checkpoint para el epoch 1 en el directorio ./drive/My Drive/Curso de NLP/BERT/ckpt_bert_squad/ckpt-2\n",
            "Epoch 1 Lote 550 Pérdida 2.6840\n",
            "Epoch 1 Lote 600 Pérdida 2.6034\n",
            "Epoch 1 Lote 650 Pérdida 2.5247\n",
            "Epoch 1 Lote 700 Pérdida 2.4478\n",
            "Epoch 1 Lote 750 Pérdida 2.3795\n",
            "Epoch 1 Lote 800 Pérdida 2.3212\n",
            "Epoch 1 Lote 850 Pérdida 2.2794\n",
            "Epoch 1 Lote 900 Pérdida 2.2269\n",
            "Epoch 1 Lote 950 Pérdida 2.1805\n",
            "Epoch 1 Lote 1000 Pérdida 2.1184\n",
            "Guardando checkpoint para el epoch 1 en el directorio ./drive/My Drive/Curso de NLP/BERT/ckpt_bert_squad/ckpt-3\n",
            "Epoch 1 Lote 1050 Pérdida 2.0712\n",
            "Epoch 1 Lote 1100 Pérdida 2.0192\n",
            "Epoch 1 Lote 1150 Pérdida 1.9696\n",
            "Epoch 1 Lote 1200 Pérdida 1.9270\n",
            "Epoch 1 Lote 1250 Pérdida 1.9186\n",
            "Epoch 1 Lote 1300 Pérdida 1.8983\n",
            "Epoch 1 Lote 1350 Pérdida 1.8858\n",
            "Epoch 1 Lote 1400 Pérdida 1.8687\n",
            "Epoch 1 Lote 1450 Pérdida 1.8431\n",
            "Epoch 1 Lote 1500 Pérdida 1.8203\n",
            "Guardando checkpoint para el epoch 1 en el directorio ./drive/My Drive/Curso de NLP/BERT/ckpt_bert_squad/ckpt-4\n",
            "Epoch 1 Lote 1550 Pérdida 1.8009\n",
            "Epoch 1 Lote 1600 Pérdida 1.7833\n",
            "Epoch 1 Lote 1650 Pérdida 1.7777\n",
            "Epoch 1 Lote 1700 Pérdida 1.7699\n",
            "Epoch 1 Lote 1750 Pérdida 1.7521\n",
            "Epoch 1 Lote 1800 Pérdida 1.7437\n",
            "Epoch 1 Lote 1850 Pérdida 1.7187\n",
            "Epoch 1 Lote 1900 Pérdida 1.6900\n",
            "Epoch 1 Lote 1950 Pérdida 1.6774\n",
            "Tiempo total para entrenar 1 epoch: 5262.5173897743225 segs\n",
            "\n",
            "Inicio del Epoch 2\n",
            "Epoch 2 Lote 0 Pérdida 1.1600\n",
            "Guardando checkpoint para el epoch 2 en el directorio ./drive/My Drive/Curso de NLP/BERT/ckpt_bert_squad/ckpt-5\n",
            "Epoch 2 Lote 50 Pérdida 1.1323\n",
            "Epoch 2 Lote 100 Pérdida 1.1650\n",
            "Epoch 2 Lote 150 Pérdida 1.0600\n",
            "Epoch 2 Lote 200 Pérdida 1.0460\n",
            "Epoch 2 Lote 250 Pérdida 0.9981\n",
            "Epoch 2 Lote 300 Pérdida 0.9548\n",
            "Epoch 2 Lote 350 Pérdida 0.9416\n",
            "Epoch 2 Lote 400 Pérdida 0.8956\n",
            "Epoch 2 Lote 450 Pérdida 0.8426\n",
            "Epoch 2 Lote 500 Pérdida 0.8125\n",
            "Guardando checkpoint para el epoch 2 en el directorio ./drive/My Drive/Curso de NLP/BERT/ckpt_bert_squad/ckpt-6\n",
            "Epoch 2 Lote 550 Pérdida 0.7875\n",
            "Epoch 2 Lote 600 Pérdida 0.7739\n",
            "Epoch 2 Lote 650 Pérdida 0.7629\n",
            "Epoch 2 Lote 700 Pérdida 0.7346\n",
            "Epoch 2 Lote 750 Pérdida 0.7113\n",
            "Epoch 2 Lote 800 Pérdida 0.6930\n",
            "Epoch 2 Lote 850 Pérdida 0.6815\n",
            "Epoch 2 Lote 900 Pérdida 0.6704\n",
            "Epoch 2 Lote 950 Pérdida 0.6603\n",
            "Epoch 2 Lote 1000 Pérdida 0.6411\n",
            "Guardando checkpoint para el epoch 2 en el directorio ./drive/My Drive/Curso de NLP/BERT/ckpt_bert_squad/ckpt-7\n",
            "Epoch 2 Lote 1050 Pérdida 0.6289\n",
            "Epoch 2 Lote 1100 Pérdida 0.6145\n",
            "Epoch 2 Lote 1150 Pérdida 0.6004\n",
            "Epoch 2 Lote 1200 Pérdida 0.5895\n",
            "Epoch 2 Lote 1250 Pérdida 0.5867\n",
            "Epoch 2 Lote 1300 Pérdida 0.5894\n",
            "Epoch 2 Lote 1350 Pérdida 0.6004\n",
            "Epoch 2 Lote 1400 Pérdida 0.5978\n",
            "Epoch 2 Lote 1450 Pérdida 0.5938\n",
            "Epoch 2 Lote 1500 Pérdida 0.5948\n",
            "Guardando checkpoint para el epoch 2 en el directorio ./drive/My Drive/Curso de NLP/BERT/ckpt_bert_squad/ckpt-8\n",
            "Epoch 2 Lote 1550 Pérdida 0.5932\n",
            "Epoch 2 Lote 1600 Pérdida 0.5958\n",
            "Epoch 2 Lote 1650 Pérdida 0.6041\n",
            "Epoch 2 Lote 1700 Pérdida 0.6120\n",
            "Epoch 2 Lote 1750 Pérdida 0.6117\n",
            "Epoch 2 Lote 1800 Pérdida 0.6166\n",
            "Epoch 2 Lote 1850 Pérdida 0.6114\n",
            "Epoch 2 Lote 1900 Pérdida 0.6073\n",
            "Epoch 2 Lote 1950 Pérdida 0.6167\n",
            "Tiempo total para entrenar 1 epoch: 5323.075678825378 segs\n",
            "\n",
            "Inicio del Epoch 3\n",
            "Epoch 3 Lote 0 Pérdida 1.6725\n",
            "Guardando checkpoint para el epoch 3 en el directorio ./drive/My Drive/Curso de NLP/BERT/ckpt_bert_squad/ckpt-9\n",
            "Epoch 3 Lote 50 Pérdida 1.1375\n",
            "Epoch 3 Lote 100 Pérdida 1.0644\n",
            "Epoch 3 Lote 150 Pérdida 1.0579\n",
            "Epoch 3 Lote 200 Pérdida 1.0375\n",
            "Epoch 3 Lote 250 Pérdida 1.0067\n",
            "Epoch 3 Lote 300 Pérdida 0.9554\n",
            "Epoch 3 Lote 350 Pérdida 0.9430\n",
            "Epoch 3 Lote 400 Pérdida 0.8927\n",
            "Epoch 3 Lote 450 Pérdida 0.8453\n",
            "Epoch 3 Lote 500 Pérdida 0.8164\n",
            "Guardando checkpoint para el epoch 3 en el directorio ./drive/My Drive/Curso de NLP/BERT/ckpt_bert_squad/ckpt-10\n",
            "Epoch 3 Lote 550 Pérdida 0.7860\n",
            "Epoch 3 Lote 600 Pérdida 0.7754\n",
            "Epoch 3 Lote 650 Pérdida 0.7596\n",
            "Epoch 3 Lote 700 Pérdida 0.7369\n",
            "Epoch 3 Lote 750 Pérdida 0.7108\n",
            "Epoch 3 Lote 800 Pérdida 0.6945\n",
            "Epoch 3 Lote 850 Pérdida 0.6831\n",
            "Epoch 3 Lote 900 Pérdida 0.6735\n",
            "Epoch 3 Lote 950 Pérdida 0.6600\n",
            "Epoch 3 Lote 1000 Pérdida 0.6426\n",
            "Guardando checkpoint para el epoch 3 en el directorio ./drive/My Drive/Curso de NLP/BERT/ckpt_bert_squad/ckpt-11\n",
            "Epoch 3 Lote 1050 Pérdida 0.6262\n",
            "Epoch 3 Lote 1100 Pérdida 0.6146\n",
            "Epoch 3 Lote 1150 Pérdida 0.6007\n",
            "Epoch 3 Lote 1200 Pérdida 0.5895\n",
            "Epoch 3 Lote 1250 Pérdida 0.5879\n",
            "Epoch 3 Lote 1300 Pérdida 0.5926\n",
            "Epoch 3 Lote 1350 Pérdida 0.6027\n",
            "Epoch 3 Lote 1400 Pérdida 0.6001\n",
            "Epoch 3 Lote 1450 Pérdida 0.5950\n",
            "Epoch 3 Lote 1500 Pérdida 0.5975\n",
            "Guardando checkpoint para el epoch 3 en el directorio ./drive/My Drive/Curso de NLP/BERT/ckpt_bert_squad/ckpt-12\n",
            "Epoch 3 Lote 1550 Pérdida 0.5964\n",
            "Epoch 3 Lote 1600 Pérdida 0.5967\n",
            "Epoch 3 Lote 1650 Pérdida 0.6051\n",
            "Epoch 3 Lote 1700 Pérdida 0.6109\n",
            "Epoch 3 Lote 1750 Pérdida 0.6126\n",
            "Epoch 3 Lote 1800 Pérdida 0.6146\n",
            "Epoch 3 Lote 1850 Pérdida 0.6134\n",
            "Epoch 3 Lote 1900 Pérdida 0.6089\n",
            "Epoch 3 Lote 1950 Pérdida 0.6163\n",
            "Tiempo total para entrenar 1 epoch: 5372.474010705948 segs\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v6WquTCiIR7t"
      },
      "source": [
        "# Fase 5: Evaluación"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qDIlHd5Tos6C"
      },
      "source": [
        "## Preparación de la evaluación"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kU7_AyTIpTTJ"
      },
      "source": [
        "Get the dev set in the session"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VGKl84s5WrhD"
      },
      "source": [
        "eval_examples = read_squad_examples(\n",
        "    \"/content/drive/My Drive/Curso de NLP/BERT/squad_data/dev-v1.1.json\",\n",
        "    is_training=False,\n",
        "    version_2_with_negative=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DAEUcZDSpYLD"
      },
      "source": [
        "Define the function that will write the tf_record file for the dev set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yCVmIgnEo83o"
      },
      "source": [
        "eval_writer = FeatureWriter(\n",
        "    filename=os.path.join(\"/content/drive/My Drive/Curso de NLP/BERT/squad_data/\",\n",
        "                          \"eval.tf_record\"),\n",
        "    is_training=False)\n",
        "# FeatureWriter: a la hr de llevar a cabo la parte de la evaluación necesitamos contar con features, los\n",
        "# cuales no son más que protocolos que tf utiliza a la hr de trabajar. Son herramientas que se utlizan para\n",
        "# proporcionar la información o almacenar la información. En otras palabras es una convención si \n",
        "# queremos usar tf squad."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C8aSLFdmp71I"
      },
      "source": [
        "Create a tokenizer for future information needs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oH5exQ7KwnuH"
      },
      "source": [
        "my_bert_layer = hub.KerasLayer(\n",
        "    \"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\",\n",
        "    trainable=False)\n",
        "vocab_file = my_bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
        "do_lower_case = my_bert_layer.resolved_object.do_lower_case.numpy()\n",
        "tokenizer = FullTokenizer(vocab_file, do_lower_case)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vdHudjJ_qAzo"
      },
      "source": [
        "Define the function that add the features (feature is a protocol in tensorflow) to our eval_features list"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bmQ5GtoTxRjU"
      },
      "source": [
        "# Como he dicho los features no son más que pedazos de info que siguen un protocolo, que siguen una\n",
        "# convención y por tanto solo lo convierto al formato que me intersa.\n",
        "def _append_feature(feature, is_padding):\n",
        "    if not is_padding:\n",
        "        eval_features.append(feature)\n",
        "    eval_writer.process_feature(feature)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HLAcwCiaqHi_"
      },
      "source": [
        "Create the eval features and the writes the tf.record file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mz7kGYmUwGQb"
      },
      "source": [
        "eval_features = []\n",
        "dataset_size = convert_examples_to_features(\n",
        "    examples=eval_examples,\n",
        "    tokenizer=tokenizer,\n",
        "    max_seq_length=384,     # este valor varia dependiendo los datos\n",
        "    doc_stride=128,\n",
        "    max_query_length=64,\n",
        "    is_training=False,\n",
        "    output_fn=_append_feature,\n",
        "    batch_size=4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WpZfwPEwMabx"
      },
      "source": [
        "eval_writer.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TbKhx3zuq844"
      },
      "source": [
        "Load the ready-to-be-used dataset to our session"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eUqYvG5TxctF"
      },
      "source": [
        "BATCH_SIZE = 4\n",
        "\n",
        "eval_dataset = create_squad_dataset(\n",
        "    \"/content/drive/My Drive/Curso de NLP/BERT/squad_data/eval.tf_record\",\n",
        "    384,#input_meta_data['max_seq_length'],\n",
        "    BATCH_SIZE,\n",
        "    is_training=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tRbKrFYoo8e8"
      },
      "source": [
        "## Llevar a cabo las prediccioness"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KyckEWDbrLEX"
      },
      "source": [
        "Definir un cierto tipo de colección (como un diccionario). Esto nos será util ya que como seguiremos trabajando con lotes en un momento dado, vamos querer crear una función que devuelva los elementos de salida por lote uno detrás de otro. Entonces, para ello nos va ir muy bien que los elementos vayan nombrados."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tyWOUKaDP0H0"
      },
      "source": [
        "# Creamos una tupla nombrado lo que nos permitirá crear tuplas dodne cada elemento ahora tenga un nombre\n",
        "# en lugar de las tuplas estandár a las que se accede por posición.\n",
        "RawResult = collections.namedtuple(\"RawResult\",\n",
        "                                   [\"unique_id\", \"start_logits\", \"end_logits\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28abKVvqrRa4"
      },
      "source": [
        "Devuelve cada elemento del lote de salida, uno por uno. Dado un elemento del lote de salida lo va a iterar en este formato para devolverme empaquetada la información que me interesa. Enotras palabras de la salida del modelos solo me quedaré con unique_ids, start_logits y end_logits. Lo que hace zip es que empaqueta la info en 3 listas con una misma longitud."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BScaA0SZQgQW"
      },
      "source": [
        "def get_raw_results(predictions):\n",
        "    for unique_ids, start_logits, end_logits in zip(predictions['unique_ids'],\n",
        "                                                    predictions['start_logits'],\n",
        "                                                    predictions['end_logits']):\n",
        "        yield RawResult(\n",
        "            unique_id=unique_ids.numpy(),       # lo convertimos a formato numpy para poder manejarlo más comodamente\n",
        "            start_logits=start_logits.numpy().tolist(),\n",
        "            end_logits=end_logits.numpy().tolist())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JiLOOmnLre5C"
      },
      "source": [
        "Hacemos nuestras predicciones"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qqD78Xdjrvpn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 530
        },
        "outputId": "b128075d-bfc1-4df6-852e-f1020bbc404f"
      },
      "source": [
        "all_results = []\n",
        "for count, inputs in enumerate(eval_dataset):\n",
        "    x, _ = inputs  \n",
        "    unique_ids = x.pop(\"unique_ids\")\n",
        "    start_logits, end_logits = bert_squad(x, training=False)\n",
        "    output_dict = dict(\n",
        "        unique_ids=unique_ids,\n",
        "        start_logits=start_logits,\n",
        "        end_logits=end_logits)\n",
        "    for result in get_raw_results(output_dict):\n",
        "        all_results.append(result)\n",
        "    if count % 100 == 0:\n",
        "        print(\"{}/{}\".format(count, 2709))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0/2709\n",
            "100/2709\n",
            "200/2709\n",
            "300/2709\n",
            "400/2709\n",
            "500/2709\n",
            "600/2709\n",
            "700/2709\n",
            "800/2709\n",
            "900/2709\n",
            "1000/2709\n",
            "1100/2709\n",
            "1200/2709\n",
            "1300/2709\n",
            "1400/2709\n",
            "1500/2709\n",
            "1600/2709\n",
            "1700/2709\n",
            "1800/2709\n",
            "1900/2709\n",
            "2000/2709\n",
            "2100/2709\n",
            "2200/2709\n",
            "2300/2709\n",
            "2400/2709\n",
            "2500/2709\n",
            "2600/2709\n",
            "2700/2709\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PjQ6kIqGriHr"
      },
      "source": [
        "Escribimos nuestras predicciones en un fichero JSON que funcionará con el script de evaluación."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "esLdRf7uM3Lz"
      },
      "source": [
        "output_prediction_file = \"/content/drive/My Drive/Curso de NLP/BERT/squad_data/predictions.json\"\n",
        "output_nbest_file = \"/content/drive/My Drive/Curso de NLP/BERT/squad_data/nbest_predictions.json\"\n",
        "output_null_log_odds_file = \"/content/drive/My Drive/Curso de NLP/BERT/squad_data/null_odds.json\"\n",
        "\n",
        "write_predictions(\n",
        "    eval_examples,\n",
        "    eval_features,\n",
        "    all_results,\n",
        "    20,\n",
        "    30,\n",
        "    True,\n",
        "    output_prediction_file,\n",
        "    output_nbest_file,\n",
        "    output_null_log_odds_file,\n",
        "    verbose=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-eaIHyDIYHHx"
      },
      "source": [
        "## Predicción casera"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P0F4l5h8Zdha"
      },
      "source": [
        "### Creación del diccionario de entrada"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IrhHzx7ycXXo"
      },
      "source": [
        "Concatenamos la pregunta y el contexto, separados por `[\"SEP\"]`, tras la tokenización, tal cual como lo hicimos con el conjunto de entrenamiento.\n",
        "\n",
        "Lo importante a recordar es que queremos que nuestra respuesta empiece y termine con una palabra real. Por ejemplo, la palabra \"ecologically\" es tokenizada como `[\"ecological\", \"##ly\"]`, y si el token de fin es `[\"ecological\"]` queremos usar la palabra \"ecologically\" como palabra final (del mismo modo si el token de fin es`[\"##ly\"]`). Por eso, empezamos dividiendo nuestro contexto en palabras, y luego pasamos a tokens, recordando qué token se corresponde con qué palabra (ver la función `tokenize_context()` para más detalle)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_tuNXt98Zm4u"
      },
      "source": [
        "#### Útiles varios"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OBjGoQ_wfmml"
      },
      "source": [
        "my_bert_layer = hub.KerasLayer(\n",
        "    \"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\",\n",
        "    trainable=False)\n",
        "vocab_file = my_bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
        "do_lower_case = my_bert_layer.resolved_object.do_lower_case.numpy()\n",
        "tokenizer = FullTokenizer(vocab_file, do_lower_case)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8f_fCe_hLC12"
      },
      "source": [
        "def is_whitespace(c):\n",
        "    '''\n",
        "    Indica si un cadena de caracteres se corresponde con un espacio en blanco / separador o no.\n",
        "    '''\n",
        "    if c == \" \" or c == \"\\t\" or c == \"\\r\" or c == \"\\n\" or ord(c) == 0x202F:\n",
        "        return True\n",
        "    return False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ZA4TYnxLGVT"
      },
      "source": [
        "def whitespace_split(text):\n",
        "    '''\n",
        "    Toma el texto y devuelve una lista de \"palabras\" separadas segun los \n",
        "    espacios en blanco / separadores anteriores.\n",
        "    '''\n",
        "    doc_tokens = []\n",
        "    prev_is_whitespace = True\n",
        "    for c in text:\n",
        "        if is_whitespace(c):\n",
        "            prev_is_whitespace = True\n",
        "        else:\n",
        "            if prev_is_whitespace:\n",
        "                doc_tokens.append(c)\n",
        "            else:\n",
        "                doc_tokens[-1] += c\n",
        "            prev_is_whitespace = False\n",
        "    return doc_tokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fsfzt3GUNWQK"
      },
      "source": [
        "def tokenize_context(text_words):\n",
        "    '''\n",
        "    Toma una lista de palabras (devueltas por whitespace_split()) y tokeniza cada\n",
        "    palabra una por una. También almacena, para cada nuevo token, la palabra original\n",
        "    del parámetro text_words.\n",
        "    '''\n",
        "    text_tok = []\n",
        "    tok_to_word_id = []\n",
        "    for word_id, word in enumerate(text_words):\n",
        "        word_tok = tokenizer.tokenize(word)\n",
        "        text_tok += word_tok\n",
        "        tok_to_word_id += [word_id]*len(word_tok)\n",
        "    return text_tok, tok_to_word_id"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c8qreqEURUOP"
      },
      "source": [
        "def get_ids(tokens):\n",
        "    return tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "def get_mask(tokens):\n",
        "    return np.char.not_equal(tokens, \"[PAD]\").astype(int)\n",
        "\n",
        "def get_segments(tokens):\n",
        "    seg_ids = []\n",
        "    current_seg_id = 0\n",
        "    for tok in tokens:\n",
        "        seg_ids.append(current_seg_id)\n",
        "        if tok == \"[SEP]\":\n",
        "            current_seg_id = 1-current_seg_id # Convierte 1 en 0 y viceversa\n",
        "    return seg_ids"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P2sPGXxsYUsY"
      },
      "source": [
        "def create_input_dict(question, context):\n",
        "    '''\n",
        "    Take a question and a context as strings and return a dictionary with the 3\n",
        "    elements needed for the model. Also return the context_words, the\n",
        "    context_tok to context_word ids correspondance and the length of\n",
        "    question_tok that we will need later.\n",
        "    '''\n",
        "    question_tok = tokenizer.tokenize(my_question)\n",
        "\n",
        "    context_words = whitespace_split(context)\n",
        "    context_tok, context_tok_to_word_id = tokenize_context(context_words)\n",
        "\n",
        "    input_tok = question_tok + [\"[SEP]\"] + context_tok + [\"[SEP]\"]\n",
        "    input_tok += [\"[PAD]\"]*(384-len(input_tok)) # in our case the model has been\n",
        "                                                # trained to have inputs of length max 384\n",
        "    input_dict = {}\n",
        "    input_dict[\"input_word_ids\"] = tf.expand_dims(tf.cast(get_ids(input_tok), tf.int32), 0)\n",
        "    input_dict[\"input_mask\"] = tf.expand_dims(tf.cast(get_mask(input_tok), tf.int32), 0)\n",
        "    input_dict[\"input_type_ids\"] = tf.expand_dims(tf.cast(get_segments(input_tok), tf.int32), 0)\n",
        "\n",
        "    return input_dict, context_words, context_tok_to_word_id, len(question_tok)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QAnaCZWTZpWT"
      },
      "source": [
        "#### Creación"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WQlM-M8rMklA"
      },
      "source": [
        "my_context = '''Neoclassical economics views inequalities in the distribution of income as arising from differences in value added by labor, capital and land. Within labor income distribution is due to differences in value added by different classifications of workers. In this perspective, wages and profits are determined by the marginal value added of each economic actor (worker, capitalist/business owner, landlord). Thus, in a market economy, inequality is a reflection of the productivity gap between highly-paid professions and lower-paid professions.'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YL_cE-o0U8mx"
      },
      "source": [
        "Neoclassical economics views inequalities in the distribution of income as arising from differences in value added by labor, capital and land. Within labor income distribution is due to differences in value added by different classifications of workers. In this perspective, wages and profits are determined by the marginal value added of each economic actor (worker, capitalist/business owner, landlord). Thus, in a market economy, inequality is a reflection of the productivity gap between highly-paid professions and lower-paid professions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SeB29SCNNQ1M"
      },
      "source": [
        "#my_question = '''What philosophy of thought addresses wealth inequality?'''\n",
        "my_question = '''What are examples of economic actors?'''\n",
        "#my_question = '''In a market economy, what is inequality a reflection of?'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-v5nLMNjZufe"
      },
      "source": [
        "my_input_dict, my_context_words, context_tok_to_word_id, question_tok_len = create_input_dict(my_question, my_context)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dT066rMtZ65X"
      },
      "source": [
        "### Predicción"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CcJgDa9gVShl"
      },
      "source": [
        "start_logits, end_logits = bert_squad(my_input_dict, training=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rhdGlIo5Z9IZ"
      },
      "source": [
        "### Interpretación"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZQJMBkVLd9wp"
      },
      "source": [
        "We remove the ids corresponding to the question and the `[\"SEP\"]` token:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TfwBJfsSTwRn"
      },
      "source": [
        "start_logits_context = start_logits.numpy()[0, question_tok_len+1:]\n",
        "end_logits_context = end_logits.numpy()[0, question_tok_len+1:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "te1u6iZAawYf"
      },
      "source": [
        "First easy interpretation:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lQJ8tp-1WvI4"
      },
      "source": [
        "start_word_id = context_tok_to_word_id[np.argmax(start_logits_context)]\n",
        "end_word_id = context_tok_to_word_id[np.argmax(end_logits_context)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k3PHA84rarse"
      },
      "source": [
        "\"Advanced\" - making sure that the start of the answer is before the end:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xiFZ2fUiRU_M"
      },
      "source": [
        "pair_scores = np.ones((len(start_logits_context), len(end_logits_context)))*(-1E10)\n",
        "for i in range(len(start_logits_context-1)):\n",
        "    for j in range(i, len(end_logits_context)):\n",
        "        pair_scores[i, j] = start_logits_context[i] + end_logits_context[j]\n",
        "pair_scores_argmax = np.argmax(pair_scores)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N9KEiFHPXXeM"
      },
      "source": [
        "start_word_id = context_tok_to_word_id[pair_scores_argmax // len(start_logits_context)]\n",
        "end_word_id = context_tok_to_word_id[pair_scores_argmax % len(end_logits_context)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kJDBL8KKa6NP"
      },
      "source": [
        "Final answer:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j0Y3WDz0XwAw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ff600bb-528d-4b14-f800-731aa4448e9a"
      },
      "source": [
        "predicted_answer = ' '.join(my_context_words[start_word_id:end_word_id+1])\n",
        "print(\"The answer to:\\n\" + my_question + \"\\nis:\\n\" + predicted_answer)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The answer to:\n",
            "What are examples of economic actors?\n",
            "is:\n",
            "(worker, capitalist/business owner, landlord).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bYGSk_5OSYUk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 161
        },
        "outputId": "dbd02f58-62e6-45c8-8ddd-b2b6989a2385"
      },
      "source": [
        "from IPython.core.display import HTML\n",
        "display(HTML(f'<h2>{my_question.upper()}</h2>'))\n",
        "marked_text = str(my_context.replace(predicted_answer, f\"<mark>{predicted_answer}</mark>\"))\n",
        "display(HTML(f\"\"\"<blockquote> {marked_text} </blockquote>\"\"\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<h2>WHAT ARE EXAMPLES OF ECONOMIC ACTORS?</h2>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<blockquote> Neoclassical economics views inequalities in the distribution of income as arising from differences in value added by labor, capital and land. Within labor income distribution is due to differences in value added by different classifications of workers. In this perspective, wages and profits are determined by the marginal value added of each economic actor <mark>(worker, capitalist/business owner, landlord).</mark> Thus, in a market economy, inequality is a reflection of the productivity gap between highly-paid professions and lower-paid professions. </blockquote>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GAXMJwopGRzM"
      },
      "source": [
        "#### Reto Final"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Brg0AXkA0r74",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 196
        },
        "outputId": "27b814f9-65bf-4ae0-90d7-c246b6363ba6"
      },
      "source": [
        "my_context = '''\n",
        "Coronavirus disease 2019 is an infectious disease caused by severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2). It was first identified in December 2019 in Wuhan, Hubei, China, and has resulted in an ongoing pandemic.\n",
        "Common symptoms include fever, cough, fatigue, shortness of breath, and loss of smell and taste.While most people have mild symptoms, some people develop acute respiratory distress syndrome (ARDS) possibly precipitated by cytokine storm, multi-organ failure, septic shock, and blood clots. The time from exposure to onset of symptoms is typically around five days, but may range from two to fourteen days.\n",
        "The virus is spread primarily via nose and mouth secretions including small droplets produced by coughing,[a] sneezing, and talking. The droplets usually do not travel through air over long distances. However, those standing in close proximity may inhale these droplets and become infected.[b] People may also become infected by touching a contaminated surface and then touching their face. The transmission may also occur through smaller droplets that are able to stay suspended in the air for longer periods of time in enclosed spaces.'''\n",
        "\n",
        "my_question = '''What are the common symptoms of the disease?'''\n",
        "\n",
        "my_input_dict, my_context_words, context_tok_to_word_id, question_tok_len = create_input_dict(my_question, my_context)\n",
        "\n",
        "start_logits, end_logits = bert_squad(my_input_dict, training=False)\n",
        "\n",
        "pair_scores = np.ones((len(start_logits_context), len(end_logits_context)))*(-1E10)\n",
        "for i in range(len(start_logits_context-1)):\n",
        "    for j in range(i, len(end_logits_context)):\n",
        "        pair_scores[i, j] = start_logits_context[i] + end_logits_context[j]\n",
        "pair_scores_argmax = np.argmax(pair_scores)\n",
        "\n",
        "start_word_id = context_tok_to_word_id[pair_scores_argmax // len(start_logits_context)]\n",
        "end_word_id = context_tok_to_word_id[pair_scores_argmax % len(end_logits_context)]\n",
        "\n",
        "predicted_answer = ' '.join(my_context_words[start_word_id:end_word_id+1])\n",
        "\n",
        "\n",
        "from IPython.core.display import HTML\n",
        "display(HTML(f'<h2>{my_question.upper()}</h2>'))\n",
        "marked_text = str(my_context.replace(predicted_answer, f\"<mark>{predicted_answer}</mark>\"))\n",
        "display(HTML(f\"\"\"<blockquote> {marked_text} </blockquote>\"\"\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<h2>WHAT ARE THE COMMON SYMPTOMS OF THE DISEASE?</h2>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<blockquote> \n",
              "Coronavirus disease 2019 is an infectious disease caused by severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2). It was first identified in December 2019 in Wuhan, Hubei, China, and has resulted in an ongoing pandemic.\n",
              "Common symptoms include fever, cough, fatigue, <mark>shortness of breath, and loss of</mark> smell and taste.While most people have mild symptoms, some people develop acute respiratory distress syndrome (ARDS) possibly precipitated by cytokine storm, multi-organ failure, septic shock, and blood clots. The time from exposure to onset of symptoms is typically around five days, but may range from two to fourteen days.\n",
              "The virus is spread primarily via nose and mouth secretions including small droplets produced by coughing,[a] sneezing, and talking. The droplets usually do not travel through air over long distances. However, those standing in close proximity may inhale these droplets and become infected.[b] People may also become infected by touching a contaminated surface and then touching their face. The transmission may also occur through smaller droplets that are able to stay suspended in the air for longer periods of time in enclosed spaces. </blockquote>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_fTt1zl8Fbom"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}