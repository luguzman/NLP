{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT_squad",
      "provenance": [],
      "collapsed_sections": [
        "qDIlHd5Tos6C"
      ],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/luguzman/NLP/blob/main/BERT_squad.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aAUq7duyG5J0"
      },
      "source": [
        "# Fase 1: Importar dependencias"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9XYlThAmGZg1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98aa566f-b339-42c1-cefb-3da80c0773d4"
      },
      "source": [
        "!pip install sentencepiece\n",
        "#!pip install tf-models-official\n",
        "!pip install tf-models-nightly # mejor instalar la versión en desarrollo\n",
        "!pip install tf-nightly"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/67/e42bd1181472c95c8cda79305df848264f2a7f62740995a46945d9797b67/sentencepiece-0.1.95-cp36-cp36m-manylinux2014_x86_64.whl (1.2MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2MB 6.4MB/s \n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.95\n",
            "Collecting tf-models-nightly\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1c/1b/1e14aed5ae2c09d10de7388d7960f4b94f3a3e24aeb689e49525b0a799f5/tf_models_nightly-2.4.0.dev20210211-py2.py3-none-any.whl (1.5MB)\n",
            "\u001b[K     |████████████████████████████████| 1.5MB 6.6MB/s \n",
            "\u001b[?25hCollecting tf-nightly\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/62/df/b8c820b98ba6536769224a0313daa26db894ef854be14b904db7764afe07/tf_nightly-2.5.0.dev20210211-cp36-cp36m-manylinux2010_x86_64.whl (407.8MB)\n",
            "\u001b[K     |████████████████████████████████| 407.8MB 32kB/s \n",
            "\u001b[?25hRequirement already satisfied: gin-config in /usr/local/lib/python3.6/dist-packages (from tf-models-nightly) (0.4.0)\n",
            "Requirement already satisfied: kaggle>=1.3.9 in /usr/local/lib/python3.6/dist-packages (from tf-models-nightly) (1.5.10)\n",
            "Requirement already satisfied: tensorflow-hub>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tf-models-nightly) (0.11.0)\n",
            "Collecting sacrebleu\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3b/7f/4fd83db8570288c3899d8e57666c2841403c15659f3d792a3cb8dc1c6689/sacrebleu-1.5.0-py3-none-any.whl (65kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 7.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.6/dist-packages (from tf-models-nightly) (1.19.5)\n",
            "Collecting pyyaml>=5.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7a/5b/bc0b5ab38247bba158504a410112b6c03f153c652734ece1849749e5f518/PyYAML-5.4.1-cp36-cp36m-manylinux1_x86_64.whl (640kB)\n",
            "\u001b[K     |████████████████████████████████| 645kB 55.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas>=0.22.0 in /usr/local/lib/python3.6/dist-packages (from tf-models-nightly) (1.1.5)\n",
            "Requirement already satisfied: tensorflow-datasets in /usr/local/lib/python3.6/dist-packages (from tf-models-nightly) (4.0.1)\n",
            "Requirement already satisfied: psutil>=5.4.3 in /usr/local/lib/python3.6/dist-packages (from tf-models-nightly) (5.4.8)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from tf-models-nightly) (0.8)\n",
            "Requirement already satisfied: google-api-python-client>=1.6.7 in /usr/local/lib/python3.6/dist-packages (from tf-models-nightly) (1.7.12)\n",
            "Collecting tensorflow-text-nightly\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fa/eb/516e3f286c273350b857f358fbd426ced83ce8f25f5a7b136b3246b18320/tensorflow_text_nightly-2.5.0.dev20210211-cp36-cp36m-manylinux1_x86_64.whl (4.4MB)\n",
            "\u001b[K     |████████████████████████████████| 4.4MB 58.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: pycocotools in /usr/local/lib/python3.6/dist-packages (from tf-models-nightly) (2.0.2)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from tf-models-nightly) (0.1.95)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.6/dist-packages (from tf-models-nightly) (7.0.0)\n",
            "Requirement already satisfied: Cython in /usr/local/lib/python3.6/dist-packages (from tf-models-nightly) (0.29.21)\n",
            "Collecting tensorflow-model-optimization>=0.4.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/55/38/4fd48ea1bfcb0b6e36d949025200426fe9c3a8bfae029f0973d85518fa5a/tensorflow_model_optimization-0.5.0-py2.py3-none-any.whl (172kB)\n",
            "\u001b[K     |████████████████████████████████| 174kB 47.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from tf-models-nightly) (1.15.0)\n",
            "Requirement already satisfied: google-cloud-bigquery>=0.31.0 in /usr/local/lib/python3.6/dist-packages (from tf-models-nightly) (1.21.0)\n",
            "Collecting seqeval\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9d/2d/233c79d5b4e5ab1dbf111242299153f3caddddbb691219f363ad55ce783d/seqeval-1.2.2.tar.gz (43kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 5.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from tf-models-nightly) (3.2.2)\n",
            "Collecting opencv-python-headless\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/96/fc/4da675cc522a749ebbcf85c5a63fba844b2d44c87e6f24e3fdb147df3270/opencv_python_headless-4.5.1.48-cp36-cp36m-manylinux2014_x86_64.whl (37.6MB)\n",
            "\u001b[K     |████████████████████████████████| 37.6MB 124kB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.6/dist-packages (from tf-models-nightly) (1.4.1)\n",
            "Collecting py-cpuinfo>=3.3.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f6/f5/8e6e85ce2e9f6e05040cf0d4e26f43a4718bcc4bce988b433276d4b1a5c1/py-cpuinfo-7.0.0.tar.gz (95kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 8.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: oauth2client in /usr/local/lib/python3.6/dist-packages (from tf-models-nightly) (4.1.3)\n",
            "Collecting tensorflow-addons\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2e/af/0ce633c373d2b0476ef8299673d22275fcc3c5ba283b2cec4aa06bc5b810/tensorflow_addons-0.12.1-cp36-cp36m-manylinux2010_x86_64.whl (703kB)\n",
            "\u001b[K     |████████████████████████████████| 706kB 54.0MB/s \n",
            "\u001b[?25hCollecting tf-slim>=1.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/02/97/b0f4a64df018ca018cc035d44f2ef08f91e2e8aa67271f6f19633a015ff7/tf_slim-1.1.0-py2.py3-none-any.whl (352kB)\n",
            "\u001b[K     |████████████████████████████████| 358kB 58.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.6/dist-packages (from tf-nightly->tf-models-nightly) (0.10.0)\n",
            "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly->tf-models-nightly) (3.3.0)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.6/dist-packages (from tf-nightly->tf-models-nightly) (1.1.2)\n",
            "Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly->tf-models-nightly) (1.12)\n",
            "Collecting grpcio~=1.34.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ab/f5/3d3bcb82beae990021cbf6877456a1aab650e68c902194566edd6a73e37c/grpcio-1.34.1-cp36-cp36m-manylinux2014_x86_64.whl (4.0MB)\n",
            "\u001b[K     |████████████████████████████████| 4.0MB 50.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.6/dist-packages (from tf-nightly->tf-models-nightly) (0.2.0)\n",
            "Collecting tf-estimator-nightly~=2.5.0.dev\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c8/3b/8df7c08029be851b2f3ce628d0d690c975ba316028280d4abd61e1d16685/tf_estimator_nightly-2.5.0.dev2021021101-py2.py3-none-any.whl (462kB)\n",
            "\u001b[K     |████████████████████████████████| 471kB 54.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly->tf-models-nightly) (1.1.0)\n",
            "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.6/dist-packages (from tf-nightly->tf-models-nightly) (0.36.2)\n",
            "Collecting gast==0.4.0\n",
            "  Downloading https://files.pythonhosted.org/packages/b6/48/583c032b79ae5b3daa02225a675aeb673e58d2cb698e78510feceb11958c/gast-0.4.0-py3-none-any.whl\n",
            "Collecting tb-nightly~=2.5.0.a\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/df/f9/402df77f491813eddcd217ea021bd10e84daca8a12c7845dcfddd77f52a7/tb_nightly-2.5.0a20210211-py3-none-any.whl (5.9MB)\n",
            "\u001b[K     |████████████████████████████████| 5.9MB 52.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.6/dist-packages (from tf-nightly->tf-models-nightly) (3.7.4.3)\n",
            "Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.6/dist-packages (from tf-nightly->tf-models-nightly) (1.12.1)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tf-nightly->tf-models-nightly) (1.6.3)\n",
            "Collecting h5py~=3.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/70/7a/e53e500335afb6b1aade11227cdf107fca54106a1dca5c9d13242a043f3b/h5py-3.1.0-cp36-cp36m-manylinux1_x86_64.whl (4.0MB)\n",
            "\u001b[K     |████████████████████████████████| 4.0MB 58.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.6/dist-packages (from tf-nightly->tf-models-nightly) (3.12.4)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.6/dist-packages (from kaggle>=1.3.9->tf-models-nightly) (1.24.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from kaggle>=1.3.9->tf-models-nightly) (4.41.1)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.6/dist-packages (from kaggle>=1.3.9->tf-models-nightly) (4.0.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.6/dist-packages (from kaggle>=1.3.9->tf-models-nightly) (2020.12.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from kaggle>=1.3.9->tf-models-nightly) (2.23.0)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.6/dist-packages (from kaggle>=1.3.9->tf-models-nightly) (2.8.1)\n",
            "Collecting portalocker\n",
            "  Downloading https://files.pythonhosted.org/packages/82/22/e684c9e2e59b561dbe36538852e81849122c666c423448e3a5c99362c228/portalocker-2.2.1-py2.py3-none-any.whl\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.22.0->tf-models-nightly) (2018.9)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->tf-models-nightly) (0.3.3)\n",
            "Requirement already satisfied: importlib-resources; python_version < \"3.9\" in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->tf-models-nightly) (5.1.0)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->tf-models-nightly) (2.3)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->tf-models-nightly) (0.16.0)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->tf-models-nightly) (0.27.0)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->tf-models-nightly) (0.1.5)\n",
            "Requirement already satisfied: attrs>=18.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->tf-models-nightly) (20.3.0)\n",
            "Requirement already satisfied: httplib2<1dev,>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.6.7->tf-models-nightly) (0.17.4)\n",
            "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.6.7->tf-models-nightly) (3.0.1)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.6.7->tf-models-nightly) (0.0.4)\n",
            "Requirement already satisfied: google-auth>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.6.7->tf-models-nightly) (1.25.0)\n",
            "Requirement already satisfied: setuptools>=18.0 in /usr/local/lib/python3.6/dist-packages (from pycocotools->tf-models-nightly) (53.0.0)\n",
            "Requirement already satisfied: google-resumable-media!=0.4.0,<0.5.0dev,>=0.3.1 in /usr/local/lib/python3.6/dist-packages (from google-cloud-bigquery>=0.31.0->tf-models-nightly) (0.4.1)\n",
            "Requirement already satisfied: google-cloud-core<2.0dev,>=1.0.3 in /usr/local/lib/python3.6/dist-packages (from google-cloud-bigquery>=0.31.0->tf-models-nightly) (1.0.3)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.6/dist-packages (from seqeval->tf-models-nightly) (0.22.2.post1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->tf-models-nightly) (0.10.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->tf-models-nightly) (2.4.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->tf-models-nightly) (1.3.1)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.6/dist-packages (from oauth2client->tf-models-nightly) (0.4.8)\n",
            "Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.6/dist-packages (from oauth2client->tf-models-nightly) (0.2.8)\n",
            "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from oauth2client->tf-models-nightly) (4.7)\n",
            "Requirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.6/dist-packages (from tensorflow-addons->tf-models-nightly) (2.7.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tb-nightly~=2.5.0.a->tf-nightly->tf-models-nightly) (1.0.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tb-nightly~=2.5.0.a->tf-nightly->tf-models-nightly) (1.8.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tb-nightly~=2.5.0.a->tf-nightly->tf-models-nightly) (0.4.2)\n",
            "Collecting tensorboard-data-server<0.4.0,>=0.3.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f2/6f/a1a04f6aece9cdabe80fba01241e76172380753801e34ad50d482fbde0c0/tensorboard_data_server-0.3.0-py3-none-manylinux2010_x86_64.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 52.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tb-nightly~=2.5.0.a->tf-nightly->tf-models-nightly) (3.3.3)\n",
            "Collecting cached-property; python_version < \"3.8\"\n",
            "  Downloading https://files.pythonhosted.org/packages/48/19/f2090f7dad41e225c7f2326e4cfe6fff49e57dedb5b53636c9551f86b069/cached_property-1.5.2-py2.py3-none-any.whl\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.6/dist-packages (from python-slugify->kaggle>=1.3.9->tf-models-nightly) (1.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->kaggle>=1.3.9->tf-models-nightly) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->kaggle>=1.3.9->tf-models-nightly) (2.10)\n",
            "Requirement already satisfied: zipp>=0.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-resources; python_version < \"3.9\"->tensorflow-datasets->tf-models-nightly) (3.4.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-metadata->tensorflow-datasets->tf-models-nightly) (1.52.0)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.4.1->google-api-python-client>=1.6.7->tf-models-nightly) (4.2.1)\n",
            "Requirement already satisfied: google-api-core<2.0.0dev,>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from google-cloud-core<2.0dev,>=1.0.3->google-cloud-bigquery>=0.31.0->tf-models-nightly) (1.16.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.21.3->seqeval->tf-models-nightly) (1.0.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tb-nightly~=2.5.0.a->tf-nightly->tf-models-nightly) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tb-nightly~=2.5.0.a->tf-nightly->tf-models-nightly) (3.4.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tb-nightly~=2.5.0.a->tf-nightly->tf-models-nightly) (3.1.0)\n",
            "Building wheels for collected packages: seqeval, py-cpuinfo\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for seqeval: filename=seqeval-1.2.2-cp36-none-any.whl size=16171 sha256=25000b8bb84a8c1f213d4e5f07633defc6eac8d097d348aedd2e6e6382d8d090\n",
            "  Stored in directory: /root/.cache/pip/wheels/52/df/1b/45d75646c37428f7e626214704a0e35bd3cfc32eda37e59e5f\n",
            "  Building wheel for py-cpuinfo (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for py-cpuinfo: filename=py_cpuinfo-7.0.0-cp36-none-any.whl size=20072 sha256=4ebf5fa8b50d29ae61d7526a9e91577e5e3a937e81b9b2d2c1cbd26b31cda119\n",
            "  Stored in directory: /root/.cache/pip/wheels/f1/93/7b/127daf0c3a5a49feb2fecd468d508067c733fba5192f726ad1\n",
            "Successfully built seqeval py-cpuinfo\n",
            "\u001b[31mERROR: tensorflow 2.4.1 has requirement gast==0.3.3, but you'll have gast 0.4.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow 2.4.1 has requirement grpcio~=1.32.0, but you'll have grpcio 1.34.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow 2.4.1 has requirement h5py~=2.10.0, but you'll have h5py 3.1.0 which is incompatible.\u001b[0m\n",
            "Installing collected packages: grpcio, tf-estimator-nightly, gast, tensorboard-data-server, tb-nightly, cached-property, h5py, tf-nightly, portalocker, sacrebleu, pyyaml, tensorflow-text-nightly, tensorflow-model-optimization, seqeval, opencv-python-headless, py-cpuinfo, tensorflow-addons, tf-slim, tf-models-nightly\n",
            "  Found existing installation: grpcio 1.32.0\n",
            "    Uninstalling grpcio-1.32.0:\n",
            "      Successfully uninstalled grpcio-1.32.0\n",
            "  Found existing installation: gast 0.3.3\n",
            "    Uninstalling gast-0.3.3:\n",
            "      Successfully uninstalled gast-0.3.3\n",
            "  Found existing installation: h5py 2.10.0\n",
            "    Uninstalling h5py-2.10.0:\n",
            "      Successfully uninstalled h5py-2.10.0\n",
            "  Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed cached-property-1.5.2 gast-0.4.0 grpcio-1.34.1 h5py-3.1.0 opencv-python-headless-4.5.1.48 portalocker-2.2.1 py-cpuinfo-7.0.0 pyyaml-5.4.1 sacrebleu-1.5.0 seqeval-1.2.2 tb-nightly-2.5.0a20210211 tensorboard-data-server-0.3.0 tensorflow-addons-0.12.1 tensorflow-model-optimization-0.5.0 tensorflow-text-nightly-2.5.0.dev20210211 tf-estimator-nightly-2.5.0.dev2021021101 tf-models-nightly-2.4.0.dev20210211 tf-nightly-2.5.0.dev20210211 tf-slim-1.1.0\n",
            "Requirement already satisfied: tf-nightly in /usr/local/lib/python3.6/dist-packages (2.5.0.dev20210211)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (1.1.2)\n",
            "Requirement already satisfied: grpcio~=1.34.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (1.34.1)\n",
            "Requirement already satisfied: tb-nightly~=2.5.0.a in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (2.5.0a20210211)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (3.12.4)\n",
            "Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (0.10.0)\n",
            "Requirement already satisfied: numpy~=1.19.2 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (1.19.5)\n",
            "Requirement already satisfied: six~=1.15.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (1.15.0)\n",
            "Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (3.7.4.3)\n",
            "Requirement already satisfied: tf-estimator-nightly~=2.5.0.dev in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (2.5.0.dev2021021101)\n",
            "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (3.3.0)\n",
            "Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (1.12.1)\n",
            "Requirement already satisfied: gast==0.4.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (0.4.0)\n",
            "Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (1.12)\n",
            "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (0.36.2)\n",
            "Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (1.1.0)\n",
            "Requirement already satisfied: h5py~=3.1.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (3.1.0)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (1.6.3)\n",
            "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (0.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tb-nightly~=2.5.0.a->tf-nightly) (3.3.3)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tb-nightly~=2.5.0.a->tf-nightly) (1.25.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tb-nightly~=2.5.0.a->tf-nightly) (53.0.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tb-nightly~=2.5.0.a->tf-nightly) (1.8.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tb-nightly~=2.5.0.a->tf-nightly) (0.4.2)\n",
            "Requirement already satisfied: tensorboard-data-server<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from tb-nightly~=2.5.0.a->tf-nightly) (0.3.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tb-nightly~=2.5.0.a->tf-nightly) (1.0.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tb-nightly~=2.5.0.a->tf-nightly) (2.23.0)\n",
            "Requirement already satisfied: cached-property; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from h5py~=3.1.0->tf-nightly) (1.5.2)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tb-nightly~=2.5.0.a->tf-nightly) (3.4.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tb-nightly~=2.5.0.a->tf-nightly) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tb-nightly~=2.5.0.a->tf-nightly) (4.2.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tb-nightly~=2.5.0.a->tf-nightly) (4.7)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tb-nightly~=2.5.0.a->tf-nightly) (1.3.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tb-nightly~=2.5.0.a->tf-nightly) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tb-nightly~=2.5.0.a->tf-nightly) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tb-nightly~=2.5.0.a->tf-nightly) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tb-nightly~=2.5.0.a->tf-nightly) (2.10)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tb-nightly~=2.5.0.a->tf-nightly) (3.4.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tb-nightly~=2.5.0.a->tf-nightly) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tb-nightly~=2.5.0.a->tf-nightly) (3.1.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eDdYqvxPHnI8"
      },
      "source": [
        "import tensorflow as tf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zFS9XSASHvQi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 57
        },
        "outputId": "f9859e9f-5200-4999-ed0d-026c3f45c3cd"
      },
      "source": [
        "tf.__version__"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2.4.0-dev20200822'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fv38eJzSH00S"
      },
      "source": [
        "import tensorflow_hub as hub        # librería que nos ayuda a importar modelos con pesos ya almacenados\n",
        "\n",
        "from official.nlp.bert.tokenization import FullTokenizer        # Es doonde esta todo lo del módulo de desarrollo de tensorflow\n",
        "from official.nlp.bert.input_pipeline import create_squad_dataset    # Nos va a peritir crear el dataset de squad a partir de importar esos ficheros que descargue y coloque en mi google drive\n",
        "from official.nlp.data.squad_lib import generate_tf_record_from_json_file   # Creará un fichero intermedio a partir del dataset, que es el llamada tf_record\n",
        "\n",
        "from official.nlp import optimization   # Mejora del optimizador Adam que encaja perfecto con BERT\n",
        "\n",
        "from official.nlp.data.squad_lib import read_squad_examples     # Basicamente es para leer los ficheros de evaluación y que podamos ver que también ha sido nuestro modole para resolver los problemas de squad\n",
        "from official.nlp.data.squad_lib import FeatureWriter           # Herramienta que nos ayudará a tarabajar la salida de BERT y validarlo como parte del proceso final \n",
        "from official.nlp.data.squad_lib import convert_examples_to_features    # Herramienta que nos ayudará a tarabajar la salida de BERT y validarlo como parte del proceso final\n",
        "from official.nlp.data.squad_lib import write_predictions       # nos permitirá escribir las predicciones en un formato json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WjzrCZJMJN1N"
      },
      "source": [
        "import numpy as np\n",
        "import math\n",
        "import random\n",
        "import time\n",
        "import json\n",
        "import collections\n",
        "import os\n",
        "\n",
        "from google.colab import drive"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iSGw1I_zHAb5"
      },
      "source": [
        "# Fase 2: Preprocesado de Datos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SfdTEReCKFky",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "82d941bc-9989-4350-d331-ad08e0f2eaec"
      },
      "source": [
        "drive.mount(\"/content/drive\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E_PcOeemKcYq"
      },
      "source": [
        "# Creamos un fichero  json intermedio que contendra el conjunto de entrenamiento junto con el vocabulario \n",
        "input_meta_data = generate_tf_record_from_json_file(\n",
        "    \"/content/drive/My Drive/Curso de NLP/BERT/squad_data/train-v1.1.json\", # Acceso al fichero de entrenamiento\n",
        "    \"/content/drive/My Drive/Curso de NLP/BERT/squad_data/vocab.txt\",       # La ruta al fichero de vocabulario \n",
        "    \"/content/drive/My Drive/Curso de NLP/BERT/squad_data/train-v1.1.tf_record\") # Indicamos el path donde queremos que se guarde este fichero tensorflow record"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7XQGdnANLziM"
      },
      "source": [
        "# Almacenamos los metadatos para entrenar \n",
        "with tf.io.gfile.GFile(\"/content/drive/My Drive/Curso de NLP/BERT/squad_data/train_meta_data\", \"w\") as writer:\n",
        "    writer.write(json.dumps(input_meta_data, indent=4) + \"\\n\") \n",
        "# \\n: es simplemente para no tener problemas de lectura pues los ficheros deberían acabar siempre con un intro al final  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "295nG2zQMYSU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c4465981-dde3-46d3-c2f1-ee873caa2f32"
      },
      "source": [
        "# Creamos el dataset a partir de todos estos datos volcados, de estos tf_records\n",
        "BATCH_SIZE = 4  # Notemos que aquí tenemos un pequeño problema y es que al crear este dataset yo no puedo indicar un tamaño de lote demasiado\n",
        "# grande y es que aparentemente Google dice que utilizar un un batch_size demasiado grande sería bastante complicado. La razón es que las entradas\n",
        "# son mucho más grandes, mucho más largas que las que hemos utilizado en formato de tweet. Recordemos que un tweet caben 140 caracteres. Aquí\n",
        "# podriamos tener frases realmente largas para la tarea de preguntas y respuestas. Recordemos también que la primera entrada es todo un texto, \n",
        "# podría ser la página entra de un libro que contenga información de un tema.\n",
        "\n",
        "train_dataset = create_squad_dataset(\n",
        "    \"/content/drive/My Drive/Curso de NLP/BERT/squad_data/train-v1.1.tf_record\",\n",
        "    input_meta_data['max_seq_length'], # sequencia más larga con la que nos encontramos que en este caso es de 384\n",
        "    BATCH_SIZE,\n",
        "    is_training=True)  # A la hr de crear el conjunto squad el de entrenamiento y el de testing es diferente"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:AutoGraph could not transform <function create_squad_dataset.<locals>._select_data_from_record at 0x7f77d552b0d0> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: '<' not supported between instances of 'str' and 'Literal'\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:AutoGraph could not transform <function create_squad_dataset.<locals>._select_data_from_record at 0x7f77d552b0d0> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: '<' not supported between instances of 'str' and 'Literal'\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING: AutoGraph could not transform <function create_squad_dataset.<locals>._select_data_from_record at 0x7f77d552b0d0> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: '<' not supported between instances of 'str' and 'Literal'\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F4939RJqHRUs"
      },
      "source": [
        "# Fase 3: Construcción del modelo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lcQcFi8kOc6K"
      },
      "source": [
        "## Capa Squad"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iynuYxtixV_w"
      },
      "source": [
        "Para cada uno de los vectores de salida que me devuelve BERT vamos a aplicarle una capa densa que se conectará con dos unidades(neuronas) en la capa de salida. Que van a ser para cada token el 1° número, qué tan problable es que la respuesta empiece en dicho token y el segundo número nos dirá lo mismo: la verosimilitud de que ese token sea el final de frase que responde a esa pregunta. Dicho esto habrá que re-dimensionarlo y colocarlo de modo que si tenemos un párrafo de 200 palabras en lugar de tener 200 pares de valores. Score de ser el inicio y score de ser el final, voy preferir tener 200 valores en dos listas separadas, ie, una lista en donde vendrá el score por palabra de ser el inicio de frase y de la misma manera la segunda pero considerando el score de que se el final de la frase."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gjmLeQjiaOo5"
      },
      "source": [
        "class BertSquadLayer(tf.keras.layers.Layer):\n",
        "\n",
        "  def __init__(self):\n",
        "    # Inicializamos la super clase. de este modo ya estarán creadas las variables y los metodos pertenecientes a la super clase.\n",
        "    super(BertSquadLayer, self).__init__()\n",
        "    # Creamos la capa densa de la que hemos platicado.\n",
        "    self.final_dense = tf.keras.layers.Dense(\n",
        "        units=2,\n",
        "        kernel_initializer=tf.keras.initializers.TruncatedNormal(stddev=0.02))  # Google recomiendo que a la hr de hacer find tuning con BERT utilicemos una normal truncada con un valor pequeño de la desviación estándar\n",
        "\n",
        "    # inputs: son los valores que saldran de la capa de BERT\n",
        "  def call(self, inputs):\n",
        "    logits = self.final_dense(inputs) # (batch_size, seq_len, 2)\n",
        "\n",
        "    # Quiero tener un par de listas dodne cada una de ellas tenga esos 200 valores pro separado, por lo que tenemos que re-dimensionar y cortar el tensor\n",
        "    logits = tf.transpose(logits, [2, 0, 1]) # (2, batch_size, seq_len) pos 2: que tenía el score de ser inicio y final será lo primero, pos 0: lote de entrenamiento pos1:dimensión que se corresponde con las palabras (longitud de la secuencia)\n",
        "    unstacked_logits = tf.unstack(logits, axis=0) # [(batch_size, seq_len), (batch_size, seq_len)] Como la primer entrada (axis=0; eje número 0) tiene el score de ser inicio y final lo desapilamos\n",
        "    return unstacked_logits[0], unstacked_logits[1] \n",
        "    # La 1° me devolverá para cada frase del lote y para cada palabra de la frase el score de ser esa palabra de ese lote el incio de la respuesta\n",
        "    # El 2° me devolverá para cada frase del lote y para cada palabra de la frase el score de ser esa palabra de ese lote el final de la respuesta\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQbQFKjUOeyf"
      },
      "source": [
        "## Modelo completo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9geXlgpYE-5c"
      },
      "source": [
        "Construimos una clase que será todo el modelo que juntará la primera parte de BERT con la capa de squad."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KgkIFb1GMT81"
      },
      "source": [
        "class BERTSquad(tf.keras.Model):\n",
        "    \n",
        "    def __init__(self,\n",
        "                 name=\"bert_squad\"):\n",
        "        super(BERTSquad, self).__init__(name=name)\n",
        "        \n",
        "        self.bert_layer = hub.KerasLayer(\n",
        "            \"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\",\n",
        "            trainable=True)\n",
        "        \n",
        "        self.squad_layer = BertSquadLayer()\n",
        "    \n",
        "    # Definimos una función de ayuda que lo que hará será formatear la entrada de BERT porque recordemos que necesitamos tener los id's, las \n",
        "    # máscaras y los tokens de frase en ese orden correctamente para la entrada del algoritmo BERT.\n",
        "    def apply_bert(self, inputs):\n",
        "        _ , sequence_output = self.bert_layer([inputs[\"input_word_ids\"],    # Clave con la que se accede a los identificadores de cada palabra\n",
        "                                               inputs[\"input_mask\"],        # Es con el que se accede a la máscara de aleatorización.   \n",
        "                                               inputs[\"input_type_ids\"]])   # Nos indica si la frase es la número 1 o la número 2\n",
        "        # Recordemos que la capa de BERT devuelve el vector se corresponde a toda la frase en 1° posición y el que corresponde a cada una de\n",
        "        # las palabras tokenizadas en 2° posición  \n",
        "        return sequence_output\n",
        "\n",
        "    def call(self, inputs):\n",
        "        seq_output = self.apply_bert(inputs)\n",
        "\n",
        "        start_logits, end_logits = self.squad_layer(seq_output)\n",
        "        \n",
        "        return start_logits, end_logits"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lBmSU2RnHV_a"
      },
      "source": [
        "# Fase 4: Entrenamiento"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WnA3WHwRIHAZ"
      },
      "source": [
        "## Creación de la IA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JEUxomxENRoJ"
      },
      "source": [
        "TRAIN_DATA_SIZE = 88641     # Tamaño/longitud de nuestro conjunto de entrenamiento.\n",
        "# Este es un dataset bastante pesado para entrenar y para poder entrenarlo en google colab sin que nos eche es hacer un pequeño truco y es no \n",
        "# utilizar todo el data set para entrenar sino que queremos ver BERT funciona correctamente por lo que definimos:\n",
        "NB_BATCHES_TRAIN = 2000     # Esto representa aprox un 10% del total de lotes que tenemos 88641/4*.10\n",
        "BATCH_SIZE = 4              \n",
        "NB_EPOCHS = 3               # Inclusive 2 podría ser más que suficiente\n",
        "# Versión modificada del optimizador Adam por Google creado explictamemnte para esta tarea:\n",
        "INIT_LR = 5e-5              # Ratio de aprendizaje al inicio. Se define con el propósito de que el ratio de aprendizaje sea pequeño al inicio para que pueda ir experimentando y vaya incrementando durante un ratio determinado de pasos:\n",
        "WARMUP_STEPS = int(NB_BATCHES_TRAIN * 0.1)  # Ratio determinado de pasos que será el 10% del lote 2000*10% = 200"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Pg6EKe2daFy"
      },
      "source": [
        "train_dataset_light = train_dataset.take(NB_BATCHES_TRAIN)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CXnH3CQWH5tt"
      },
      "source": [
        "Creamos y compilamos nuestro modelo BERTSquad"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eHd5MzTdNIZq"
      },
      "source": [
        "bert_squad = BERTSquad()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dQrfVYTrIGMN"
      },
      "source": [
        "Creamos nuestro optimizador Adam modificado por Google."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G0cvDjBm_KXT"
      },
      "source": [
        "optimizer = optimization.create_optimizer(\n",
        "    init_lr=INIT_LR,\n",
        "    num_train_steps=NB_BATCHES_TRAIN,\n",
        "    num_warmup_steps=WARMUP_STEPS)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "96ns-oGSdRXz"
      },
      "source": [
        "Creamos nuestra función de perdidas personalizada. Recordemos que nuestra capa final nos devolverá 2 vectores uno en donde nos devuelve la verosimilitud de que un token sea el token de incio y otro considerando cual es el token final. Al final de cuentas suena como que hay que resolver un problema de clasificación pues hay que decidir si es o no el token que buscamos. Por lo tanto, utilizaremos sparse_categorical_crossentropy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I6kG-HpzVK7v"
      },
      "source": [
        "# labels: etiquetas reales\n",
        "# model_outputs: salidas del modelo\n",
        "def squad_loss_fn(labels, model_outputs):\n",
        "    start_positions = labels['start_positions']\n",
        "    end_positions = labels['end_positions']\n",
        "    start_logits, end_logits = model_outputs\n",
        "\n",
        "    start_loss = tf.keras.backend.sparse_categorical_crossentropy(\n",
        "        start_positions, start_logits, from_logits=True) \n",
        "# from_logits=True: Significa que la respuesta de nuestro modelo, en este caso el segundo parámetro, no es reralmente una probabilidad y es que en \n",
        "# ningún momento hemos aplicado una función softmax o algo así que se encargue de que todos los valores esten entre 0 y 1 y la suma de todos sea\n",
        "# igual a 1. Ahora solo tenemos números de medidas de verosimilitud, pero que en principio serán números cualesquiera, podrían ser positivos \n",
        "# o negativos y al aplicar from_logits=True, la función sparse_categorical_crossentropy se encargará de hacer las modificaciones adecuadas \n",
        "# y de comparar la respuesta para devovlernos esa pérdida. \n",
        "\n",
        "    end_loss = tf.keras.backend.sparse_categorical_crossentropy(\n",
        "        end_positions, end_logits, from_logits=True)\n",
        "    \n",
        "    # Obtenemos la media de ambas perdidas\n",
        "    total_loss = (tf.reduce_mean(start_loss) + tf.reduce_mean(end_loss)) / 2    \n",
        "    # reduce_mean: ya que queremos obtener la pérdida de todo el lote. En un principio obtendríamos 4 valores ya que recordemos que son 4 frases \n",
        "    # (dado que BATCH_SIZE = 4)\n",
        "\n",
        "    return total_loss\n",
        "\n",
        "# Finalmente, como queremos construir nuestro propio bucle de entrenamiento personalizado, nos va a intersar tener un tracking de la función \n",
        "# de perdidas (global duarante toda la fase de training) que acabamos de crear y que no es una de las estándar que utilizamos de las de tf.\n",
        "train_loss = tf.keras.metrics.Mean(name=\"train_loss\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HN_C_WA5R_Cb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "783e1376-0065-4189-f959-8d3f62855e25"
      },
      "source": [
        "next(iter(train_dataset_light))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "({'input_mask': <tf.Tensor: shape=(4, 384), dtype=int32, numpy=\n",
              "  array([[1, 1, 1, ..., 0, 0, 0],\n",
              "         [1, 1, 1, ..., 0, 0, 0],\n",
              "         [1, 1, 1, ..., 0, 0, 0],\n",
              "         [1, 1, 1, ..., 0, 0, 0]], dtype=int32)>,\n",
              "  'input_type_ids': <tf.Tensor: shape=(4, 384), dtype=int32, numpy=\n",
              "  array([[0, 0, 0, ..., 0, 0, 0],\n",
              "         [0, 0, 0, ..., 0, 0, 0],\n",
              "         [0, 0, 0, ..., 0, 0, 0],\n",
              "         [0, 0, 0, ..., 0, 0, 0]], dtype=int32)>,\n",
              "  'input_word_ids': <tf.Tensor: shape=(4, 384), dtype=int32, numpy=\n",
              "  array([[ 101, 2216, 2040, ...,    0,    0,    0],\n",
              "         [ 101, 2054, 7017, ...,    0,    0,    0],\n",
              "         [ 101, 2054, 2001, ...,    0,    0,    0],\n",
              "         [ 101, 2054, 2565, ...,    0,    0,    0]], dtype=int32)>},\n",
              " {'end_positions': <tf.Tensor: shape=(4,), dtype=int32, numpy=array([111,  74,  82, 102], dtype=int32)>,\n",
              "  'start_positions': <tf.Tensor: shape=(4,), dtype=int32, numpy=array([109,  71,  80, 100], dtype=int32)>})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lLZ2wEgzjzKF"
      },
      "source": [
        "Compilamos el modelo:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V-iE2QFC_KRI"
      },
      "source": [
        "bert_squad.compile(optimizer,\n",
        "                   squad_loss_fn)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hjp-_4OyTbuK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "f85640d4-9adf-44f9-b612-5c414e085077"
      },
      "source": [
        "# Crearemos un sistema de checkpoint en google colab que en caso de que se reinicie la sesión podemos reanudar\n",
        "# desde elúltimo checkpoint que se haya guardado o incluso más adelante , seguir entrenando con más textos \n",
        "# desde donde lo habíamos dejado en lugar de tener que crear la red neuronal desde el inicio.\n",
        "\n",
        "# Definimos una ruta dentro de mi Google Drive donde se guardarán los checkpoints\n",
        "checkpoint_path = \"./drive/My Drive/Curso de NLP/BERT/ckpt_bert_squad/\"\n",
        "\n",
        "# Guardamos el modelo que queremos guardar autoamticamente siempre que sea posible\n",
        "ckpt = tf.train.Checkpoint(bert_squad=bert_squad)\n",
        "\n",
        "# Definimos el manager que será el encargado de guardar los checkpoints en la ruta establecida\n",
        "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=1)\n",
        "# max_to_keep: Es para definir que queremos que siempre se guarden los últimos 5 checkpoints.\n",
        "# En este caso si quisieramos guardar todos bastaría con poner max_to_keep = 3 pues son 3 epochs\n",
        "\n",
        "# Las siguientes líneas de código lo que hacen es preguntarle al Checkpoint Manager si hay o no hay \n",
        "# último checkpoint\n",
        "if ckpt_manager.latest_checkpoint:      # esta linea devuelve un None si no hay checkpoint previo (If None:)\n",
        "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
        "    print(\"Último checkpoint restaurado!!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Último checkpoint restaurado!!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bDgEq09xIOOl"
      },
      "source": [
        "## Entrenamiento personalizado"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ywelW3uaSbT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "1c27881e-7ebe-438e-b6dd-6830bd4bcc79"
      },
      "source": [
        "for epoch in range(NB_EPOCHS):\n",
        "    print(\"Inicio del Epoch {}\".format(epoch+1))\n",
        "    start = time.time()\n",
        "    \n",
        "    train_loss.reset_states()   # Devuelve toda la función de pérdidas a 0 y así durante un epoch llevar traking solo de este\n",
        "    \n",
        "    for (batch, (inputs, targets)) in enumerate(train_dataset_light):\n",
        "        with tf.GradientTape() as tape:\n",
        "            model_outputs = bert_squad(inputs)\n",
        "            loss = squad_loss_fn(targets, model_outputs)\n",
        "        \n",
        "        gradients = tape.gradient(loss, bert_squad.trainable_variables)     # Se obtiene el gradiente solo para las variables entrenables de BERT\n",
        "        optimizer.apply_gradients(zip(gradients, bert_squad.trainable_variables))   # juntamos la info de los gradientes y a que variables pertenecen\n",
        "        \n",
        "        train_loss(loss)\n",
        "        \n",
        "        # Si el lote es múltiplo de 50 que se impriman los siguientes valores:\n",
        "        if batch % 50 == 0:\n",
        "            print(\"Epoch {} Lote {} Pérdida {:.4f}\".format(\n",
        "                epoch+1, batch, train_loss.result()))\n",
        "        \n",
        "        # Si el lote es múltiplo de 500 se hará un checkpoint\n",
        "        if batch % 500 == 0:\n",
        "            ckpt_save_path = ckpt_manager.save()\n",
        "            print(\"Guardando checkpoint para el epoch {} en el directorio {}\".format(epoch+1,\n",
        "                                                                ckpt_save_path))\n",
        "    print(\"Tiempo total para entrenar 1 epoch: {} segs\\n\".format(time.time() - start))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Inicio del Epoch 1\n",
            "Epoch 1 Lote 0 Pérdida 6.0621\n",
            "Guardando checkpoint para el epoch 1 en el directorio ./drive/My Drive/Curso de NLP/BERT/ckpt_bert_squad/ckpt-1\n",
            "Epoch 1 Lote 50 Pérdida 5.8209\n",
            "Epoch 1 Lote 100 Pérdida 5.2858\n",
            "Epoch 1 Lote 150 Pérdida 4.6081\n",
            "Epoch 1 Lote 200 Pérdida 4.1292\n",
            "Epoch 1 Lote 250 Pérdida 3.7406\n",
            "Epoch 1 Lote 300 Pérdida 3.4795\n",
            "Epoch 1 Lote 350 Pérdida 3.3128\n",
            "Epoch 1 Lote 400 Pérdida 3.1128\n",
            "Epoch 1 Lote 450 Pérdida 2.9574\n",
            "Epoch 1 Lote 500 Pérdida 2.8129\n",
            "Guardando checkpoint para el epoch 1 en el directorio ./drive/My Drive/Curso de NLP/BERT/ckpt_bert_squad/ckpt-2\n",
            "Epoch 1 Lote 550 Pérdida 2.6840\n",
            "Epoch 1 Lote 600 Pérdida 2.6034\n",
            "Epoch 1 Lote 650 Pérdida 2.5247\n",
            "Epoch 1 Lote 700 Pérdida 2.4478\n",
            "Epoch 1 Lote 750 Pérdida 2.3795\n",
            "Epoch 1 Lote 800 Pérdida 2.3212\n",
            "Epoch 1 Lote 850 Pérdida 2.2794\n",
            "Epoch 1 Lote 900 Pérdida 2.2269\n",
            "Epoch 1 Lote 950 Pérdida 2.1805\n",
            "Epoch 1 Lote 1000 Pérdida 2.1184\n",
            "Guardando checkpoint para el epoch 1 en el directorio ./drive/My Drive/Curso de NLP/BERT/ckpt_bert_squad/ckpt-3\n",
            "Epoch 1 Lote 1050 Pérdida 2.0712\n",
            "Epoch 1 Lote 1100 Pérdida 2.0192\n",
            "Epoch 1 Lote 1150 Pérdida 1.9696\n",
            "Epoch 1 Lote 1200 Pérdida 1.9270\n",
            "Epoch 1 Lote 1250 Pérdida 1.9186\n",
            "Epoch 1 Lote 1300 Pérdida 1.8983\n",
            "Epoch 1 Lote 1350 Pérdida 1.8858\n",
            "Epoch 1 Lote 1400 Pérdida 1.8687\n",
            "Epoch 1 Lote 1450 Pérdida 1.8431\n",
            "Epoch 1 Lote 1500 Pérdida 1.8203\n",
            "Guardando checkpoint para el epoch 1 en el directorio ./drive/My Drive/Curso de NLP/BERT/ckpt_bert_squad/ckpt-4\n",
            "Epoch 1 Lote 1550 Pérdida 1.8009\n",
            "Epoch 1 Lote 1600 Pérdida 1.7833\n",
            "Epoch 1 Lote 1650 Pérdida 1.7777\n",
            "Epoch 1 Lote 1700 Pérdida 1.7699\n",
            "Epoch 1 Lote 1750 Pérdida 1.7521\n",
            "Epoch 1 Lote 1800 Pérdida 1.7437\n",
            "Epoch 1 Lote 1850 Pérdida 1.7187\n",
            "Epoch 1 Lote 1900 Pérdida 1.6900\n",
            "Epoch 1 Lote 1950 Pérdida 1.6774\n",
            "Tiempo total para entrenar 1 epoch: 5262.5173897743225 segs\n",
            "\n",
            "Inicio del Epoch 2\n",
            "Epoch 2 Lote 0 Pérdida 1.1600\n",
            "Guardando checkpoint para el epoch 2 en el directorio ./drive/My Drive/Curso de NLP/BERT/ckpt_bert_squad/ckpt-5\n",
            "Epoch 2 Lote 50 Pérdida 1.1323\n",
            "Epoch 2 Lote 100 Pérdida 1.1650\n",
            "Epoch 2 Lote 150 Pérdida 1.0600\n",
            "Epoch 2 Lote 200 Pérdida 1.0460\n",
            "Epoch 2 Lote 250 Pérdida 0.9981\n",
            "Epoch 2 Lote 300 Pérdida 0.9548\n",
            "Epoch 2 Lote 350 Pérdida 0.9416\n",
            "Epoch 2 Lote 400 Pérdida 0.8956\n",
            "Epoch 2 Lote 450 Pérdida 0.8426\n",
            "Epoch 2 Lote 500 Pérdida 0.8125\n",
            "Guardando checkpoint para el epoch 2 en el directorio ./drive/My Drive/Curso de NLP/BERT/ckpt_bert_squad/ckpt-6\n",
            "Epoch 2 Lote 550 Pérdida 0.7875\n",
            "Epoch 2 Lote 600 Pérdida 0.7739\n",
            "Epoch 2 Lote 650 Pérdida 0.7629\n",
            "Epoch 2 Lote 700 Pérdida 0.7346\n",
            "Epoch 2 Lote 750 Pérdida 0.7113\n",
            "Epoch 2 Lote 800 Pérdida 0.6930\n",
            "Epoch 2 Lote 850 Pérdida 0.6815\n",
            "Epoch 2 Lote 900 Pérdida 0.6704\n",
            "Epoch 2 Lote 950 Pérdida 0.6603\n",
            "Epoch 2 Lote 1000 Pérdida 0.6411\n",
            "Guardando checkpoint para el epoch 2 en el directorio ./drive/My Drive/Curso de NLP/BERT/ckpt_bert_squad/ckpt-7\n",
            "Epoch 2 Lote 1050 Pérdida 0.6289\n",
            "Epoch 2 Lote 1100 Pérdida 0.6145\n",
            "Epoch 2 Lote 1150 Pérdida 0.6004\n",
            "Epoch 2 Lote 1200 Pérdida 0.5895\n",
            "Epoch 2 Lote 1250 Pérdida 0.5867\n",
            "Epoch 2 Lote 1300 Pérdida 0.5894\n",
            "Epoch 2 Lote 1350 Pérdida 0.6004\n",
            "Epoch 2 Lote 1400 Pérdida 0.5978\n",
            "Epoch 2 Lote 1450 Pérdida 0.5938\n",
            "Epoch 2 Lote 1500 Pérdida 0.5948\n",
            "Guardando checkpoint para el epoch 2 en el directorio ./drive/My Drive/Curso de NLP/BERT/ckpt_bert_squad/ckpt-8\n",
            "Epoch 2 Lote 1550 Pérdida 0.5932\n",
            "Epoch 2 Lote 1600 Pérdida 0.5958\n",
            "Epoch 2 Lote 1650 Pérdida 0.6041\n",
            "Epoch 2 Lote 1700 Pérdida 0.6120\n",
            "Epoch 2 Lote 1750 Pérdida 0.6117\n",
            "Epoch 2 Lote 1800 Pérdida 0.6166\n",
            "Epoch 2 Lote 1850 Pérdida 0.6114\n",
            "Epoch 2 Lote 1900 Pérdida 0.6073\n",
            "Epoch 2 Lote 1950 Pérdida 0.6167\n",
            "Tiempo total para entrenar 1 epoch: 5323.075678825378 segs\n",
            "\n",
            "Inicio del Epoch 3\n",
            "Epoch 3 Lote 0 Pérdida 1.6725\n",
            "Guardando checkpoint para el epoch 3 en el directorio ./drive/My Drive/Curso de NLP/BERT/ckpt_bert_squad/ckpt-9\n",
            "Epoch 3 Lote 50 Pérdida 1.1375\n",
            "Epoch 3 Lote 100 Pérdida 1.0644\n",
            "Epoch 3 Lote 150 Pérdida 1.0579\n",
            "Epoch 3 Lote 200 Pérdida 1.0375\n",
            "Epoch 3 Lote 250 Pérdida 1.0067\n",
            "Epoch 3 Lote 300 Pérdida 0.9554\n",
            "Epoch 3 Lote 350 Pérdida 0.9430\n",
            "Epoch 3 Lote 400 Pérdida 0.8927\n",
            "Epoch 3 Lote 450 Pérdida 0.8453\n",
            "Epoch 3 Lote 500 Pérdida 0.8164\n",
            "Guardando checkpoint para el epoch 3 en el directorio ./drive/My Drive/Curso de NLP/BERT/ckpt_bert_squad/ckpt-10\n",
            "Epoch 3 Lote 550 Pérdida 0.7860\n",
            "Epoch 3 Lote 600 Pérdida 0.7754\n",
            "Epoch 3 Lote 650 Pérdida 0.7596\n",
            "Epoch 3 Lote 700 Pérdida 0.7369\n",
            "Epoch 3 Lote 750 Pérdida 0.7108\n",
            "Epoch 3 Lote 800 Pérdida 0.6945\n",
            "Epoch 3 Lote 850 Pérdida 0.6831\n",
            "Epoch 3 Lote 900 Pérdida 0.6735\n",
            "Epoch 3 Lote 950 Pérdida 0.6600\n",
            "Epoch 3 Lote 1000 Pérdida 0.6426\n",
            "Guardando checkpoint para el epoch 3 en el directorio ./drive/My Drive/Curso de NLP/BERT/ckpt_bert_squad/ckpt-11\n",
            "Epoch 3 Lote 1050 Pérdida 0.6262\n",
            "Epoch 3 Lote 1100 Pérdida 0.6146\n",
            "Epoch 3 Lote 1150 Pérdida 0.6007\n",
            "Epoch 3 Lote 1200 Pérdida 0.5895\n",
            "Epoch 3 Lote 1250 Pérdida 0.5879\n",
            "Epoch 3 Lote 1300 Pérdida 0.5926\n",
            "Epoch 3 Lote 1350 Pérdida 0.6027\n",
            "Epoch 3 Lote 1400 Pérdida 0.6001\n",
            "Epoch 3 Lote 1450 Pérdida 0.5950\n",
            "Epoch 3 Lote 1500 Pérdida 0.5975\n",
            "Guardando checkpoint para el epoch 3 en el directorio ./drive/My Drive/Curso de NLP/BERT/ckpt_bert_squad/ckpt-12\n",
            "Epoch 3 Lote 1550 Pérdida 0.5964\n",
            "Epoch 3 Lote 1600 Pérdida 0.5967\n",
            "Epoch 3 Lote 1650 Pérdida 0.6051\n",
            "Epoch 3 Lote 1700 Pérdida 0.6109\n",
            "Epoch 3 Lote 1750 Pérdida 0.6126\n",
            "Epoch 3 Lote 1800 Pérdida 0.6146\n",
            "Epoch 3 Lote 1850 Pérdida 0.6134\n",
            "Epoch 3 Lote 1900 Pérdida 0.6089\n",
            "Epoch 3 Lote 1950 Pérdida 0.6163\n",
            "Tiempo total para entrenar 1 epoch: 5372.474010705948 segs\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v6WquTCiIR7t"
      },
      "source": [
        "# Fase 5: Evaluación"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qDIlHd5Tos6C"
      },
      "source": [
        "## Preparación de la evaluación"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kU7_AyTIpTTJ"
      },
      "source": [
        "Get the dev set in the session"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VGKl84s5WrhD"
      },
      "source": [
        "eval_examples = read_squad_examples(\n",
        "    \"/content/drive/My Drive/Curso de NLP/BERT/squad_data/dev-v1.1.json\",\n",
        "    is_training=False,\n",
        "    version_2_with_negative=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DAEUcZDSpYLD"
      },
      "source": [
        "Define the function that will write the tf_record file for the dev set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yCVmIgnEo83o"
      },
      "source": [
        "eval_writer = FeatureWriter(\n",
        "    filename=os.path.join(\"/content/drive/My Drive/Curso de NLP/BERT/squad_data/\",\n",
        "                          \"eval.tf_record\"),\n",
        "    is_training=False)\n",
        "# FeatureWriter: a la hr de llevar a cabo la parte de la evaluación necesitamos contar con features, los\n",
        "# cuales no son más que protocolos que tf utiliza a la hr de trabajar. Son herramientas que se utlizan para\n",
        "# proporcionar la información o almacenar la información. En otras palabras es una convención si \n",
        "# queremos usar tf squad."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C8aSLFdmp71I"
      },
      "source": [
        "Create a tokenizer for future information needs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oH5exQ7KwnuH"
      },
      "source": [
        "my_bert_layer = hub.KerasLayer(\n",
        "    \"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\",\n",
        "    trainable=False)\n",
        "vocab_file = my_bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
        "do_lower_case = my_bert_layer.resolved_object.do_lower_case.numpy()\n",
        "tokenizer = FullTokenizer(vocab_file, do_lower_case)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vdHudjJ_qAzo"
      },
      "source": [
        "Define the function that add the features (feature is a protocol in tensorflow) to our eval_features list"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bmQ5GtoTxRjU"
      },
      "source": [
        "# Como he dicho los features no son más que pedazos de info que siguen un protocolo, que siguen una\n",
        "# convención y por tanto solo lo convierto al formato que me intersa.\n",
        "def _append_feature(feature, is_padding):\n",
        "    if not is_padding:\n",
        "        eval_features.append(feature)\n",
        "    eval_writer.process_feature(feature)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HLAcwCiaqHi_"
      },
      "source": [
        "Create the eval features and the writes the tf.record file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mz7kGYmUwGQb"
      },
      "source": [
        "eval_features = []\n",
        "dataset_size = convert_examples_to_features(\n",
        "    examples=eval_examples,\n",
        "    tokenizer=tokenizer,\n",
        "    max_seq_length=384,     # este valor varia dependiendo los datos\n",
        "    doc_stride=128,\n",
        "    max_query_length=64,\n",
        "    is_training=False,\n",
        "    output_fn=_append_feature,\n",
        "    batch_size=4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WpZfwPEwMabx"
      },
      "source": [
        "eval_writer.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TbKhx3zuq844"
      },
      "source": [
        "Load the ready-to-be-used dataset to our session"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eUqYvG5TxctF"
      },
      "source": [
        "BATCH_SIZE = 4\n",
        "\n",
        "eval_dataset = create_squad_dataset(\n",
        "    \"/content/drive/My Drive/Curso de NLP/BERT/squad_data/eval.tf_record\",\n",
        "    384,#input_meta_data['max_seq_length'],\n",
        "    BATCH_SIZE,\n",
        "    is_training=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tRbKrFYoo8e8"
      },
      "source": [
        "## Llevar a cabo las prediccioness"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KyckEWDbrLEX"
      },
      "source": [
        "Definir un cierto tipo de colección (como un diccionario). Esto nos será util ya que como seguiremos trabajando con lotes en un momento dado, vamos querer crear una función que devuelva los elementos de salida por lote uno detrás de otro. Entonces, para ello nos va ir muy bien que los elementos vayan nombrados."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tyWOUKaDP0H0"
      },
      "source": [
        "# Creamos una tupla nombrado lo que nos permitirá crear tuplas dodne cada elemento ahora tenga un nombre\n",
        "# en lugar de las tuplas estandár a las que se accede por posición.\n",
        "RawResult = collections.namedtuple(\"RawResult\",\n",
        "                                   [\"unique_id\", \"start_logits\", \"end_logits\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28abKVvqrRa4"
      },
      "source": [
        "Devuelve cada elemento del lote de salida, uno por uno. Dado un elemento del lote de salida lo va a iterar en este formato para devolverme empaquetada la información que me interesa. Enotras palabras de la salida del modelos solo me quedaré con unique_ids, start_logits y end_logits. Lo que hace zip es que empaqueta la info en 3 listas con una misma longitud."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BScaA0SZQgQW"
      },
      "source": [
        "def get_raw_results(predictions):\n",
        "    for unique_ids, start_logits, end_logits in zip(predictions['unique_ids'],\n",
        "                                                    predictions['start_logits'],\n",
        "                                                    predictions['end_logits']):\n",
        "        yield RawResult(\n",
        "            unique_id=unique_ids.numpy(),       # lo convertimos a formato numpy para poder manejarlo más comodamente\n",
        "            start_logits=start_logits.numpy().tolist(), # lo convertimos a lista ya que más adelante utilizaré una de las funciones de Google que aceptan como entrada a estas como listas\n",
        "            end_logits=end_logits.numpy().tolist())\n",
        "# si no conocen yield: significa que esta función realmente actuará como si fuera un iterador, ie, podemos\n",
        "# hacer un bucle sobre este objeto cada vez que la llamemos. Funcionará exactamente como un iterador, de\n",
        "# modo que podemos hacer un bucle a tráves del objeto get_raw_results de una predicción e ir obteniendo cada \n",
        "# una de la entradas. De ahí esa gracia de yield porque devolverá un elemento y esperará a que un bucle pida\n",
        "# la siguiente observación. Entonces, como la predicción nos vendrá en lotes yo le puedo dar todo el lote \n",
        "# e iterar directamente sobre el resultado de get_raw_results para devolver empaquetado el id, el loggit \n",
        "# de entrada y el loggit de salida."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JiLOOmnLre5C"
      },
      "source": [
        "Hacemos nuestras predicciones"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qqD78Xdjrvpn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 530
        },
        "outputId": "b128075d-bfc1-4df6-852e-f1020bbc404f"
      },
      "source": [
        "all_results = []\n",
        "for count, inputs in enumerate(eval_dataset):\n",
        "    x, _ = inputs   # nos quedamos como entrada solo la pregunta y nos deshacemos de la respuesta\n",
        "    unique_ids = x.pop(\"unique_ids\")    # Quitamos el identificador ya que si se lo doy a la capa de BERT no sabrá muy bien que hacer con él\n",
        "    start_logits, end_logits = bert_squad(x, training=False)\n",
        "    output_dict = dict(\n",
        "        unique_ids=unique_ids,  # Fijemonos que he hecho pop pero lo he guardado en una variable. Me quedado con una referencia.\n",
        "        start_logits=start_logits,\n",
        "        end_logits=end_logits)\n",
        "    for result in get_raw_results(output_dict): # Como get_raw_results es un iterador no me dará ningun problema\n",
        "        all_results.append(result)\n",
        "    if count % 100 == 0:\n",
        "        print(\"{}/{}\".format(count, 2709))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0/2709\n",
            "100/2709\n",
            "200/2709\n",
            "300/2709\n",
            "400/2709\n",
            "500/2709\n",
            "600/2709\n",
            "700/2709\n",
            "800/2709\n",
            "900/2709\n",
            "1000/2709\n",
            "1100/2709\n",
            "1200/2709\n",
            "1300/2709\n",
            "1400/2709\n",
            "1500/2709\n",
            "1600/2709\n",
            "1700/2709\n",
            "1800/2709\n",
            "1900/2709\n",
            "2000/2709\n",
            "2100/2709\n",
            "2200/2709\n",
            "2300/2709\n",
            "2400/2709\n",
            "2500/2709\n",
            "2600/2709\n",
            "2700/2709\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PjQ6kIqGriHr"
      },
      "source": [
        "Escribimos nuestras predicciones en un fichero JSON que funcionará con el script de evaluación. El que realmente nos intersa es el primero los otros dos son ficheros que le hace falta Google y hay que mencionar el path en donde están si no pues fallaría la función."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "esLdRf7uM3Lz"
      },
      "source": [
        "output_prediction_file = \"/content/drive/My Drive/Curso de NLP/BERT/squad_data/predictions.json\"\n",
        "output_nbest_file = \"/content/drive/My Drive/Curso de NLP/BERT/squad_data/nbest_predictions.json\"\n",
        "output_null_log_odds_file = \"/content/drive/My Drive/Curso de NLP/BERT/squad_data/null_odds.json\"\n",
        "\n",
        "write_predictions(\n",
        "    eval_examples,  \n",
        "    eval_features,  # formato interno que habíamos meniconado que sería necesario para poder crear el fichero correctamente\n",
        "    all_results,    # los resultados que hemos obtenido \n",
        "    20,             # No sé que sea pero son necesarios\n",
        "    30,             # No sé que sea pero son necesarios\n",
        "    True,           # Ya que hemos echo uso del lower case en la fase de tokenización.\n",
        "    output_prediction_file, \n",
        "    output_nbest_file,\n",
        "    output_null_log_odds_file,\n",
        "    verbose=False)  # Para que no se nos impirma mucha basura en la pantalla mientras corre"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-eaIHyDIYHHx"
      },
      "source": [
        "## Predicción casera"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P0F4l5h8Zdha"
      },
      "source": [
        "### Creación del diccionario de entrada"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IrhHzx7ycXXo"
      },
      "source": [
        "Concatenamos la pregunta y el contexto, separados por `[\"SEP\"]`, tras la tokenización, tal cual como lo hicimos con el conjunto de entrenamiento.\n",
        "\n",
        "Lo importante a recordar es que queremos que nuestra respuesta empiece y termine con una palabra real. Por ejemplo, la palabra \"ecologically\" es tokenizada como `[\"ecological\", \"##ly\"]`, y si el token de fin es `[\"ecological\"]` queremos usar la palabra \"ecologically\" como palabra final (del mismo modo si el token de fin es`[\"##ly\"]`). Por eso, empezamos dividiendo nuestro contexto en palabras, y luego pasamos a tokens, recordando qué token se corresponde con qué palabra (ver la función `tokenize_context()` para más detalle)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_tuNXt98Zm4u"
      },
      "source": [
        "#### Útiles varios"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OBjGoQ_wfmml"
      },
      "source": [
        "my_bert_layer = hub.KerasLayer(\n",
        "    \"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\",\n",
        "    trainable=False)\n",
        "vocab_file = my_bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
        "do_lower_case = my_bert_layer.resolved_object.do_lower_case.numpy()\n",
        "tokenizer = FullTokenizer(vocab_file, do_lower_case)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8f_fCe_hLC12"
      },
      "source": [
        "def is_whitespace(c):\n",
        "    '''\n",
        "    Indica si un cadena de caracteres se corresponde con un espacio en blanco / separador o no.\n",
        "    '''\n",
        "    if c == \" \" or c == \"\\t\" or c == \"\\r\" or c == \"\\n\" or ord(c) == 0x202F:\n",
        "        return True\n",
        "    return False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ZA4TYnxLGVT"
      },
      "source": [
        "# Lo que haremos aqui es enchufarle todo el texto y tendremos en doc_tokens los tokens del documento\n",
        "# y si previamente procedían de un espacio en blanco (prev_is_whitespace) y para cada caracter del texto \n",
        "# iremos leyendo, leyendo si es espacio en blanco (is_whitespace(c)) acabo de encontrar un espacio \n",
        "# en blanco prev_is_whitespace = True. Si no es un espacio en blanco (else), entonces depende, si\n",
        "# previamente teníamos un espacio en blanco (if prev_is_whitespace) añadimos un token a los tokens \n",
        "# del documento y si no lo único que hacemos es ir apendizando caracteres al documento para que \n",
        "# sea unicamente cuando leo un espacio en blanco que todos los document tokens han sido recopilados\n",
        "# que hemos localizado todas las palabras y por lo tanto podemos añadirlo al token. En pocas palabras,\n",
        "# esto lo que nos va a hacer es romper el texto en una lista de palabras pero gestionada por mi no \n",
        "# gestionada por el tokenizador.\n",
        "def whitespace_split(text):\n",
        "    '''\n",
        "    Toma el texto y devuelve una lista de \"palabras\" separadas segun los \n",
        "    espacios en blanco / separadores anteriores.\n",
        "    '''\n",
        "    doc_tokens = []\n",
        "    prev_is_whitespace = True\n",
        "    for c in text:\n",
        "        if is_whitespace(c):\n",
        "            prev_is_whitespace = True\n",
        "        else:\n",
        "            if prev_is_whitespace:\n",
        "                doc_tokens.append(c)\n",
        "            else:\n",
        "                doc_tokens[-1] += c\n",
        "            prev_is_whitespace = False\n",
        "    return doc_tokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fsfzt3GUNWQK"
      },
      "source": [
        "def tokenize_context(text_words):\n",
        "    '''\n",
        "    Toma una lista de palabras (devueltas por whitespace_split()) y tokeniza cada\n",
        "    palabra una por una. También almacena, para cada nuevo token, la palabra original\n",
        "    del parámetro text_words.\n",
        "    '''\n",
        "    text_tok = []\n",
        "    tok_to_word_id = []\n",
        "    for word_id, word in enumerate(text_words):\n",
        "        word_tok = tokenizer.tokenize(word)\n",
        "        text_tok += word_tok\n",
        "        tok_to_word_id += [word_id]*len(word_tok)   # leer *\n",
        "    return text_tok, tok_to_word_id # Devolvemos los tokens del texto y los identificadores de palabras que corresponden a cada uno de los tokens \n",
        "# *: añadimos el identificador de la palabra len(word_tok) veces de modo que si una sola palabra pasa a \n",
        "# ser 3 tokens porque tiene un prefijo en el cuerpo y un sufijo pues guardaremos 3 veces su identificador.\n",
        "# Este es el truco para tener una biyección en todo momento de palabra y token o tokens respectivos. Esta\n",
        "# es la razón del porque es necessario whitespace_split y tokenize_context"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-1cdrPrUsvdL"
      },
      "source": [
        "Necesitamos crear las 3 entradas diferentes para cada oración."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c8qreqEURUOP"
      },
      "source": [
        "# Devulve los identificadores para cada token\n",
        "def get_ids(tokens):\n",
        "    return tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "# Devulve todo lo que no es el caracter de padding \n",
        "def get_mask(tokens):\n",
        "    return np.char.not_equal(tokens, \"[PAD]\").astype(int)\n",
        "\n",
        "# Devulve 0 o 1 depeniendo de si esta antes o después el separador o incluso si hubiera más separadores\n",
        "# me iría combinando 0 o 1 para cada una de las palabras que hubiera de entre los separadores\n",
        "def get_segments(tokens):\n",
        "    seg_ids = []\n",
        "    current_seg_id = 0\n",
        "    for tok in tokens:\n",
        "        seg_ids.append(current_seg_id)\n",
        "        if tok == \"[SEP]\":\n",
        "            current_seg_id = 1-current_seg_id # Convierte 1 en 0 y viceversa\n",
        "    return seg_ids"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qOgZcgSttqG8"
      },
      "source": [
        "Creamos nuestro diccionario de entradas de modo que le damos una pregunta y un contexto ynos devolverá un diccionario con los 3 elementos que le hacen falta al modelo. A saber, los tokens o los id's de tokens las máscaras y el identificador de frase. También devolverá las palabras del contexto (context_words) además id's de los tokens de contextos (context_tok_to_word_id) y la longitud de los tokens."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P2sPGXxsYUsY"
      },
      "source": [
        "def create_input_dict(question, context):\n",
        "    '''\n",
        "    Take a question and a context as strings and return a dictionary with the 3\n",
        "    elements needed for the model. Also return the context_words, the\n",
        "    context_tok to context_word ids correspondance and the length of\n",
        "    question_tok that we will need later.\n",
        "    '''\n",
        "    question_tok = tokenizer.tokenize(my_question)\n",
        "\n",
        "    context_words = whitespace_split(context)\n",
        "    context_tok, context_tok_to_word_id = tokenize_context(context_words)\n",
        "\n",
        "    input_tok = question_tok + [\"[SEP]\"] + context_tok + [\"[SEP]\"]\n",
        "    input_tok += [\"[PAD]\"]*(384-len(input_tok)) # in our case the model has been\n",
        "                                                # trained to have inputs of length max 384\n",
        "    # Generamos la entrada para BERT                                            \n",
        "    input_dict = {}\n",
        "    input_dict[\"input_word_ids\"] = tf.expand_dims(tf.cast(get_ids(input_tok), tf.int32), 0)\n",
        "    input_dict[\"input_mask\"] = tf.expand_dims(tf.cast(get_mask(input_tok), tf.int32), 0)\n",
        "    input_dict[\"input_type_ids\"] = tf.expand_dims(tf.cast(get_segments(input_tok), tf.int32), 0)\n",
        "\n",
        "    return input_dict, context_words, context_tok_to_word_id, len(question_tok)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QAnaCZWTZpWT"
      },
      "source": [
        "#### Creación"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WQlM-M8rMklA"
      },
      "source": [
        "my_context = '''Neoclassical economics views inequalities in the distribution of income as arising from differences in value added by labor, capital and land. Within labor income distribution is due to differences in value added by different classifications of workers. In this perspective, wages and profits are determined by the marginal value added of each economic actor (worker, capitalist/business owner, landlord). Thus, in a market economy, inequality is a reflection of the productivity gap between highly-paid professions and lower-paid professions.'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YL_cE-o0U8mx"
      },
      "source": [
        "Neoclassical economics views inequalities in the distribution of income as arising from differences in value added by labor, capital and land. Within labor income distribution is due to differences in value added by different classifications of workers. In this perspective, wages and profits are determined by the marginal value added of each economic actor (worker, capitalist/business owner, landlord). Thus, in a market economy, inequality is a reflection of the productivity gap between highly-paid professions and lower-paid professions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SeB29SCNNQ1M"
      },
      "source": [
        "#my_question = '''What philosophy of thought addresses wealth inequality?'''\n",
        "my_question = '''What are examples of economic actors?'''\n",
        "#my_question = '''In a market economy, what is inequality a reflection of?'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-v5nLMNjZufe"
      },
      "source": [
        "my_input_dict, my_context_words, context_tok_to_word_id, question_tok_len = create_input_dict(my_question, my_context)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dT066rMtZ65X"
      },
      "source": [
        "### Predicción"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CcJgDa9gVShl"
      },
      "source": [
        "start_logits, end_logits = bert_squad(my_input_dict, training=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rhdGlIo5Z9IZ"
      },
      "source": [
        "### Interpretación"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZQJMBkVLd9wp"
      },
      "source": [
        "We remove the ids corresponding to the question and the `[\"SEP\"]` token:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TfwBJfsSTwRn"
      },
      "source": [
        "start_logits_context = start_logits.numpy()[0, question_tok_len+1:] \n",
        "end_logits_context = end_logits.numpy()[0, question_tok_len+1:]\n",
        "# 0: Corresponde al lote númeor 0 ya que solo estamos pasando un texto y question_tok_len+1: elimino \n",
        "# los tokens de la pregunta + 1 que corresponde al separador quedando así unicamente con el contexto"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "te1u6iZAawYf"
      },
      "source": [
        "First easy interpretation:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lQJ8tp-1WvI4"
      },
      "source": [
        "# Del contexto dónde empieza la respuesta, es simplemente buscar de los logits (start_logits_context) la\n",
        "# posición a la que correpsonde el argumento máximo, en qué posición se encuentra la palbra o el token\n",
        "# con mayor score y búscala en context_tok_to_word_id. Y este sería el identificador de la palabra de\n",
        "# inicio de la respuesta\n",
        "start_word_id = context_tok_to_word_id[np.argmax(start_logits_context)]\n",
        "end_word_id = context_tok_to_word_id[np.argmax(end_logits_context)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k3PHA84rarse"
      },
      "source": [
        "\"Advanced\" - making sure that the start of the answer is before the end:\r\n",
        "\r\n",
        "Para que no ocurre que nos diga que el token de inicio de la respuesta es por ejemplo el token 21 y el final de la respuesta es el 14. Esta es la versión mejorada con un doble bucle y busca el par de respuestas cuyo argumento máximo cumpla que el token de inicio sea anterior al token final."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xiFZ2fUiRU_M"
      },
      "source": [
        "pair_scores = np.ones((len(start_logits_context), len(end_logits_context)))*(-1E10)\n",
        "for i in range(len(start_logits_context-1)):\n",
        "    for j in range(i, len(end_logits_context)):\n",
        "        pair_scores[i, j] = start_logits_context[i] + end_logits_context[j]\n",
        "pair_scores_argmax = np.argmax(pair_scores)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N9KEiFHPXXeM"
      },
      "source": [
        "start_word_id = context_tok_to_word_id[pair_scores_argmax // len(start_logits_context)] # corresponde a la fila\n",
        "end_word_id = context_tok_to_word_id[pair_scores_argmax % len(end_logits_context)]      # corresponde a la columna"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kJDBL8KKa6NP"
      },
      "source": [
        "Final answer:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j0Y3WDz0XwAw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ff600bb-528d-4b14-f800-731aa4448e9a"
      },
      "source": [
        "predicted_answer = ' '.join(my_context_words[start_word_id:end_word_id+1])\n",
        "print(\"The answer to:\\n\" + my_question + \"\\nis:\\n\" + predicted_answer)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The answer to:\n",
            "What are examples of economic actors?\n",
            "is:\n",
            "(worker, capitalist/business owner, landlord).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bYGSk_5OSYUk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 161
        },
        "outputId": "dbd02f58-62e6-45c8-8ddd-b2b6989a2385"
      },
      "source": [
        "from IPython.core.display import HTML\n",
        "display(HTML(f'<h2>{my_question.upper()}</h2>'))\n",
        "marked_text = str(my_context.replace(predicted_answer, f\"<mark>{predicted_answer}</mark>\"))\n",
        "display(HTML(f\"\"\"<blockquote> {marked_text} </blockquote>\"\"\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<h2>WHAT ARE EXAMPLES OF ECONOMIC ACTORS?</h2>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<blockquote> Neoclassical economics views inequalities in the distribution of income as arising from differences in value added by labor, capital and land. Within labor income distribution is due to differences in value added by different classifications of workers. In this perspective, wages and profits are determined by the marginal value added of each economic actor <mark>(worker, capitalist/business owner, landlord).</mark> Thus, in a market economy, inequality is a reflection of the productivity gap between highly-paid professions and lower-paid professions. </blockquote>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GAXMJwopGRzM"
      },
      "source": [
        "#### Reto Final"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Brg0AXkA0r74",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 196
        },
        "outputId": "27b814f9-65bf-4ae0-90d7-c246b6363ba6"
      },
      "source": [
        "my_context = '''\n",
        "Coronavirus disease 2019 is an infectious disease caused by severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2). It was first identified in December 2019 in Wuhan, Hubei, China, and has resulted in an ongoing pandemic.\n",
        "Common symptoms include fever, cough, fatigue, shortness of breath, and loss of smell and taste.While most people have mild symptoms, some people develop acute respiratory distress syndrome (ARDS) possibly precipitated by cytokine storm, multi-organ failure, septic shock, and blood clots. The time from exposure to onset of symptoms is typically around five days, but may range from two to fourteen days.\n",
        "The virus is spread primarily via nose and mouth secretions including small droplets produced by coughing,[a] sneezing, and talking. The droplets usually do not travel through air over long distances. However, those standing in close proximity may inhale these droplets and become infected.[b] People may also become infected by touching a contaminated surface and then touching their face. The transmission may also occur through smaller droplets that are able to stay suspended in the air for longer periods of time in enclosed spaces.'''\n",
        "\n",
        "my_question = '''What are the common symptoms of the disease?'''\n",
        "\n",
        "my_input_dict, my_context_words, context_tok_to_word_id, question_tok_len = create_input_dict(my_question, my_context)\n",
        "\n",
        "start_logits, end_logits = bert_squad(my_input_dict, training=False)\n",
        "\n",
        "pair_scores = np.ones((len(start_logits_context), len(end_logits_context)))*(-1E10)\n",
        "for i in range(len(start_logits_context-1)):\n",
        "    for j in range(i, len(end_logits_context)):\n",
        "        pair_scores[i, j] = start_logits_context[i] + end_logits_context[j]\n",
        "pair_scores_argmax = np.argmax(pair_scores)\n",
        "\n",
        "start_word_id = context_tok_to_word_id[pair_scores_argmax // len(start_logits_context)]\n",
        "end_word_id = context_tok_to_word_id[pair_scores_argmax % len(end_logits_context)]\n",
        "\n",
        "predicted_answer = ' '.join(my_context_words[start_word_id:end_word_id+1])\n",
        "\n",
        "\n",
        "from IPython.core.display import HTML\n",
        "display(HTML(f'<h2>{my_question.upper()}</h2>'))\n",
        "marked_text = str(my_context.replace(predicted_answer, f\"<mark>{predicted_answer}</mark>\"))\n",
        "display(HTML(f\"\"\"<blockquote> {marked_text} </blockquote>\"\"\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<h2>WHAT ARE THE COMMON SYMPTOMS OF THE DISEASE?</h2>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<blockquote> \n",
              "Coronavirus disease 2019 is an infectious disease caused by severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2). It was first identified in December 2019 in Wuhan, Hubei, China, and has resulted in an ongoing pandemic.\n",
              "Common symptoms include fever, cough, fatigue, <mark>shortness of breath, and loss of</mark> smell and taste.While most people have mild symptoms, some people develop acute respiratory distress syndrome (ARDS) possibly precipitated by cytokine storm, multi-organ failure, septic shock, and blood clots. The time from exposure to onset of symptoms is typically around five days, but may range from two to fourteen days.\n",
              "The virus is spread primarily via nose and mouth secretions including small droplets produced by coughing,[a] sneezing, and talking. The droplets usually do not travel through air over long distances. However, those standing in close proximity may inhale these droplets and become infected.[b] People may also become infected by touching a contaminated surface and then touching their face. The transmission may also occur through smaller droplets that are able to stay suspended in the air for longer periods of time in enclosed spaces. </blockquote>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_fTt1zl8Fbom"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}