{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT_squad",
      "provenance": [],
      "collapsed_sections": [
        "qDIlHd5Tos6C"
      ],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/luguzman/NLP/blob/main/BERT_squad.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aAUq7duyG5J0"
      },
      "source": [
        "# Fase 1: Importar dependencias"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9XYlThAmGZg1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "512c91f7-4672-4442-8a20-ef77822119ed"
      },
      "source": [
        "!pip install sentencepiece\n",
        "#!pip install tf-models-official\n",
        "!pip install tf-models-nightly # mejor instalar la versiÃ³n en desarrollo\n",
        "!pip install tf-nightly"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (0.1.95)\n",
            "Requirement already satisfied: tf-models-nightly in /usr/local/lib/python3.6/dist-packages (2.4.0.dev20210215)\n",
            "Requirement already satisfied: gin-config in /usr/local/lib/python3.6/dist-packages (from tf-models-nightly) (0.4.0)\n",
            "Requirement already satisfied: tensorflow-model-optimization>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tf-models-nightly) (0.5.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from tf-models-nightly) (1.15.0)\n",
            "Requirement already satisfied: sacrebleu in /usr/local/lib/python3.6/dist-packages (from tf-models-nightly) (1.5.0)\n",
            "Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.6/dist-packages (from tf-models-nightly) (1.4.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from tf-models-nightly) (3.2.2)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from tf-models-nightly) (0.1.95)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from tf-models-nightly) (0.8)\n",
            "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.6/dist-packages (from tf-models-nightly) (1.19.5)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.6/dist-packages (from tf-models-nightly) (5.4.1)\n",
            "Requirement already satisfied: py-cpuinfo>=3.3.0 in /usr/local/lib/python3.6/dist-packages (from tf-models-nightly) (7.0.0)\n",
            "Requirement already satisfied: pandas>=0.22.0 in /usr/local/lib/python3.6/dist-packages (from tf-models-nightly) (1.1.5)\n",
            "Requirement already satisfied: google-api-python-client>=1.6.7 in /usr/local/lib/python3.6/dist-packages (from tf-models-nightly) (1.7.12)\n",
            "Requirement already satisfied: tf-slim>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tf-models-nightly) (1.1.0)\n",
            "Requirement already satisfied: oauth2client in /usr/local/lib/python3.6/dist-packages (from tf-models-nightly) (4.1.3)\n",
            "Requirement already satisfied: tensorflow-addons in /usr/local/lib/python3.6/dist-packages (from tf-models-nightly) (0.12.1)\n",
            "Requirement already satisfied: psutil>=5.4.3 in /usr/local/lib/python3.6/dist-packages (from tf-models-nightly) (5.4.8)\n",
            "Requirement already satisfied: seqeval in /usr/local/lib/python3.6/dist-packages (from tf-models-nightly) (1.2.2)\n",
            "Requirement already satisfied: tensorflow-datasets in /usr/local/lib/python3.6/dist-packages (from tf-models-nightly) (4.0.1)\n",
            "Requirement already satisfied: google-cloud-bigquery>=0.31.0 in /usr/local/lib/python3.6/dist-packages (from tf-models-nightly) (1.21.0)\n",
            "Requirement already satisfied: Cython in /usr/local/lib/python3.6/dist-packages (from tf-models-nightly) (0.29.21)\n",
            "Requirement already satisfied: opencv-python-headless in /usr/local/lib/python3.6/dist-packages (from tf-models-nightly) (4.5.1.48)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.6/dist-packages (from tf-models-nightly) (7.0.0)\n",
            "Requirement already satisfied: tensorflow-text-nightly in /usr/local/lib/python3.6/dist-packages (from tf-models-nightly) (2.5.0.dev20210215)\n",
            "Requirement already satisfied: kaggle>=1.3.9 in /usr/local/lib/python3.6/dist-packages (from tf-models-nightly) (1.5.10)\n",
            "Requirement already satisfied: pycocotools in /usr/local/lib/python3.6/dist-packages (from tf-models-nightly) (2.0.2)\n",
            "Requirement already satisfied: tf-nightly in /usr/local/lib/python3.6/dist-packages (from tf-models-nightly) (2.5.0.dev20210215)\n",
            "Requirement already satisfied: tensorflow-hub>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tf-models-nightly) (0.11.0)\n",
            "Requirement already satisfied: dm-tree~=0.1.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-model-optimization>=0.4.1->tf-models-nightly) (0.1.5)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.6/dist-packages (from sacrebleu->tf-models-nightly) (2.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->tf-models-nightly) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->tf-models-nightly) (1.3.1)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->tf-models-nightly) (2.8.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->tf-models-nightly) (2.4.7)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.22.0->tf-models-nightly) (2018.9)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.6.7->tf-models-nightly) (0.0.4)\n",
            "Requirement already satisfied: httplib2<1dev,>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.6.7->tf-models-nightly) (0.17.4)\n",
            "Requirement already satisfied: google-auth>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.6.7->tf-models-nightly) (1.25.0)\n",
            "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.6.7->tf-models-nightly) (3.0.1)\n",
            "Requirement already satisfied: absl-py>=0.2.2 in /usr/local/lib/python3.6/dist-packages (from tf-slim>=1.1.0->tf-models-nightly) (0.10.0)\n",
            "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from oauth2client->tf-models-nightly) (4.7)\n",
            "Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.6/dist-packages (from oauth2client->tf-models-nightly) (0.2.8)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.6/dist-packages (from oauth2client->tf-models-nightly) (0.4.8)\n",
            "Requirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.6/dist-packages (from tensorflow-addons->tf-models-nightly) (2.7.1)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.6/dist-packages (from seqeval->tf-models-nightly) (0.22.2.post1)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->tf-models-nightly) (3.12.4)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->tf-models-nightly) (0.3.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->tf-models-nightly) (2.23.0)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->tf-models-nightly) (0.27.0)\n",
            "Requirement already satisfied: importlib-resources; python_version < \"3.9\" in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->tf-models-nightly) (5.1.0)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->tf-models-nightly) (2.3)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->tf-models-nightly) (0.16.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->tf-models-nightly) (4.41.1)\n",
            "Requirement already satisfied: attrs>=18.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->tf-models-nightly) (20.3.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->tf-models-nightly) (1.1.0)\n",
            "Requirement already satisfied: google-cloud-core<2.0dev,>=1.0.3 in /usr/local/lib/python3.6/dist-packages (from google-cloud-bigquery>=0.31.0->tf-models-nightly) (1.0.3)\n",
            "Requirement already satisfied: google-resumable-media!=0.4.0,<0.5.0dev,>=0.3.1 in /usr/local/lib/python3.6/dist-packages (from google-cloud-bigquery>=0.31.0->tf-models-nightly) (0.4.1)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.6/dist-packages (from kaggle>=1.3.9->tf-models-nightly) (4.0.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.6/dist-packages (from kaggle>=1.3.9->tf-models-nightly) (2020.12.5)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.6/dist-packages (from kaggle>=1.3.9->tf-models-nightly) (1.24.3)\n",
            "Requirement already satisfied: setuptools>=18.0 in /usr/local/lib/python3.6/dist-packages (from pycocotools->tf-models-nightly) (53.0.0)\n",
            "Requirement already satisfied: gast==0.4.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly->tf-models-nightly) (0.4.0)\n",
            "Requirement already satisfied: grpcio~=1.34.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly->tf-models-nightly) (1.34.1)\n",
            "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly->tf-models-nightly) (3.3.0)\n",
            "Requirement already satisfied: tb-nightly~=2.5.0.a in /usr/local/lib/python3.6/dist-packages (from tf-nightly->tf-models-nightly) (2.5.0a20210215)\n",
            "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.6/dist-packages (from tf-nightly->tf-models-nightly) (0.36.2)\n",
            "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.6/dist-packages (from tf-nightly->tf-models-nightly) (0.2.0)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tf-nightly->tf-models-nightly) (1.6.3)\n",
            "Requirement already satisfied: h5py~=3.1.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly->tf-models-nightly) (3.1.0)\n",
            "Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.6/dist-packages (from tf-nightly->tf-models-nightly) (1.12.1)\n",
            "Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.6/dist-packages (from tf-nightly->tf-models-nightly) (3.7.4.3)\n",
            "Requirement already satisfied: tf-estimator-nightly~=2.5.0.dev in /usr/local/lib/python3.6/dist-packages (from tf-nightly->tf-models-nightly) (2.5.0.dev2021021501)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.6/dist-packages (from tf-nightly->tf-models-nightly) (1.1.2)\n",
            "Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly->tf-models-nightly) (1.12)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.4.1->google-api-python-client>=1.6.7->tf-models-nightly) (4.2.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.21.3->seqeval->tf-models-nightly) (1.0.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->tensorflow-datasets->tf-models-nightly) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->tensorflow-datasets->tf-models-nightly) (2.10)\n",
            "Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-metadata->tensorflow-datasets->tf-models-nightly) (1.52.0)\n",
            "Requirement already satisfied: zipp>=0.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-resources; python_version < \"3.9\"->tensorflow-datasets->tf-models-nightly) (3.4.0)\n",
            "Requirement already satisfied: google-api-core<2.0.0dev,>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from google-cloud-core<2.0dev,>=1.0.3->google-cloud-bigquery>=0.31.0->tf-models-nightly) (1.16.0)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.6/dist-packages (from python-slugify->kaggle>=1.3.9->tf-models-nightly) (1.3)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tb-nightly~=2.5.0.a->tf-nightly->tf-models-nightly) (1.0.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from tb-nightly~=2.5.0.a->tf-nightly->tf-models-nightly) (0.3.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tb-nightly~=2.5.0.a->tf-nightly->tf-models-nightly) (3.3.3)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tb-nightly~=2.5.0.a->tf-nightly->tf-models-nightly) (1.8.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tb-nightly~=2.5.0.a->tf-nightly->tf-models-nightly) (0.4.2)\n",
            "Requirement already satisfied: cached-property; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from h5py~=3.1.0->tf-nightly->tf-models-nightly) (1.5.2)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tb-nightly~=2.5.0.a->tf-nightly->tf-models-nightly) (3.4.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tb-nightly~=2.5.0.a->tf-nightly->tf-models-nightly) (1.3.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tb-nightly~=2.5.0.a->tf-nightly->tf-models-nightly) (3.1.0)\n",
            "Requirement already satisfied: tf-nightly in /usr/local/lib/python3.6/dist-packages (2.5.0.dev20210215)\n",
            "Requirement already satisfied: grpcio~=1.34.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (1.34.1)\n",
            "Requirement already satisfied: numpy~=1.19.2 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (1.19.5)\n",
            "Requirement already satisfied: tf-estimator-nightly~=2.5.0.dev in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (2.5.0.dev2021021501)\n",
            "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (0.36.2)\n",
            "Requirement already satisfied: six~=1.15.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (1.15.0)\n",
            "Requirement already satisfied: h5py~=3.1.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (3.1.0)\n",
            "Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (1.12)\n",
            "Requirement already satisfied: tb-nightly~=2.5.0.a in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (2.5.0a20210215)\n",
            "Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (1.1.0)\n",
            "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (3.3.0)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (1.6.3)\n",
            "Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (0.10.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (3.12.4)\n",
            "Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (1.12.1)\n",
            "Requirement already satisfied: gast==0.4.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (0.4.0)\n",
            "Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (3.7.4.3)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (1.1.2)\n",
            "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (0.2.0)\n",
            "Requirement already satisfied: cached-property; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from h5py~=3.1.0->tf-nightly) (1.5.2)\n",
            "Requirement already satisfied: tensorboard-data-server<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from tb-nightly~=2.5.0.a->tf-nightly) (0.3.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tb-nightly~=2.5.0.a->tf-nightly) (53.0.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tb-nightly~=2.5.0.a->tf-nightly) (1.8.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tb-nightly~=2.5.0.a->tf-nightly) (1.0.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tb-nightly~=2.5.0.a->tf-nightly) (0.4.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tb-nightly~=2.5.0.a->tf-nightly) (3.3.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tb-nightly~=2.5.0.a->tf-nightly) (2.23.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tb-nightly~=2.5.0.a->tf-nightly) (1.25.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tb-nightly~=2.5.0.a->tf-nightly) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tb-nightly~=2.5.0.a->tf-nightly) (3.4.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tb-nightly~=2.5.0.a->tf-nightly) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tb-nightly~=2.5.0.a->tf-nightly) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tb-nightly~=2.5.0.a->tf-nightly) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tb-nightly~=2.5.0.a->tf-nightly) (2.10)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tb-nightly~=2.5.0.a->tf-nightly) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tb-nightly~=2.5.0.a->tf-nightly) (4.2.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tb-nightly~=2.5.0.a->tf-nightly) (4.7)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tb-nightly~=2.5.0.a->tf-nightly) (3.1.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tb-nightly~=2.5.0.a->tf-nightly) (3.4.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tb-nightly~=2.5.0.a->tf-nightly) (0.4.8)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eDdYqvxPHnI8"
      },
      "source": [
        "import tensorflow as tf"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zFS9XSASHvQi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "c9caa935-8e2c-4e38-80bc-0365139e9c8c"
      },
      "source": [
        "tf.__version__"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2.5.0-dev20210215'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fv38eJzSH00S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27e9ff51-5713-4299-bf0d-2b4c4ef4e721"
      },
      "source": [
        "import tensorflow_hub as hub        # librerÃ­a que nos ayuda a importar modelos con pesos ya almacenados\n",
        "\n",
        "from official.nlp.bert.tokenization import FullTokenizer        # Es doonde esta todo lo del mÃ³dulo de desarrollo de tensorflow\n",
        "from official.nlp.bert.input_pipeline import create_squad_dataset    # Nos va a peritir crear el dataset de squad a partir de importar esos ficheros que descargue y coloque en mi google drive\n",
        "from official.nlp.data.squad_lib import generate_tf_record_from_json_file   # CrearÃ¡ un fichero intermedio a partir del dataset, que es el llamada tf_record\n",
        "\n",
        "from official.nlp import optimization   # Mejora del optimizador Adam que encaja perfecto con BERT\n",
        "\n",
        "from official.nlp.data.squad_lib import read_squad_examples     # Basicamente es para leer los ficheros de evaluaciÃ³n y que podamos ver que tambiÃ©n ha sido nuestro modole para resolver los problemas de squad\n",
        "from official.nlp.data.squad_lib import FeatureWriter           # Herramienta que nos ayudarÃ¡ a tarabajar la salida de BERT y validarlo como parte del proceso final \n",
        "from official.nlp.data.squad_lib import convert_examples_to_features    # Herramienta que nos ayudarÃ¡ a tarabajar la salida de BERT y validarlo como parte del proceso final\n",
        "from official.nlp.data.squad_lib import write_predictions       # nos permitirÃ¡ escribir las predicciones en un formato json"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow_addons/utils/ensure_tf_install.py:43: UserWarning: You are currently using a nightly version of TensorFlow (2.5.0-dev20210215). \n",
            "TensorFlow Addons offers no support for the nightly versions of TensorFlow. Some things might work, some other might not. \n",
            "If you encounter a bug, do not file an issue on GitHub.\n",
            "  UserWarning,\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WjzrCZJMJN1N"
      },
      "source": [
        "import numpy as np\n",
        "import math\n",
        "import random\n",
        "import time\n",
        "import json\n",
        "import collections\n",
        "import os\n",
        "\n",
        "from google.colab import drive"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iSGw1I_zHAb5"
      },
      "source": [
        "# Fase 2: Preprocesado de Datos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SfdTEReCKFky",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "21aad6ad-6ef9-4716-baeb-fd0f9870177a"
      },
      "source": [
        "drive.mount(\"/content/drive\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E_PcOeemKcYq"
      },
      "source": [
        "# Creamos un fichero  json intermedio que contendra el conjunto de entrenamiento junto con el vocabulario \n",
        "input_meta_data = generate_tf_record_from_json_file(\n",
        "    \"/content/drive/MyDrive/Curso NLP/BERT/squad_data/train-v1.1.json\", # Acceso al fichero de entrenamiento\n",
        "    \"/content/drive/MyDrive/Curso NLP/BERT/squad_data/vocab.txt\",       # La ruta al fichero de vocabulario \n",
        "    \"/content/drive/MyDrive/Curso NLP/BERT/squad_data/train-v1.1.tf_record\") # Indicamos el path donde queremos que se guarde este fichero tensorflow record"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7XQGdnANLziM"
      },
      "source": [
        "# Almacenamos los metadatos para entrenar \n",
        "with tf.io.gfile.GFile(\"/content/drive/MyDrive/Curso NLP/BERT/squad_data/train_meta_data\", \"w\") as writer:\n",
        "    writer.write(json.dumps(input_meta_data, indent=4) + \"\\n\") \n",
        "# \\n: es simplemente para no tener problemas de lectura pues los ficheros deberÃ­an acabar siempre con un intro al final  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "295nG2zQMYSU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae1de55c-ac6a-4536-937b-5eebc402c791"
      },
      "source": [
        "# Creamos el dataset a partir de todos estos datos volcados, de estos tf_records\n",
        "BATCH_SIZE = 4  # Notemos que aquÃ­ tenemos un pequeÃ±o problema y es que al crear este dataset yo no puedo indicar un tamaÃ±o de lote demasiado\n",
        "# grande y es que aparentemente Google dice que utilizar un un batch_size demasiado grande serÃ­a bastante complicado. La razÃ³n es que las entradas\n",
        "# son mucho mÃ¡s grandes, mucho mÃ¡s largas que las que hemos utilizado en formato de tweet. Recordemos que un tweet caben 140 caracteres. AquÃ­\n",
        "# podriamos tener frases realmente largas para la tarea de preguntas y respuestas. Recordemos tambiÃ©n que la primera entrada es todo un texto, \n",
        "# podrÃ­a ser la pÃ¡gina entra de un libro que contenga informaciÃ³n de un tema.\n",
        "\n",
        "train_dataset = create_squad_dataset(\n",
        "    \"/content/drive/MyDrive/Curso NLP/BERT/squad_data/train-v1.1.tf_record\",\n",
        "    input_meta_data['max_seq_length'], # sequencia mÃ¡s larga con la que nos encontramos que en este caso es de 384\n",
        "    BATCH_SIZE,\n",
        "    is_training=True)  # A la hr de crear el conjunto squad el de entrenamiento y el de testing es diferente"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:AutoGraph could not transform <function create_squad_dataset.<locals>._select_data_from_record at 0x7fa0359f5b70> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: '<' not supported between instances of 'Literal' and 'str'\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:AutoGraph could not transform <function create_squad_dataset.<locals>._select_data_from_record at 0x7fa0359f5b70> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: '<' not supported between instances of 'Literal' and 'str'\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING: AutoGraph could not transform <function create_squad_dataset.<locals>._select_data_from_record at 0x7fa0359f5b70> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: '<' not supported between instances of 'Literal' and 'str'\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F4939RJqHRUs"
      },
      "source": [
        "# Fase 3: ConstrucciÃ³n del modelo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lcQcFi8kOc6K"
      },
      "source": [
        "## Capa Squad"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iynuYxtixV_w"
      },
      "source": [
        "Para cada uno de los vectores de salida que me devuelve BERT vamos a aplicarle una capa densa que se conectarÃ¡ con dos unidades(neuronas) en la capa de salida. Que van a ser para cada token el 1Â° nÃºmero, quÃ© tan problable es que la respuesta empiece en dicho token y el segundo nÃºmero nos dirÃ¡ lo mismo: la verosimilitud de que ese token sea el final de frase que responde a esa pregunta. Dicho esto habrÃ¡ que re-dimensionarlo y colocarlo de modo que si tenemos un pÃ¡rrafo de 200 palabras en lugar de tener 200 pares de valores. Score de ser el inicio y score de ser el final, voy preferir tener 200 valores en dos listas separadas, ie, una lista en donde vendrÃ¡ el score por palabra de ser el inicio de frase y de la misma manera la segunda pero considerando el score de que se el final de la frase."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gjmLeQjiaOo5"
      },
      "source": [
        "class BertSquadLayer(tf.keras.layers.Layer):\n",
        "\n",
        "  def __init__(self):\n",
        "    # Inicializamos la super clase. de este modo ya estarÃ¡n creadas las variables y los metodos pertenecientes a la super clase.\n",
        "    super(BertSquadLayer, self).__init__()\n",
        "    # Creamos la capa densa de la que hemos platicado.\n",
        "    self.final_dense = tf.keras.layers.Dense(\n",
        "        units=2,\n",
        "        kernel_initializer=tf.keras.initializers.TruncatedNormal(stddev=0.02))  # Google recomiendo que a la hr de hacer find tuning con BERT utilicemos una normal truncada con un valor pequeÃ±o de la desviaciÃ³n estÃ¡ndar\n",
        "\n",
        "    # inputs: son los valores que saldran de la capa de BERT\n",
        "  def call(self, inputs):\n",
        "    logits = self.final_dense(inputs) # (batch_size, seq_len, 2)\n",
        "\n",
        "    # Quiero tener un par de listas dodne cada una de ellas tenga esos 200 valores pero separado, por lo que tenemos que re-dimensionar y cortar el tensor\n",
        "    logits = tf.transpose(logits, [2, 0, 1]) # (2, batch_size, seq_len) pos 2: que tenÃ­a el score de ser inicio y final serÃ¡ lo primero, pos 0: lote de entrenamiento pos1:dimensiÃ³n que se corresponde con las palabras (longitud de la secuencia)\n",
        "    unstacked_logits = tf.unstack(logits, axis=0) # [(batch_size, seq_len), (batch_size, seq_len)] Como la primer entrada (axis=0; eje nÃºmero 0) tiene el score de ser inicio y final lo desapilamos\n",
        "    return unstacked_logits[0], unstacked_logits[1] \n",
        "    # La 1Â° me devolverÃ¡ para cada frase del lote y para cada palabra de la frase el score de ser esa palabra de ese lote el incio de la respuesta\n",
        "    # El 2Â° me devolverÃ¡ para cada frase del lote y para cada palabra de la frase el score de ser esa palabra de ese lote el final de la respuesta\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQbQFKjUOeyf"
      },
      "source": [
        "## Modelo completo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9geXlgpYE-5c"
      },
      "source": [
        "Construimos una clase que serÃ¡ todo el modelo que juntarÃ¡ la primera parte de BERT con la capa de squad."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KgkIFb1GMT81"
      },
      "source": [
        "class BERTSquad(tf.keras.Model):\n",
        "    \n",
        "    def __init__(self,\n",
        "                 name=\"bert_squad\"):\n",
        "        super(BERTSquad, self).__init__(name=name)\n",
        "        \n",
        "        self.bert_layer = hub.KerasLayer(\n",
        "            \"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\",\n",
        "            trainable=True)\n",
        "        \n",
        "        self.squad_layer = BertSquadLayer()\n",
        "    \n",
        "    # Definimos una funciÃ³n de ayuda que lo que harÃ¡ serÃ¡ formatear la entrada de BERT porque recordemos que necesitamos tener los id's, las \n",
        "    # mÃ¡scaras y los tokens de frase en ese orden correctamente para la entrada del algoritmo BERT.\n",
        "    def apply_bert(self, inputs):\n",
        "        _ , sequence_output = self.bert_layer([inputs[\"input_word_ids\"],    # Clave con la que se accede a los identificadores de cada palabra\n",
        "                                               inputs[\"input_mask\"],        # Es con el que se accede a la mÃ¡scara de aleatorizaciÃ³n.   \n",
        "                                               inputs[\"input_type_ids\"]])   # Nos indica si la frase es la nÃºmero 1 o la nÃºmero 2\n",
        "        # Recordemos que la capa de BERT devuelve el vector se corresponde a toda la frase en 1Â° posiciÃ³n y el que corresponde a cada una de\n",
        "        # las palabras tokenizadas en 2Â° posiciÃ³n. El primero te devuelve un embedding para toda la frase, para capturar el sentido global, \n",
        "        # por ejemplo. El segundo lo que hace es el embedding palabra a palabra, con lo cual tienes mÃ¡s nivel de detalle de cara al embedding, \n",
        "        # aunque tambiÃ©n mÃ¡s datos.\n",
        "        return sequence_output\n",
        "\n",
        "    def call(self, inputs):\n",
        "        seq_output = self.apply_bert(inputs)\n",
        "\n",
        "        start_logits, end_logits = self.squad_layer(seq_output)\n",
        "        \n",
        "        return start_logits, end_logits"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CoSG_xnQ82WV"
      },
      "source": [
        "# Ejemplo una vez que se haya corrido lo necesario para obtener my_input_dict\r\n",
        "bert_layer_prueba = hub.KerasLayer(\r\n",
        "            \"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\",\r\n",
        "            trainable=True)\r\n",
        "a , sequence_output = bert_layer_prueba([my_input_dict[\"input_word_ids\"],    # Clave con la que se accede a los identificadores de cada palabra\r\n",
        "                                               my_input_dict[\"input_mask\"],        # Es con el que se accede a la mÃ¡scara de aleatorizaciÃ³n.   \r\n",
        "                                               my_input_dict[\"input_type_ids\"]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d9Xz1t_89FMK",
        "outputId": "a451834b-678c-4fe6-8c2f-25ca3af47536"
      },
      "source": [
        "sequence_output[0]"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(384, 768), dtype=float32, numpy=\n",
              "array([[-0.32203275,  0.33639544, -0.52068424, ..., -0.48251238,\n",
              "         0.19905187,  0.5228576 ],\n",
              "       [ 0.24670917,  0.3269383 , -0.34180155, ..., -0.03714953,\n",
              "         0.80947983,  0.55424744],\n",
              "       [ 0.20521492,  0.6600305 , -0.19028215, ..., -0.7606953 ,\n",
              "         0.16049762,  0.0456326 ],\n",
              "       ...,\n",
              "       [-0.04197102, -0.3263684 ,  0.1733743 , ..., -0.02852666,\n",
              "        -0.00841513,  0.17027545],\n",
              "       [ 0.2570309 , -0.32238752,  0.2062163 , ..., -0.34203506,\n",
              "        -0.07409365, -0.07363216],\n",
              "       [-0.51614445, -0.48724455, -0.30160457, ...,  0.2570429 ,\n",
              "         0.22799626,  0.09290694]], dtype=float32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lBmSU2RnHV_a"
      },
      "source": [
        "# Fase 4: Entrenamiento"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WnA3WHwRIHAZ"
      },
      "source": [
        "## CreaciÃ³n de la IA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HIwy4VYON9M9",
        "outputId": "c52037c2-8442-41e6-a05c-c698bba28353",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "sum(1 for _ in tf.compat.v1.io.tf_record_iterator(\"/content/drive/MyDrive/Curso NLP/BERT/squad_data/train-v1.1.tf_record\")) "
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "88641"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JEUxomxENRoJ"
      },
      "source": [
        "TRAIN_DATA_SIZE = sum(1 for _ in tf.compat.v1.io.tf_record_iterator(\"/content/drive/MyDrive/Curso NLP/BERT/squad_data/train-v1.1.tf_record\"))      # TamaÃ±o/longitud de nuestro conjunto de entrenamiento. 88641\n",
        "# Este es un dataset bastante pesado para entrenar y para poder entrenarlo en google colab sin que nos eche es hacer un pequeÃ±o truco y es no \n",
        "# utilizar todo el data set para entrenar sino que queremos ver BERT funciona correctamente por lo que definimos:\n",
        "NB_BATCHES_TRAIN = 2000     # Esto representa aprox un 10% del total de lotes que tenemos 88641/4*.10\n",
        "BATCH_SIZE = 4              \n",
        "NB_EPOCHS = 3               # Inclusive 2 podrÃ­a ser mÃ¡s que suficiente\n",
        "# VersiÃ³n modificada del optimizador Adam por Google creado explictamemnte para esta tarea:\n",
        "INIT_LR = 5e-5              # Ratio de aprendizaje al inicio. Se define con el propÃ³sito de que el ratio de aprendizaje sea pequeÃ±o al inicio para que pueda ir experimentando y vaya incrementando durante un ratio determinado de pasos:\n",
        "WARMUP_STEPS = int(NB_BATCHES_TRAIN * 0.1)  # Ratio determinado de pasos que serÃ¡ el 10% del lote 2000*10% = 200"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Pg6EKe2daFy"
      },
      "source": [
        "train_dataset_light = train_dataset.take(NB_BATCHES_TRAIN)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CXnH3CQWH5tt"
      },
      "source": [
        "Creamos y compilamos nuestro modelo BERTSquad"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eHd5MzTdNIZq"
      },
      "source": [
        "bert_squad = BERTSquad()"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dQrfVYTrIGMN"
      },
      "source": [
        "Creamos nuestro optimizador Adam modificado por Google."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G0cvDjBm_KXT"
      },
      "source": [
        "optimizer = optimization.create_optimizer(\n",
        "    init_lr=INIT_LR,\n",
        "    num_train_steps=NB_BATCHES_TRAIN,\n",
        "    num_warmup_steps=WARMUP_STEPS)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "96ns-oGSdRXz"
      },
      "source": [
        "Creamos nuestra funciÃ³n de perdidas personalizada. Recordemos que nuestra capa final nos devolverÃ¡ 2 vectores uno en donde nos devuelve la verosimilitud de que un token sea el token de incio y otro considerando cual es el token final. Al final de cuentas suena como que hay que resolver un problema de clasificaciÃ³n pues hay que decidir si es o no el token que buscamos. Por lo tanto, utilizaremos sparse_categorical_crossentropy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I6kG-HpzVK7v"
      },
      "source": [
        "# labels: etiquetas reales\n",
        "# model_outputs: salidas del modelo\n",
        "def squad_loss_fn(labels, model_outputs):\n",
        "    start_positions = labels['start_positions']\n",
        "    end_positions = labels['end_positions']\n",
        "    start_logits, end_logits = model_outputs\n",
        "\n",
        "    start_loss = tf.keras.backend.sparse_categorical_crossentropy(\n",
        "        start_positions, start_logits, from_logits=True) \n",
        "# from_logits=True: Significa que la respuesta de nuestro modelo, en este caso el segundo parÃ¡metro, no es reralmente una probabilidad y es que en \n",
        "# ningÃºn momento hemos aplicado una funciÃ³n softmax o algo asÃ­ que se encargue de que todos los valores esten entre 0 y 1 y la suma de todos sea\n",
        "# igual a 1. Ahora solo tenemos nÃºmeros de medidas de verosimilitud, pero que en principio serÃ¡n nÃºmeros cualesquiera, podrÃ­an ser positivos \n",
        "# o negativos y al aplicar from_logits=True, la funciÃ³n sparse_categorical_crossentropy se encargarÃ¡ de hacer las modificaciones adecuadas \n",
        "# y de comparar la respuesta para devovlernos esa pÃ©rdida. \n",
        "\n",
        "    end_loss = tf.keras.backend.sparse_categorical_crossentropy(\n",
        "        end_positions, end_logits, from_logits=True)\n",
        "    \n",
        "    # Obtenemos la media de ambas perdidas\n",
        "    total_loss = (tf.reduce_mean(start_loss) + tf.reduce_mean(end_loss)) / 2    \n",
        "    # reduce_mean: ya que queremos obtener la pÃ©rdida de todo el lote. En un principio obtendrÃ­amos 4 valores ya que recordemos que son 4 frases \n",
        "    # (dado que BATCH_SIZE = 4)\n",
        "\n",
        "    return total_loss\n",
        "\n",
        "# Finalmente, como queremos construir nuestro propio bucle de entrenamiento personalizado, nos va a intersar tener un tracking de la funciÃ³n \n",
        "# de perdidas (global duarante toda la fase de training) que acabamos de crear y que no es una de las estÃ¡ndar que utilizamos de las de tf.\n",
        "train_loss = tf.keras.metrics.Mean(name=\"train_loss\")"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HN_C_WA5R_Cb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "783e1376-0065-4189-f959-8d3f62855e25"
      },
      "source": [
        "next(iter(train_dataset_light))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "({'input_mask': <tf.Tensor: shape=(4, 384), dtype=int32, numpy=\n",
              "  array([[1, 1, 1, ..., 0, 0, 0],\n",
              "         [1, 1, 1, ..., 0, 0, 0],\n",
              "         [1, 1, 1, ..., 0, 0, 0],\n",
              "         [1, 1, 1, ..., 0, 0, 0]], dtype=int32)>,\n",
              "  'input_type_ids': <tf.Tensor: shape=(4, 384), dtype=int32, numpy=\n",
              "  array([[0, 0, 0, ..., 0, 0, 0],\n",
              "         [0, 0, 0, ..., 0, 0, 0],\n",
              "         [0, 0, 0, ..., 0, 0, 0],\n",
              "         [0, 0, 0, ..., 0, 0, 0]], dtype=int32)>,\n",
              "  'input_word_ids': <tf.Tensor: shape=(4, 384), dtype=int32, numpy=\n",
              "  array([[ 101, 2216, 2040, ...,    0,    0,    0],\n",
              "         [ 101, 2054, 7017, ...,    0,    0,    0],\n",
              "         [ 101, 2054, 2001, ...,    0,    0,    0],\n",
              "         [ 101, 2054, 2565, ...,    0,    0,    0]], dtype=int32)>},\n",
              " {'end_positions': <tf.Tensor: shape=(4,), dtype=int32, numpy=array([111,  74,  82, 102], dtype=int32)>,\n",
              "  'start_positions': <tf.Tensor: shape=(4,), dtype=int32, numpy=array([109,  71,  80, 100], dtype=int32)>})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lLZ2wEgzjzKF"
      },
      "source": [
        "Compilamos el modelo:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V-iE2QFC_KRI"
      },
      "source": [
        "bert_squad.compile(optimizer,\n",
        "                   squad_loss_fn)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hjp-_4OyTbuK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36440c1d-c28e-46c5-a51e-d8a64f9a7ccf"
      },
      "source": [
        "# Crearemos un sistema de checkpoint en google colab que en caso de que se reinicie la sesiÃ³n podemos reanudar\n",
        "# desde elÃºltimo checkpoint que se haya guardado o incluso mÃ¡s adelante , seguir entrenando con mÃ¡s textos \n",
        "# desde donde lo habÃ­amos dejado en lugar de tener que crear la red neuronal desde el inicio.\n",
        "\n",
        "# Definimos una ruta dentro de mi Google Drive donde se guardarÃ¡n los checkpoints\n",
        "checkpoint_path = \"./drive/MyDrive/Curso NLP/BERT/ckpt_bert_squad/\"\n",
        "\n",
        "# Guardamos el modelo que queremos guardar autoamticamente siempre que sea posible\n",
        "ckpt = tf.train.Checkpoint(bert_squad=bert_squad)\n",
        "\n",
        "# Definimos el manager que serÃ¡ el encargado de guardar los checkpoints en la ruta establecida\n",
        "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=1)\n",
        "# max_to_keep: Es para definir que queremos que siempre se guarden los Ãºltimos 5 checkpoints.\n",
        "# En este caso si quisieramos guardar todos bastarÃ­a con poner max_to_keep = 3 pues son 3 epochs\n",
        "\n",
        "# Las siguientes lÃ­neas de cÃ³digo lo que hacen es preguntarle al Checkpoint Manager si hay o no hay \n",
        "# Ãºltimo checkpoint\n",
        "if ckpt_manager.latest_checkpoint:      # esta linea devuelve un None si no hay checkpoint previo (If None:)\n",
        "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
        "    print(\"Ãltimo checkpoint restaurado!!\")"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Ãltimo checkpoint restaurado!!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bDgEq09xIOOl"
      },
      "source": [
        "## Entrenamiento personalizado"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ywelW3uaSbT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "1c27881e-7ebe-438e-b6dd-6830bd4bcc79"
      },
      "source": [
        "for epoch in range(NB_EPOCHS):\n",
        "    print(\"Inicio del Epoch {}\".format(epoch+1))\n",
        "    start = time.time()\n",
        "    \n",
        "    train_loss.reset_states()   # Devuelve toda la funciÃ³n de pÃ©rdidas a 0 y asÃ­ durante un epoch llevar traking solo de este\n",
        "    \n",
        "    for (batch, (inputs, targets)) in enumerate(train_dataset_light):\n",
        "        with tf.GradientTape() as tape:\n",
        "            model_outputs = bert_squad(inputs)\n",
        "            loss = squad_loss_fn(targets, model_outputs)\n",
        "        \n",
        "        gradients = tape.gradient(loss, bert_squad.trainable_variables)     # Se obtiene el gradiente solo para las variables entrenables de BERT\n",
        "        optimizer.apply_gradients(zip(gradients, bert_squad.trainable_variables))   # juntamos la info de los gradientes y a que variables pertenecen\n",
        "        \n",
        "        train_loss(loss)\n",
        "        \n",
        "        # Si el lote es mÃºltiplo de 50 que se impriman los siguientes valores:\n",
        "        if batch % 50 == 0:\n",
        "            print(\"Epoch {} Lote {} PÃ©rdida {:.4f}\".format(\n",
        "                epoch+1, batch, train_loss.result()))\n",
        "        \n",
        "        # Si el lote es mÃºltiplo de 500 se harÃ¡ un checkpoint\n",
        "        if batch % 500 == 0:\n",
        "            ckpt_save_path = ckpt_manager.save()\n",
        "            print(\"Guardando checkpoint para el epoch {} en el directorio {}\".format(epoch+1,\n",
        "                                                                ckpt_save_path))\n",
        "    print(\"Tiempo total para entrenar 1 epoch: {} segs\\n\".format(time.time() - start))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Inicio del Epoch 1\n",
            "Epoch 1 Lote 0 PÃ©rdida 6.0621\n",
            "Guardando checkpoint para el epoch 1 en el directorio ./drive/My Drive/Curso de NLP/BERT/ckpt_bert_squad/ckpt-1\n",
            "Epoch 1 Lote 50 PÃ©rdida 5.8209\n",
            "Epoch 1 Lote 100 PÃ©rdida 5.2858\n",
            "Epoch 1 Lote 150 PÃ©rdida 4.6081\n",
            "Epoch 1 Lote 200 PÃ©rdida 4.1292\n",
            "Epoch 1 Lote 250 PÃ©rdida 3.7406\n",
            "Epoch 1 Lote 300 PÃ©rdida 3.4795\n",
            "Epoch 1 Lote 350 PÃ©rdida 3.3128\n",
            "Epoch 1 Lote 400 PÃ©rdida 3.1128\n",
            "Epoch 1 Lote 450 PÃ©rdida 2.9574\n",
            "Epoch 1 Lote 500 PÃ©rdida 2.8129\n",
            "Guardando checkpoint para el epoch 1 en el directorio ./drive/My Drive/Curso de NLP/BERT/ckpt_bert_squad/ckpt-2\n",
            "Epoch 1 Lote 550 PÃ©rdida 2.6840\n",
            "Epoch 1 Lote 600 PÃ©rdida 2.6034\n",
            "Epoch 1 Lote 650 PÃ©rdida 2.5247\n",
            "Epoch 1 Lote 700 PÃ©rdida 2.4478\n",
            "Epoch 1 Lote 750 PÃ©rdida 2.3795\n",
            "Epoch 1 Lote 800 PÃ©rdida 2.3212\n",
            "Epoch 1 Lote 850 PÃ©rdida 2.2794\n",
            "Epoch 1 Lote 900 PÃ©rdida 2.2269\n",
            "Epoch 1 Lote 950 PÃ©rdida 2.1805\n",
            "Epoch 1 Lote 1000 PÃ©rdida 2.1184\n",
            "Guardando checkpoint para el epoch 1 en el directorio ./drive/My Drive/Curso de NLP/BERT/ckpt_bert_squad/ckpt-3\n",
            "Epoch 1 Lote 1050 PÃ©rdida 2.0712\n",
            "Epoch 1 Lote 1100 PÃ©rdida 2.0192\n",
            "Epoch 1 Lote 1150 PÃ©rdida 1.9696\n",
            "Epoch 1 Lote 1200 PÃ©rdida 1.9270\n",
            "Epoch 1 Lote 1250 PÃ©rdida 1.9186\n",
            "Epoch 1 Lote 1300 PÃ©rdida 1.8983\n",
            "Epoch 1 Lote 1350 PÃ©rdida 1.8858\n",
            "Epoch 1 Lote 1400 PÃ©rdida 1.8687\n",
            "Epoch 1 Lote 1450 PÃ©rdida 1.8431\n",
            "Epoch 1 Lote 1500 PÃ©rdida 1.8203\n",
            "Guardando checkpoint para el epoch 1 en el directorio ./drive/My Drive/Curso de NLP/BERT/ckpt_bert_squad/ckpt-4\n",
            "Epoch 1 Lote 1550 PÃ©rdida 1.8009\n",
            "Epoch 1 Lote 1600 PÃ©rdida 1.7833\n",
            "Epoch 1 Lote 1650 PÃ©rdida 1.7777\n",
            "Epoch 1 Lote 1700 PÃ©rdida 1.7699\n",
            "Epoch 1 Lote 1750 PÃ©rdida 1.7521\n",
            "Epoch 1 Lote 1800 PÃ©rdida 1.7437\n",
            "Epoch 1 Lote 1850 PÃ©rdida 1.7187\n",
            "Epoch 1 Lote 1900 PÃ©rdida 1.6900\n",
            "Epoch 1 Lote 1950 PÃ©rdida 1.6774\n",
            "Tiempo total para entrenar 1 epoch: 5262.5173897743225 segs\n",
            "\n",
            "Inicio del Epoch 2\n",
            "Epoch 2 Lote 0 PÃ©rdida 1.1600\n",
            "Guardando checkpoint para el epoch 2 en el directorio ./drive/My Drive/Curso de NLP/BERT/ckpt_bert_squad/ckpt-5\n",
            "Epoch 2 Lote 50 PÃ©rdida 1.1323\n",
            "Epoch 2 Lote 100 PÃ©rdida 1.1650\n",
            "Epoch 2 Lote 150 PÃ©rdida 1.0600\n",
            "Epoch 2 Lote 200 PÃ©rdida 1.0460\n",
            "Epoch 2 Lote 250 PÃ©rdida 0.9981\n",
            "Epoch 2 Lote 300 PÃ©rdida 0.9548\n",
            "Epoch 2 Lote 350 PÃ©rdida 0.9416\n",
            "Epoch 2 Lote 400 PÃ©rdida 0.8956\n",
            "Epoch 2 Lote 450 PÃ©rdida 0.8426\n",
            "Epoch 2 Lote 500 PÃ©rdida 0.8125\n",
            "Guardando checkpoint para el epoch 2 en el directorio ./drive/My Drive/Curso de NLP/BERT/ckpt_bert_squad/ckpt-6\n",
            "Epoch 2 Lote 550 PÃ©rdida 0.7875\n",
            "Epoch 2 Lote 600 PÃ©rdida 0.7739\n",
            "Epoch 2 Lote 650 PÃ©rdida 0.7629\n",
            "Epoch 2 Lote 700 PÃ©rdida 0.7346\n",
            "Epoch 2 Lote 750 PÃ©rdida 0.7113\n",
            "Epoch 2 Lote 800 PÃ©rdida 0.6930\n",
            "Epoch 2 Lote 850 PÃ©rdida 0.6815\n",
            "Epoch 2 Lote 900 PÃ©rdida 0.6704\n",
            "Epoch 2 Lote 950 PÃ©rdida 0.6603\n",
            "Epoch 2 Lote 1000 PÃ©rdida 0.6411\n",
            "Guardando checkpoint para el epoch 2 en el directorio ./drive/My Drive/Curso de NLP/BERT/ckpt_bert_squad/ckpt-7\n",
            "Epoch 2 Lote 1050 PÃ©rdida 0.6289\n",
            "Epoch 2 Lote 1100 PÃ©rdida 0.6145\n",
            "Epoch 2 Lote 1150 PÃ©rdida 0.6004\n",
            "Epoch 2 Lote 1200 PÃ©rdida 0.5895\n",
            "Epoch 2 Lote 1250 PÃ©rdida 0.5867\n",
            "Epoch 2 Lote 1300 PÃ©rdida 0.5894\n",
            "Epoch 2 Lote 1350 PÃ©rdida 0.6004\n",
            "Epoch 2 Lote 1400 PÃ©rdida 0.5978\n",
            "Epoch 2 Lote 1450 PÃ©rdida 0.5938\n",
            "Epoch 2 Lote 1500 PÃ©rdida 0.5948\n",
            "Guardando checkpoint para el epoch 2 en el directorio ./drive/My Drive/Curso de NLP/BERT/ckpt_bert_squad/ckpt-8\n",
            "Epoch 2 Lote 1550 PÃ©rdida 0.5932\n",
            "Epoch 2 Lote 1600 PÃ©rdida 0.5958\n",
            "Epoch 2 Lote 1650 PÃ©rdida 0.6041\n",
            "Epoch 2 Lote 1700 PÃ©rdida 0.6120\n",
            "Epoch 2 Lote 1750 PÃ©rdida 0.6117\n",
            "Epoch 2 Lote 1800 PÃ©rdida 0.6166\n",
            "Epoch 2 Lote 1850 PÃ©rdida 0.6114\n",
            "Epoch 2 Lote 1900 PÃ©rdida 0.6073\n",
            "Epoch 2 Lote 1950 PÃ©rdida 0.6167\n",
            "Tiempo total para entrenar 1 epoch: 5323.075678825378 segs\n",
            "\n",
            "Inicio del Epoch 3\n",
            "Epoch 3 Lote 0 PÃ©rdida 1.6725\n",
            "Guardando checkpoint para el epoch 3 en el directorio ./drive/My Drive/Curso de NLP/BERT/ckpt_bert_squad/ckpt-9\n",
            "Epoch 3 Lote 50 PÃ©rdida 1.1375\n",
            "Epoch 3 Lote 100 PÃ©rdida 1.0644\n",
            "Epoch 3 Lote 150 PÃ©rdida 1.0579\n",
            "Epoch 3 Lote 200 PÃ©rdida 1.0375\n",
            "Epoch 3 Lote 250 PÃ©rdida 1.0067\n",
            "Epoch 3 Lote 300 PÃ©rdida 0.9554\n",
            "Epoch 3 Lote 350 PÃ©rdida 0.9430\n",
            "Epoch 3 Lote 400 PÃ©rdida 0.8927\n",
            "Epoch 3 Lote 450 PÃ©rdida 0.8453\n",
            "Epoch 3 Lote 500 PÃ©rdida 0.8164\n",
            "Guardando checkpoint para el epoch 3 en el directorio ./drive/My Drive/Curso de NLP/BERT/ckpt_bert_squad/ckpt-10\n",
            "Epoch 3 Lote 550 PÃ©rdida 0.7860\n",
            "Epoch 3 Lote 600 PÃ©rdida 0.7754\n",
            "Epoch 3 Lote 650 PÃ©rdida 0.7596\n",
            "Epoch 3 Lote 700 PÃ©rdida 0.7369\n",
            "Epoch 3 Lote 750 PÃ©rdida 0.7108\n",
            "Epoch 3 Lote 800 PÃ©rdida 0.6945\n",
            "Epoch 3 Lote 850 PÃ©rdida 0.6831\n",
            "Epoch 3 Lote 900 PÃ©rdida 0.6735\n",
            "Epoch 3 Lote 950 PÃ©rdida 0.6600\n",
            "Epoch 3 Lote 1000 PÃ©rdida 0.6426\n",
            "Guardando checkpoint para el epoch 3 en el directorio ./drive/My Drive/Curso de NLP/BERT/ckpt_bert_squad/ckpt-11\n",
            "Epoch 3 Lote 1050 PÃ©rdida 0.6262\n",
            "Epoch 3 Lote 1100 PÃ©rdida 0.6146\n",
            "Epoch 3 Lote 1150 PÃ©rdida 0.6007\n",
            "Epoch 3 Lote 1200 PÃ©rdida 0.5895\n",
            "Epoch 3 Lote 1250 PÃ©rdida 0.5879\n",
            "Epoch 3 Lote 1300 PÃ©rdida 0.5926\n",
            "Epoch 3 Lote 1350 PÃ©rdida 0.6027\n",
            "Epoch 3 Lote 1400 PÃ©rdida 0.6001\n",
            "Epoch 3 Lote 1450 PÃ©rdida 0.5950\n",
            "Epoch 3 Lote 1500 PÃ©rdida 0.5975\n",
            "Guardando checkpoint para el epoch 3 en el directorio ./drive/My Drive/Curso de NLP/BERT/ckpt_bert_squad/ckpt-12\n",
            "Epoch 3 Lote 1550 PÃ©rdida 0.5964\n",
            "Epoch 3 Lote 1600 PÃ©rdida 0.5967\n",
            "Epoch 3 Lote 1650 PÃ©rdida 0.6051\n",
            "Epoch 3 Lote 1700 PÃ©rdida 0.6109\n",
            "Epoch 3 Lote 1750 PÃ©rdida 0.6126\n",
            "Epoch 3 Lote 1800 PÃ©rdida 0.6146\n",
            "Epoch 3 Lote 1850 PÃ©rdida 0.6134\n",
            "Epoch 3 Lote 1900 PÃ©rdida 0.6089\n",
            "Epoch 3 Lote 1950 PÃ©rdida 0.6163\n",
            "Tiempo total para entrenar 1 epoch: 5372.474010705948 segs\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v6WquTCiIR7t"
      },
      "source": [
        "# Fase 5: EvaluaciÃ³n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qDIlHd5Tos6C"
      },
      "source": [
        "## PreparaciÃ³n de la evaluaciÃ³n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kU7_AyTIpTTJ"
      },
      "source": [
        "Get the dev set in the session"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VGKl84s5WrhD"
      },
      "source": [
        "eval_examples = read_squad_examples(\n",
        "    \"/content/drive/MyDrive/Curso NLP/BERT/squad_data/dev-v1.1.json\",\n",
        "    is_training=False,\n",
        "    version_2_with_negative=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DAEUcZDSpYLD"
      },
      "source": [
        "Define the function that will write the tf_record file for the dev set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yCVmIgnEo83o"
      },
      "source": [
        "eval_writer = FeatureWriter(\n",
        "    filename=os.path.join(\"/content/drive/MyDrive/Curso NLP/BERT/squad_data/\",\n",
        "                          \"eval.tf_record\"),\n",
        "    is_training=False)\n",
        "# FeatureWriter: a la hr de llevar a cabo la parte de la evaluaciÃ³n necesitamos contar con features, los\n",
        "# cuales no son mÃ¡s que protocolos que tf utiliza a la hr de trabajar. Son herramientas que se utlizan para\n",
        "# proporcionar la informaciÃ³n o almacenar la informaciÃ³n. En otras palabras es una convenciÃ³n si \n",
        "# queremos usar tf squad."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C8aSLFdmp71I"
      },
      "source": [
        "Create a tokenizer for future information needs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oH5exQ7KwnuH"
      },
      "source": [
        "my_bert_layer = hub.KerasLayer(\n",
        "    \"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\",\n",
        "    trainable=False)\n",
        "vocab_file = my_bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
        "do_lower_case = my_bert_layer.resolved_object.do_lower_case.numpy()\n",
        "tokenizer = FullTokenizer(vocab_file, do_lower_case)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vdHudjJ_qAzo"
      },
      "source": [
        "Define the function that add the features (feature is a protocol in tensorflow) to our eval_features list"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bmQ5GtoTxRjU"
      },
      "source": [
        "# Como he dicho los features no son mÃ¡s que pedazos de info que siguen un protocolo, que siguen una\n",
        "# convenciÃ³n y por tanto solo lo convierto al formato que me intersa.\n",
        "def _append_feature(feature, is_padding):\n",
        "    if not is_padding:\n",
        "        eval_features.append(feature)\n",
        "    eval_writer.process_feature(feature)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HLAcwCiaqHi_"
      },
      "source": [
        "Create the eval features and the writes the tf.record file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mz7kGYmUwGQb"
      },
      "source": [
        "eval_features = []\n",
        "dataset_size = convert_examples_to_features(\n",
        "    examples=eval_examples,\n",
        "    tokenizer=tokenizer,\n",
        "    max_seq_length=384,     # este valor varia dependiendo los datos\n",
        "    doc_stride=128,\n",
        "    max_query_length=64,\n",
        "    is_training=False,\n",
        "    output_fn=_append_feature,\n",
        "    batch_size=4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WpZfwPEwMabx"
      },
      "source": [
        "eval_writer.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TbKhx3zuq844"
      },
      "source": [
        "Load the ready-to-be-used dataset to our session"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eUqYvG5TxctF"
      },
      "source": [
        "BATCH_SIZE = 4\n",
        "\n",
        "eval_dataset = create_squad_dataset(\n",
        "    \"/content/drive/MyDrive/Curso NLP/BERT/squad_data/eval.tf_record\",\n",
        "    384,#input_meta_data['max_seq_length'],\n",
        "    BATCH_SIZE,\n",
        "    is_training=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tRbKrFYoo8e8"
      },
      "source": [
        "## Llevar a cabo las prediccioness"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KyckEWDbrLEX"
      },
      "source": [
        "Definir un cierto tipo de colecciÃ³n (como un diccionario). Esto nos serÃ¡ util ya que como seguiremos trabajando con lotes en un momento dado, vamos querer crear una funciÃ³n que devuelva los elementos de salida por lote uno detrÃ¡s de otro. Entonces, para ello nos va ir muy bien que los elementos vayan nombrados."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tyWOUKaDP0H0"
      },
      "source": [
        "# Creamos una tupla nombrado lo que nos permitirÃ¡ crear tuplas dodne cada elemento ahora tenga un nombre\n",
        "# en lugar de las tuplas estandÃ¡r a las que se accede por posiciÃ³n.\n",
        "RawResult = collections.namedtuple(\"RawResult\",\n",
        "                                   [\"unique_id\", \"start_logits\", \"end_logits\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28abKVvqrRa4"
      },
      "source": [
        "Devuelve cada elemento del lote de salida, uno por uno. Dado un elemento del lote de salida lo va a iterar en este formato para devolverme empaquetada la informaciÃ³n que me interesa. Enotras palabras de la salida del modelos solo me quedarÃ© con unique_ids, start_logits y end_logits. Lo que hace zip es que empaqueta la info en 3 listas con una misma longitud."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BScaA0SZQgQW"
      },
      "source": [
        "def get_raw_results(predictions):\n",
        "    for unique_ids, start_logits, end_logits in zip(predictions['unique_ids'],\n",
        "                                                    predictions['start_logits'],\n",
        "                                                    predictions['end_logits']):\n",
        "        yield RawResult(\n",
        "            unique_id=unique_ids.numpy(),       # lo convertimos a formato numpy para poder manejarlo mÃ¡s comodamente\n",
        "            start_logits=start_logits.numpy().tolist(), # lo convertimos a lista ya que mÃ¡s adelante utilizarÃ© una de las funciones de Google que aceptan como entrada a estas como listas\n",
        "            end_logits=end_logits.numpy().tolist())\n",
        "# si no conocen yield: significa que esta funciÃ³n realmente actuarÃ¡ como si fuera un iterador, ie, podemos\n",
        "# hacer un bucle sobre este objeto cada vez que la llamemos. FuncionarÃ¡ exactamente como un iterador, de\n",
        "# modo que podemos hacer un bucle a trÃ¡ves del objeto get_raw_results de una predicciÃ³n e ir obteniendo cada \n",
        "# una de la entradas. De ahÃ­ esa gracia de yield porque devolverÃ¡ un elemento y esperarÃ¡ a que un bucle pida\n",
        "# la siguiente observaciÃ³n. Entonces, como la predicciÃ³n nos vendrÃ¡ en lotes yo le puedo dar todo el lote \n",
        "# e iterar directamente sobre el resultado de get_raw_results para devolver empaquetado el id, el loggit \n",
        "# de entrada y el loggit de salida."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JiLOOmnLre5C"
      },
      "source": [
        "Hacemos nuestras predicciones"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qqD78Xdjrvpn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 530
        },
        "outputId": "b128075d-bfc1-4df6-852e-f1020bbc404f"
      },
      "source": [
        "all_results = []\n",
        "for count, inputs in enumerate(eval_dataset):\n",
        "    x, _ = inputs   # nos quedamos como entrada solo la pregunta y nos deshacemos de la respuesta\n",
        "    unique_ids = x.pop(\"unique_ids\")    # Quitamos el identificador ya que si se lo doy a la capa de BERT no sabrÃ¡ muy bien que hacer con Ã©l\n",
        "    start_logits, end_logits = bert_squad(x, training=False)\n",
        "    output_dict = dict(\n",
        "        unique_ids=unique_ids,  # Fijemonos que he hecho pop pero lo he guardado en una variable. Me quedado con una referencia.\n",
        "        start_logits=start_logits,\n",
        "        end_logits=end_logits)\n",
        "    for result in get_raw_results(output_dict): # Como get_raw_results es un iterador no me darÃ¡ ningun problema\n",
        "        all_results.append(result)\n",
        "    if count % 100 == 0:\n",
        "        print(\"{}/{}\".format(count, 2709))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0/2709\n",
            "100/2709\n",
            "200/2709\n",
            "300/2709\n",
            "400/2709\n",
            "500/2709\n",
            "600/2709\n",
            "700/2709\n",
            "800/2709\n",
            "900/2709\n",
            "1000/2709\n",
            "1100/2709\n",
            "1200/2709\n",
            "1300/2709\n",
            "1400/2709\n",
            "1500/2709\n",
            "1600/2709\n",
            "1700/2709\n",
            "1800/2709\n",
            "1900/2709\n",
            "2000/2709\n",
            "2100/2709\n",
            "2200/2709\n",
            "2300/2709\n",
            "2400/2709\n",
            "2500/2709\n",
            "2600/2709\n",
            "2700/2709\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PjQ6kIqGriHr"
      },
      "source": [
        "Escribimos nuestras predicciones en un fichero JSON que funcionarÃ¡ con el script de evaluaciÃ³n. El que realmente nos intersa es el primero los otros dos son ficheros que le hace falta Google y hay que mencionar el path en donde estÃ¡n si no pues fallarÃ­a la funciÃ³n."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "esLdRf7uM3Lz"
      },
      "source": [
        "output_prediction_file = \"/content/drive/MyDrive/Curso NLP/BERT/squad_data/predictions.json\"\n",
        "output_nbest_file = \"/content/drive/MyDrive/Curso NLP/BERT/squad_data/nbest_predictions.json\"\n",
        "output_null_log_odds_file = \"/content/drive/MyDrive/Curso NLP/BERT/squad_data/null_odds.json\"\n",
        "\n",
        "write_predictions(\n",
        "    eval_examples,  \n",
        "    eval_features,  # formato interno que habÃ­amos meniconado que serÃ­a necesario para poder crear el fichero correctamente\n",
        "    all_results,    # los resultados que hemos obtenido \n",
        "    20,             # No sÃ© que sea pero son necesarios\n",
        "    30,             # No sÃ© que sea pero son necesarios\n",
        "    True,           # Ya que hemos echo uso del lower case en la fase de tokenizaciÃ³n.\n",
        "    output_prediction_file, \n",
        "    output_nbest_file,\n",
        "    output_null_log_odds_file,\n",
        "    verbose=False)  # Para que no se nos impirma mucha basura en la pantalla mientras corre"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-eaIHyDIYHHx"
      },
      "source": [
        "## PredicciÃ³n casera"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P0F4l5h8Zdha"
      },
      "source": [
        "### CreaciÃ³n del diccionario de entrada"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IrhHzx7ycXXo"
      },
      "source": [
        "Concatenamos la pregunta y el contexto, separados por `[\"SEP\"]`, tras la tokenizaciÃ³n, tal cual como lo hicimos con el conjunto de entrenamiento.\n",
        "\n",
        "Lo importante a recordar es que queremos que nuestra respuesta empiece y termine con una palabra real. Por ejemplo, la palabra \"ecologically\" es tokenizada como `[\"ecological\", \"##ly\"]`, y si el token de fin es `[\"ecological\"]` queremos usar la palabra \"ecologically\" como palabra final (del mismo modo si el token de fin es`[\"##ly\"]`). Por eso, empezamos dividiendo nuestro contexto en palabras, y luego pasamos a tokens, recordando quÃ© token se corresponde con quÃ© palabra (ver la funciÃ³n `tokenize_context()` para mÃ¡s detalle)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_tuNXt98Zm4u"
      },
      "source": [
        "#### Ãtiles varios"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OBjGoQ_wfmml"
      },
      "source": [
        "my_bert_layer = hub.KerasLayer(\n",
        "    \"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\",\n",
        "    trainable=False)\n",
        "vocab_file = my_bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
        "do_lower_case = my_bert_layer.resolved_object.do_lower_case.numpy()\n",
        "tokenizer = FullTokenizer(vocab_file, do_lower_case)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8f_fCe_hLC12"
      },
      "source": [
        "def is_whitespace(c):\n",
        "    '''\n",
        "    Indica si un cadena de caracteres se corresponde con un espacio en blanco / separador o no.\n",
        "    '''\n",
        "    if c == \" \" or c == \"\\t\" or c == \"\\r\" or c == \"\\n\" or ord(c) == 0x202F:\n",
        "        return True\n",
        "    return False\n",
        "# The ord() function returns an integer representing the Unicode character. \n",
        "# 0x202F: narrow no-break space in Unicode Hexadecimal UTF-16/UTF-16BE (hex)\n",
        "# \"\\r\" is a carriage return. Is a very unique feature of Python. \\r es uno de los caracteres de control de la codificaciÃ³n ASCII, Unicode, o\n",
        "# EBCDIC, que hace que se mueva el cursor a la primera posiciÃ³n de una lÃ­nea"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ZA4TYnxLGVT"
      },
      "source": [
        "# Lo que haremos aqui es enchufarle todo el texto y tendremos en doc_tokens los tokens del documento\n",
        "# y si previamente procedÃ­an de un espacio en blanco (prev_is_whitespace). Y para cada caracter del texto \n",
        "# iremos leyendo, leyendo si es espacio en blanco (is_whitespace(c)) nos dirÃ¡ acabo de encontrar un espacio \n",
        "# en blanco prev_is_whitespace = True. Si no es un espacio en blanco (else), entonces depende, si\n",
        "# previamente tenÃ­amos un espacio en blanco (if prev_is_whitespace) aÃ±adimos un token a los tokens \n",
        "# del documento y si no lo Ãºnico que hacemos es ir apendizando caracteres al documento para que \n",
        "# sea unicamente cuando leo un espacio en blanco que todos los document tokens han sido recopilados\n",
        "# que hemos localizado todas las palabras y por lo tanto podemos aÃ±adirlo al token. En pocas palabras,\n",
        "# esto lo que nos va a hacer es romper el texto en una lista de palabras pero gestionada por mi no \n",
        "# gestionada por el tokenizador.\n",
        "def whitespace_split(text):\n",
        "    '''\n",
        "    Toma el texto y devuelve una lista de \"palabras\" separadas segun los \n",
        "    espacios en blanco / separadores anteriores.\n",
        "    '''\n",
        "    doc_tokens = []\n",
        "    prev_is_whitespace = True\n",
        "    for c in text:\n",
        "        if is_whitespace(c):\n",
        "            prev_is_whitespace = True\n",
        "        else:\n",
        "            if prev_is_whitespace:\n",
        "                doc_tokens.append(c)\n",
        "            else:\n",
        "                doc_tokens[-1] += c\n",
        "            prev_is_whitespace = False\n",
        "    return doc_tokens"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "RcI0TnH9pQMv",
        "outputId": "1cb53f38-107c-4c0f-a505-3b7e06776a11"
      },
      "source": [
        "my_context"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Neoclassical economics views inequalities in the distribution of income as arising from differences in value added by labor, capital and land. Within labor income distribution is due to differences in value added by different classifications of workers. In this perspective, wages and profits are determined by the marginal value added of each economic actor (worker, capitalist/business owner, landlord). Thus, in a market economy, inequality is a reflection of the productivity gap between highly-paid professions and lower-paid professions.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k2T35MwbpUd3",
        "outputId": "f01d72cf-2eda-4250-9190-e2e497682aab"
      },
      "source": [
        "doc_tokens = []\r\n",
        "prev_is_whitespace = True\r\n",
        "for c in my_context[:50]:\r\n",
        "    if is_whitespace(c):\r\n",
        "        prev_is_whitespace = True\r\n",
        "        print(doc_tokens)\r\n",
        "    else:\r\n",
        "        if prev_is_whitespace:\r\n",
        "            print(\"Agregando:\", c)\r\n",
        "            doc_tokens.append(c)\r\n",
        "        else:\r\n",
        "            print(\"a\")\r\n",
        "            doc_tokens[-1] += c\r\n",
        "        prev_is_whitespace = False"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Neoclassical']\n",
            "['Neoclassical', 'economics']\n",
            "['Neoclassical', 'economics', 'views']\n",
            "['Neoclassical', 'economics', 'views', 'inequalities']\n",
            "['Neoclassical', 'economics', 'views', 'inequalities', 'in']\n",
            "['Neoclassical', 'economics', 'views', 'inequalities', 'in', 'the']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fsfzt3GUNWQK"
      },
      "source": [
        "def tokenize_context(text_words):\n",
        "    '''\n",
        "    Toma una lista de palabras (devueltas por whitespace_split()) y tokeniza cada\n",
        "    palabra una por una. TambiÃ©n almacena, para cada nuevo token, la palabra original\n",
        "    del parÃ¡metro text_words.\n",
        "    '''\n",
        "    text_tok = []\n",
        "    tok_to_word_id = []\n",
        "    for word_id, word in enumerate(text_words):\n",
        "        word_tok = tokenizer.tokenize(word)\n",
        "        text_tok += word_tok\n",
        "        tok_to_word_id += [word_id]*len(word_tok)   # leer *\n",
        "    return text_tok, tok_to_word_id # Devolvemos los tokens del texto y los identificadores de palabras que corresponden a cada uno de los tokens \n",
        "# *: aÃ±adimos el identificador de la palabra len(word_tok) veces de modo que si una sola palabra pasa a \n",
        "# ser 3 tokens porque tiene un prefijo en el cuerpo y un sufijo pues guardaremos 3 veces su identificador.\n",
        "# Este es el truco para tener una biyecciÃ³n en todo momento de palabra y token o tokens respectivos. Esta\n",
        "# es la razÃ³n del porque es necessario whitespace_split y tokenize_context"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HblnU8uZuUTQ",
        "outputId": "e4b9c9d9-fd61-4668-e0e3-1e213c0ffc11"
      },
      "source": [
        "# Ejemplo\r\n",
        "tokenize_context(['Neoclassical', 'economics', 'views', 'inequalities'])"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['neoclassical', 'economics', 'views', 'in', '##e', '##qual', '##ities'],\n",
              " [0, 1, 2, 3, 3, 3, 3])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-1cdrPrUsvdL"
      },
      "source": [
        "Necesitamos crear las 3 entradas diferentes para cada oraciÃ³n."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c8qreqEURUOP"
      },
      "source": [
        "# Devulve los identificadores para cada token\n",
        "def get_ids(tokens):\n",
        "    return tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "# Devulve todo lo que no es el caracter de padding \n",
        "def get_mask(tokens):\n",
        "    return np.char.not_equal(tokens, \"[PAD]\").astype(int)\n",
        "\n",
        "# Devulve 0 o 1 depeniendo de si esta antes o despuÃ©s el separador o incluso si hubiera mÃ¡s separadores\n",
        "# me irÃ­a combinando 0 o 1 para cada una de las palabras que hubiera de entre los separadores\n",
        "def get_segments(tokens):\n",
        "    seg_ids = []\n",
        "    current_seg_id = 0\n",
        "    for tok in tokens:\n",
        "        seg_ids.append(current_seg_id)\n",
        "        if tok == \"[SEP]\":\n",
        "            current_seg_id = 1-current_seg_id # Convierte 1 en 0 y viceversa\n",
        "    return seg_ids"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qOgZcgSttqG8"
      },
      "source": [
        "Creamos nuestro diccionario de entradas de modo que le damos una pregunta y un contexto ynos devolverÃ¡ un diccionario con los 3 elementos que le hacen falta al modelo. A saber, los tokens o los id's de tokens las mÃ¡scaras y el identificador de frase. TambiÃ©n devolverÃ¡ las palabras del contexto (context_words) ademÃ¡s id's de los tokens de contextos (context_tok_to_word_id) y la longitud de los tokens."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P2sPGXxsYUsY"
      },
      "source": [
        "def create_input_dict(question, context):\n",
        "    '''\n",
        "    Take a question and a context as strings and return a dictionary with the 3\n",
        "    elements needed for the model. Also return the context_words, the\n",
        "    context_tok to context_word ids correspondance and the length of\n",
        "    question_tok that we will need later.\n",
        "    '''\n",
        "    question_tok = tokenizer.tokenize(my_question)\n",
        "\n",
        "    context_words = whitespace_split(context)\n",
        "    context_tok, context_tok_to_word_id = tokenize_context(context_words)\n",
        "\n",
        "    input_tok = question_tok + [\"[SEP]\"] + context_tok + [\"[SEP]\"]\n",
        "    input_tok += [\"[PAD]\"]*(384-len(input_tok)) # in our case the model has been\n",
        "                                                # trained to have inputs of length max 384 so it would fill with '[PAD]'\n",
        "    # Generamos la entrada para BERT                                            \n",
        "    input_dict = {}\n",
        "    input_dict[\"input_word_ids\"] = tf.expand_dims(tf.cast(get_ids(input_tok), tf.int32), 0) # Devulve los identificadores para cada token\n",
        "    input_dict[\"input_mask\"] = tf.expand_dims(tf.cast(get_mask(input_tok), tf.int32), 0)    # Devulve todo lo que no es el caracter de padding \n",
        "    input_dict[\"input_type_ids\"] = tf.expand_dims(tf.cast(get_segments(input_tok), tf.int32), 0) \n",
        "    # Devulve 0 o 1 depeniendo de si esta antes o despuÃ©s el separador o incluso si hubiera mÃ¡s separadores\n",
        "    # me irÃ­a combinando 0 o 1 para cada una de las palabras que hubiera de entre los separadores\n",
        "\n",
        "    return input_dict, context_words, context_tok_to_word_id, len(question_tok)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QAnaCZWTZpWT"
      },
      "source": [
        "#### CreaciÃ³n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WQlM-M8rMklA"
      },
      "source": [
        "my_context = '''Neoclassical economics views inequalities in the distribution of income as arising from differences in value added by labor, capital and land. Within labor income distribution is due to differences in value added by different classifications of workers. In this perspective, wages and profits are determined by the marginal value added of each economic actor (worker, capitalist/business owner, landlord). Thus, in a market economy, inequality is a reflection of the productivity gap between highly-paid professions and lower-paid professions.'''"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YL_cE-o0U8mx"
      },
      "source": [
        "Neoclassical economics views inequalities in the distribution of income as arising from differences in value added by labor, capital and land. Within labor income distribution is due to differences in value added by different classifications of workers. In this perspective, wages and profits are determined by the marginal value added of each economic actor (worker, capitalist/business owner, landlord). Thus, in a market economy, inequality is a reflection of the productivity gap between highly-paid professions and lower-paid professions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SeB29SCNNQ1M"
      },
      "source": [
        "#my_question = '''What philosophy of thought addresses wealth inequality?'''\n",
        "my_question = '''What are examples of economic actors?'''\n",
        "#my_question = '''In a market economy, what is inequality a reflection of?'''"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-v5nLMNjZufe"
      },
      "source": [
        "my_input_dict, my_context_words, context_tok_to_word_id, question_tok_len = create_input_dict(my_question, my_context)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iu8DDxgNj-jR",
        "outputId": "41d24137-f8bc-48b6-f09e-0fce97cdc116"
      },
      "source": [
        "context_tok_to_word_id"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0,\n",
              " 1,\n",
              " 2,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 4,\n",
              " 5,\n",
              " 6,\n",
              " 7,\n",
              " 8,\n",
              " 9,\n",
              " 10,\n",
              " 11,\n",
              " 12,\n",
              " 13,\n",
              " 14,\n",
              " 15,\n",
              " 16,\n",
              " 17,\n",
              " 17,\n",
              " 18,\n",
              " 19,\n",
              " 20,\n",
              " 20,\n",
              " 21,\n",
              " 22,\n",
              " 23,\n",
              " 24,\n",
              " 25,\n",
              " 26,\n",
              " 27,\n",
              " 28,\n",
              " 29,\n",
              " 30,\n",
              " 31,\n",
              " 32,\n",
              " 33,\n",
              " 34,\n",
              " 35,\n",
              " 36,\n",
              " 36,\n",
              " 37,\n",
              " 38,\n",
              " 39,\n",
              " 39,\n",
              " 40,\n",
              " 41,\n",
              " 42,\n",
              " 43,\n",
              " 44,\n",
              " 45,\n",
              " 46,\n",
              " 47,\n",
              " 48,\n",
              " 49,\n",
              " 50,\n",
              " 51,\n",
              " 52,\n",
              " 53,\n",
              " 54,\n",
              " 54,\n",
              " 54,\n",
              " 55,\n",
              " 55,\n",
              " 55,\n",
              " 56,\n",
              " 56,\n",
              " 57,\n",
              " 57,\n",
              " 57,\n",
              " 58,\n",
              " 58,\n",
              " 59,\n",
              " 60,\n",
              " 61,\n",
              " 62,\n",
              " 62,\n",
              " 63,\n",
              " 64,\n",
              " 65,\n",
              " 66,\n",
              " 67,\n",
              " 68,\n",
              " 69,\n",
              " 70,\n",
              " 71,\n",
              " 72,\n",
              " 72,\n",
              " 72,\n",
              " 73,\n",
              " 74,\n",
              " 75,\n",
              " 75,\n",
              " 75,\n",
              " 76,\n",
              " 76]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dT066rMtZ65X"
      },
      "source": [
        "### PredicciÃ³n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CcJgDa9gVShl"
      },
      "source": [
        "start_logits, end_logits = bert_squad(my_input_dict, training=False)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rhdGlIo5Z9IZ"
      },
      "source": [
        "### InterpretaciÃ³n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZQJMBkVLd9wp"
      },
      "source": [
        "We remove the ids corresponding to the question and the `[\"SEP\"]` token:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TfwBJfsSTwRn"
      },
      "source": [
        "start_logits_context = start_logits.numpy()[0, question_tok_len+1:] \n",
        "end_logits_context = end_logits.numpy()[0, question_tok_len+1:]\n",
        "# 0: Corresponde al lote nÃºmero 0 ya que solo estamos pasando un texto y question_tok_len+1: elimino \n",
        "# los tokens de la pregunta + 1 que corresponde al separador quedando asÃ­ unicamente con el contexto"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "te1u6iZAawYf"
      },
      "source": [
        "First easy interpretation:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lQJ8tp-1WvI4"
      },
      "source": [
        "# Del contexto dÃ³nde empieza la respuesta, es simplemente buscar de los logits (start_logits_context) la\n",
        "# posiciÃ³n a la que correpsonde el argumento mÃ¡ximo, en quÃ© posiciÃ³n se encuentra la palbra o el token\n",
        "# con mayor score y bÃºscala en context_tok_to_word_id. Y este serÃ­a el identificador de la palabra de\n",
        "# inicio de la respuesta\n",
        "start_word_id = context_tok_to_word_id[np.argmax(start_logits_context)]\n",
        "end_word_id = context_tok_to_word_id[np.argmax(end_logits_context)]"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F9mi6EfM-nGc",
        "outputId": "ab43c26b-c5ef-4791-b6cc-161ef629da13"
      },
      "source": [
        "context_tok_to_word_id[np.argmax(start_logits_context)]"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "54"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8fe75tZm-s7E",
        "outputId": "83855388-f526-4f39-d5c4-e60d23a31460"
      },
      "source": [
        "context_tok_to_word_id[np.argmax(end_logits_context)]"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "57"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k3PHA84rarse"
      },
      "source": [
        "\"Advanced\" - making sure that the start of the answer is before the end:\r\n",
        "\r\n",
        "Para que no ocurre que nos diga que el token de inicio de la respuesta es por ejemplo el token 21 y el final de la respuesta es el 14. Esta es la versiÃ³n mejorada con un doble bucle y busca el par de respuestas cuyo argumento mÃ¡ximo cumpla que el token de inicio sea anterior al token final."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xiFZ2fUiRU_M"
      },
      "source": [
        "pair_scores = np.ones((len(start_logits_context), len(end_logits_context)))*(-1E10)\n",
        "for i in range(len(start_logits_context-1)):\n",
        "    for j in range(i, len(end_logits_context)):\n",
        "        pair_scores[i, j] = start_logits_context[i] + end_logits_context[j]\n",
        "pair_scores_argmax = np.argmax(pair_scores)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N9KEiFHPXXeM"
      },
      "source": [
        "start_word_id = context_tok_to_word_id[pair_scores_argmax // len(start_logits_context)] # corresponde a la fila\n",
        "end_word_id = context_tok_to_word_id[pair_scores_argmax % len(end_logits_context)]      # corresponde a la columna"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kJDBL8KKa6NP"
      },
      "source": [
        "Final answer:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j0Y3WDz0XwAw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6938a310-dbd0-46b5-8af8-22b4a8f026da"
      },
      "source": [
        "predicted_answer = ' '.join(my_context_words[start_word_id:end_word_id+1])\n",
        "print(\"The answer to:\\n\" + my_question + \"\\nis:\\n\" + predicted_answer)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The answer to:\n",
            "What are examples of economic actors?\n",
            "is:\n",
            "(worker, capitalist/business owner, landlord).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bYGSk_5OSYUk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        },
        "outputId": "36e26381-5abd-4c76-d37f-b4cdb253eca2"
      },
      "source": [
        "from IPython.core.display import HTML\n",
        "display(HTML(f'<h2>{my_question.upper()}</h2>'))\n",
        "marked_text = str(my_context.replace(predicted_answer, f\"<mark>{predicted_answer}</mark>\"))\n",
        "display(HTML(f\"\"\"<blockquote> {marked_text} </blockquote>\"\"\"))"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<h2>WHAT ARE EXAMPLES OF ECONOMIC ACTORS?</h2>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<blockquote> Neoclassical economics views inequalities in the distribution of income as arising from differences in value added by labor, capital and land. Within labor income distribution is due to differences in value added by different classifications of workers. In this perspective, wages and profits are determined by the marginal value added of each economic actor <mark>(worker, capitalist/business owner, landlord).</mark> Thus, in a market economy, inequality is a reflection of the productivity gap between highly-paid professions and lower-paid professions. </blockquote>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GAXMJwopGRzM"
      },
      "source": [
        "#### Reto Final"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Brg0AXkA0r74",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 196
        },
        "outputId": "27b814f9-65bf-4ae0-90d7-c246b6363ba6"
      },
      "source": [
        "my_context = '''\n",
        "Coronavirus disease 2019 is an infectious disease caused by severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2). It was first identified in December 2019 in Wuhan, Hubei, China, and has resulted in an ongoing pandemic.\n",
        "Common symptoms include fever, cough, fatigue, shortness of breath, and loss of smell and taste.While most people have mild symptoms, some people develop acute respiratory distress syndrome (ARDS) possibly precipitated by cytokine storm, multi-organ failure, septic shock, and blood clots. The time from exposure to onset of symptoms is typically around five days, but may range from two to fourteen days.\n",
        "The virus is spread primarily via nose and mouth secretions including small droplets produced by coughing,[a] sneezing, and talking. The droplets usually do not travel through air over long distances. However, those standing in close proximity may inhale these droplets and become infected.[b] People may also become infected by touching a contaminated surface and then touching their face. The transmission may also occur through smaller droplets that are able to stay suspended in the air for longer periods of time in enclosed spaces.'''\n",
        "\n",
        "my_question = '''What are the common symptoms of the disease?'''\n",
        "\n",
        "my_input_dict, my_context_words, context_tok_to_word_id, question_tok_len = create_input_dict(my_question, my_context)\n",
        "\n",
        "start_logits, end_logits = bert_squad(my_input_dict, training=False)\n",
        "\n",
        "pair_scores = np.ones((len(start_logits_context), len(end_logits_context)))*(-1E10)\n",
        "for i in range(len(start_logits_context-1)):\n",
        "    for j in range(i, len(end_logits_context)):\n",
        "        pair_scores[i, j] = start_logits_context[i] + end_logits_context[j]\n",
        "pair_scores_argmax = np.argmax(pair_scores)\n",
        "\n",
        "start_word_id = context_tok_to_word_id[pair_scores_argmax // len(start_logits_context)]\n",
        "end_word_id = context_tok_to_word_id[pair_scores_argmax % len(end_logits_context)]\n",
        "\n",
        "predicted_answer = ' '.join(my_context_words[start_word_id:end_word_id+1])\n",
        "\n",
        "\n",
        "from IPython.core.display import HTML\n",
        "display(HTML(f'<h2>{my_question.upper()}</h2>'))\n",
        "marked_text = str(my_context.replace(predicted_answer, f\"<mark>{predicted_answer}</mark>\"))\n",
        "display(HTML(f\"\"\"<blockquote> {marked_text} </blockquote>\"\"\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<h2>WHAT ARE THE COMMON SYMPTOMS OF THE DISEASE?</h2>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<blockquote> \n",
              "Coronavirus disease 2019 is an infectious disease caused by severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2). It was first identified in December 2019 in Wuhan, Hubei, China, and has resulted in an ongoing pandemic.\n",
              "Common symptoms include fever, cough, fatigue, <mark>shortness of breath, and loss of</mark> smell and taste.While most people have mild symptoms, some people develop acute respiratory distress syndrome (ARDS) possibly precipitated by cytokine storm, multi-organ failure, septic shock, and blood clots. The time from exposure to onset of symptoms is typically around five days, but may range from two to fourteen days.\n",
              "The virus is spread primarily via nose and mouth secretions including small droplets produced by coughing,[a] sneezing, and talking. The droplets usually do not travel through air over long distances. However, those standing in close proximity may inhale these droplets and become infected.[b] People may also become infected by touching a contaminated surface and then touching their face. The transmission may also occur through smaller droplets that are able to stay suspended in the air for longer periods of time in enclosed spaces. </blockquote>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_fTt1zl8Fbom"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}