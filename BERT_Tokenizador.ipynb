{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT - Tokenizador",
      "provenance": [],
      "collapsed_sections": [
        "B-4oGSu5jxUi",
        "VxONsFVHkFLU",
        "vSix1l4jkIxp"
      ],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/luguzman/NLP/blob/main/BERT_Tokenizador.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ca6rYUxNi_MO"
      },
      "source": [
        "# Fase 1: Importar las dependencias"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "76HfPILdC5lD"
      },
      "source": [
        "import numpy as np\n",
        "import math\n",
        "import re\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "import random\n",
        "\n",
        "from google.colab import drive"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y1h4YVFfDd1t"
      },
      "source": [
        "!pip install bert-for-tf2\n",
        "!pip install sentencepiece"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qMqTwu9jENrO"
      },
      "source": [
        "try:\n",
        "    %tensorflow_version 2.x\n",
        "except Exception:\n",
        "    pass\n",
        "import tensorflow as tf\n",
        "\n",
        "import tensorflow_hub as hub    # librería que nos ayuda a importar modelos con pesos ya almacenados\n",
        "\n",
        "from tensorflow.keras import layers\n",
        "import bert"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f0_xu0I3jFP9"
      },
      "source": [
        "# Fase 2: Pre procesado de datos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v60JTFKojIq5"
      },
      "source": [
        "## Carga de los ficheros"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mTtO7NkPjKUd"
      },
      "source": [
        "Importamos los ficheros desde nuestro Google Drive personal."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aRCxQui8Gqi_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "outputId": "e722e912-258b-45b5-a7ca-b35c19e597c9"
      },
      "source": [
        "drive.mount(\"/content/drive\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f6iT5nxDHLRz"
      },
      "source": [
        "cols = [\"sentiment\", \"id\", \"date\", \"query\", \"user\", \"text\"]\n",
        "data = pd.read_csv(\n",
        "    \"/content/drive/My Drive/Curso de NLP/BERT/sentiment_data/training.csv\",\n",
        "    header=None,\n",
        "    names=cols,\n",
        "    engine=\"python\",\n",
        "    encoding=\"latin1\"\n",
        ")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kKnCVewUIBkc"
      },
      "source": [
        "data.drop([\"id\", \"date\", \"query\", \"user\"],\n",
        "          axis=1,\n",
        "          inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xWWUo_XVeqoG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 260
        },
        "outputId": "eb9ffdbf-419c-420e-d435-f3cbce25651f"
      },
      "source": [
        "data.head(6)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentiment</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>is upset that he can't update his Facebook by ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>my whole body feels itchy and like its on fire</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0</td>\n",
              "      <td>@Kwesidei not the whole crew</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   sentiment                                               text\n",
              "0          0  @switchfoot http://twitpic.com/2y1zl - Awww, t...\n",
              "1          0  is upset that he can't update his Facebook by ...\n",
              "2          0  @Kenichan I dived many times for the ball. Man...\n",
              "3          0    my whole body feels itchy and like its on fire \n",
              "4          0  @nationwideclass no, it's not behaving at all....\n",
              "5          0                      @Kwesidei not the whole crew "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Quzx5tnjUtl"
      },
      "source": [
        "## Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M8hlexmRjXIS"
      },
      "source": [
        "### Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jBSUDL-UP-W_"
      },
      "source": [
        "def clean_tweet(tweet):\n",
        "    tweet = BeautifulSoup(tweet, \"lxml\").get_text()\n",
        "    # Eliminar el @\n",
        "    tweet = re.sub(r\"@[A-Za-z0-9]+\", ' ', tweet)\n",
        "    # Eliminar los links de la URL\n",
        "    tweet = re.sub(r\"https?://[A-Za-z0-9./]+\", ' ', tweet)\n",
        "    # Conservamos solamente las letras\n",
        "    tweet = re.sub(r\"[^a-zA-Z.!?']\", ' ', tweet)\n",
        "    # Eliminamos espacios en blanco adicionales\n",
        "    tweet = re.sub(r\" +\", ' ', tweet)\n",
        "    return tweet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8jiMaQsLWiTS"
      },
      "source": [
        "data_clean = [clean_tweet(tweet) for tweet in data.text]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EaqLE0fdWtni"
      },
      "source": [
        "data_labels = data.sentiment.values\n",
        "data_labels[data_labels == 4] = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6eh7sIquja5t"
      },
      "source": [
        "### Tokenization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K59PriX4jgBV"
      },
      "source": [
        "Necesitaremos crear una capa BERT para tener acceso a los meta datos para el tokenizador (como el tamaño del vocabulario)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0wry-st-HMN0"
      },
      "source": [
        "FullTokenizer = bert.bert_tokenization.FullTokenizer # Instacia que nos permitirá convertir los textos a token\n",
        "# Una vez teniendo este tokenizador necesitamos info adicional de este como cual es el tamaño del vocabulario,\n",
        "# convertir todo a minúsculas, etc y por lo tanto aplicaremos la siguiente capa:\n",
        "bert_layer = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\",   \n",
        "                            trainable=False)\n",
        "# en_uncased: english en minúscula\n",
        "# L-12: Es la versión sencilla de BERT, para que sea rápido de entrenar vs el L-24\n",
        "# trainable: Se utiliza para indicar si haremos find tuning, si hay que entrenar algo adicional de los propios pesos\n",
        "\n",
        "# Extraemos el tamaño del vocabulario(file temporal que genera el hub) a partir del tokenizer. \n",
        "# Es más que el tamaño es el diccionario completo.\n",
        "vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
        "# Del mismo modo necesitamos la info de la conversión a minúsculas por parte de BERT.\n",
        "do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
        "tokenizer = FullTokenizer(vocab_file, do_lower_case)\n",
        "# Esto es normal que tarde pues con esta versión baja la arquitectura junto con los 110 M de pesos"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aaf7yQZ1_xLx"
      },
      "source": [
        "# Ejemplo de que podemos hacer con nuestro tokenizer\r\n",
        "tokenizer.tokenize(\"My dog loves strawberries.\")\r\n",
        "tokenizer.convert_tokens_to_ids(tokenizer.tokenize(\"My dog loves strawberries.\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LggMv7k7Z3Ij"
      },
      "source": [
        "# Generamos una lista de tokens númericos, asociados a cada frase.\n",
        "def encode_sentence(sent):\n",
        "    return tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sent))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EGfTo5uIa2is"
      },
      "source": [
        "data_inputs = [encode_sentence(sentence) for sentence in data_clean]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B-4oGSu5jxUi"
      },
      "source": [
        "### Creación del data set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mLg0Z7QOj_YZ"
      },
      "source": [
        "Crearemos padded batches (por lo que rellenamos las frases para cada lote de forma independiente), de esta forma añadimos el mínimo número de tokens de padding posible. Para eso, ordenamos las frases por longitud, aplicamos padded_batches y luego las mezclamos."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HS_f6gWsLfLM"
      },
      "source": [
        "# Primero creamos una variable la cual agrupará 3 cosas a la vez. De manero que lo haremos es empaquetar \n",
        "# de cada frase, la frase misma, su etiqueta, y su longitud\n",
        "data_with_len = [[sent, data_labels[i], len(sent)]\n",
        "                 for i, sent in enumerate(data_inputs)]\n",
        "# Ahora necesitamos mezclarlas porque en el dataset original resulta que las frases han sido ordenadas \n",
        "# según la etiqueta que corresponde al sentimiento. Esto queire decir que la primera mitad (los primeros \n",
        "# 800 000 tweets) corresponden a frases con sentimiento negativo y la segunda mitad a positivos. Y lo \n",
        "# quiero mezclar ya que no quiero tener bloques donde tenga frases donde todo se negativo y/o positivo contantemente. \n",
        "random.shuffle(data_with_len)\n",
        "# Utilizamos la función sort para ordenar de acuerdo a la longitud de la frase\n",
        "data_with_len.sort(key=lambda x: x[2])\n",
        "# Vamos a limpiar la información. Solo nos quedaremos la frase y su correspondiente sentimiento pero \n",
        "# unicamente si la frase tiene más de 7 palabras.\n",
        "sorted_all = [(sent_lab[0], sent_lab[1])\n",
        "              for sent_lab in data_with_len if sent_lab[2] > 7]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ry0uJJg8lSQR"
      },
      "source": [
        "# En este punto nuestras aun no tiene todas la misma longitud por lo que utilizaremos un generator,\n",
        "# para que se encargue de arreglarlas. Un generedor como funciona es simplemente le das un elemnto \n",
        "# y te duevuelve otro.  \n",
        "all_dataset = tf.data.Dataset.from_generator(lambda: sorted_all,\n",
        "                                             output_types=(tf.int32, tf.int32)) # Tipo de dato de la salida (enteros en este caso)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cF74g5hpYzaZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "aff24bb7-d6d8-4493-bb83-3cb697f41c92"
      },
      "source": [
        "# iter: itera sobre los elementos de la lista. next: simplemente haría que nos devuelva la sig iteración.\r\n",
        "# En este caso como le pusimo que se quedará con las frases con más de 7 palabras es de esperar que la primer\r\n",
        "# que nos devolviera fuera de longitud 8\r\n",
        "next(iter(all_dataset))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<tf.Tensor: shape=(8,), dtype=int32, numpy=\n",
              " array([ 8038,  2100,   999,  4283,  2005,  1996, 21461,   999],\n",
              "       dtype=int32)>, <tf.Tensor: shape=(), dtype=int32, numpy=1>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZzHAhlfTlrcj"
      },
      "source": [
        "# Ahora haremos el proceso de Padding pero recordemos que será por bloques así reduciremos el entrenamiento\n",
        "BATCH_SIZE = 32\n",
        "all_batched = all_dataset.padded_batch(BATCH_SIZE, padded_shapes=((None, ), ()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s3ktdxCEm4Yh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 671
        },
        "outputId": "42c3ecc4-6e16-41d2-a99c-e9bbea94a985"
      },
      "source": [
        "#next(iter(all_batched))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<tf.Tensor: shape=(32, 8), dtype=int32, numpy=\n",
              " array([[ 8038,  2100,   999,  4283,  2005,  1996, 21461,   999],\n",
              "        [ 4283,  2005,  1996,  2128,  2102, 28394,  2102,   999],\n",
              "        [17012,  3566,  2056,  1045,  5807,  1005,  1056,  1012],\n",
              "        [ 5760,  6199, 17299,  2003,  4569,  1998,  2025,  3733],\n",
              "        [ 1057,  2074,  2561,  2100,  2081,  2026,  2154,   999],\n",
              "        [ 1045,  3246,  1996,  6876,  1045,  7303,  2131,  2209],\n",
              "        [ 2026,  2793,  1005,  1055,  2061,  4064, 22708,  1012],\n",
              "        [ 2009,  1005,  1055,  2033,   999,   999,   999, 14089],\n",
              "        [ 4931,  2045,  6898,   999,   999,  3407,  3407,  2420],\n",
              "        [13442,  2909,  1012,  2507,  2028,  1037,  3046,   999],\n",
              "        [14163,  3270,  3270,  3270,  3270,  2057,  2003,  3297],\n",
              "        [ 4083,  2000,  2293,  2166,  1005,  1055,  8220,   999],\n",
              "        [ 2003,  2061,  9479,  1012,  2009, 13403,  2061,  2172],\n",
              "        [13183,  2696,  2031,  4569,  1999, 14068,  4665,   999],\n",
              "        [12098, 13871,  5603,  2232, 10047,  6015,  1997,  8505],\n",
              "        [ 4957,  2987,  1005,  1056,  3711,  2000,  2147,  2153],\n",
              "        [ 8038,  2100,   999, 10047,  2006,  2026,  2126,  2188],\n",
              "        [ 2024,  2017,  2746,  2000,  2710, 15933,  2574,  1029],\n",
              "        [ 2873,  2080,  1051,  9541, 11631,  2053,  2204,  3374],\n",
              "        [ 2339,  2017,  2064,  1005,  1056,  2175,  1029,  1029],\n",
              "        [ 1045,  1005,  1049,  2061,  2439,  2302,  2026,  3042],\n",
              "        [ 3492,  4377,  1045,  2064,  2102,  3422,  2009,  2295],\n",
              "        [10930,  2057,  5987,  8808,  9850,  2043,  2057,  2709],\n",
              "        [ 4283,   999,  1045,  3246,  2009,  2097,  2022,  1012],\n",
              "        [ 1045,  2145,  2903,  2023,  2003,  2035,  1037,  4682],\n",
              "        [ 2183,  2000,  3637,  2156,  2017,  4364,  4826,  1012],\n",
              "        [ 2643,  1045,  5223,  7513,  1040,  9737,  2157,  2085],\n",
              "        [ 1045,  4299,  2008,  1045,  2071,  2175,  2648,  2651],\n",
              "        [15854, 11338, 14151,  3532, 16489,  2187,  2369,  2295],\n",
              "        [ 1045,  1005,  1049,  7501,   999,  5438,  2033,   999],\n",
              "        [ 2003, 27017,  2012,  2188,  1012,  1012,  1012,  4542],\n",
              "        [ 3861,  2039, 18570,  2025,  2183,  2205,  2080,  2092]],\n",
              "       dtype=int32)>, <tf.Tensor: shape=(32,), dtype=int32, numpy=\n",
              " array([1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1,\n",
              "        1, 1, 0, 1, 0, 0, 0, 1, 0, 0], dtype=int32)>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xrPqJeYpmfcv"
      },
      "source": [
        "# Generamos nustro conjunto de entrenamiento y testing por lotes.\n",
        "\n",
        "# Declaramos el número total de lotes que va a haber.\n",
        "NB_BATCHES = math.ceil(len(sorted_all) / BATCH_SIZE)\n",
        "# Declaramos el número total de lotes de test que va a haber.\n",
        "NB_BATCHES_TEST = NB_BATCHES // 10  # Nos quedamos con el 10%\n",
        "# SOlo recordemos que los lotes los tenemos ordenados por lo que si nos tomamos el 90% priemro para \n",
        "# entrenar y el 10% restante para testing me van a quedar el 10% más largo. Por lo que volveremos a \n",
        "# mezclar cada uno de los lotes.\n",
        "all_batched.shuffle(NB_BATCHES)\n",
        "test_dataset = all_batched.take(NB_BATCHES_TEST)\n",
        "train_dataset = all_batched.skip(NB_BATCHES_TEST)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VxONsFVHkFLU"
      },
      "source": [
        "# Fase 3: Construcción del modelo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L6DD3k3qPLDQ"
      },
      "source": [
        "class DCNN(tf.keras.Model):\n",
        "    # Declaramos el constructor\n",
        "    def __init__(self,\n",
        "                 vocab_size,    # Tamaño del vocabulario que vendrá dado por el tokenizador\n",
        "                 emb_dim=128,   # Primera capa de embbedingo con una dimension de 128\n",
        "                 nb_filters=50, # Número de filtros que le aplicaremos a RNC por cada vector de características\n",
        "                 FFN_units=512, # Número de neuronas en la capa oculta. En este caso la penultima capa densa\n",
        "                 nb_classes=2,  # Ya que es un proceso de clasificación ninario\n",
        "                 dropout_rate=0.1,  # Apaga el 10% de las neuronas de manera aleatoria en cada epoch para evitar el overfitting\n",
        "                 training=True,\n",
        "                 name=\"dcnn\"):\n",
        "        # Para poder utlizar todas las varibales de tk.keras.Model lo primero y más importante tenemos que \n",
        "        # llamar a la super clase, ya que como hemos hecho que el método herede de tf.keras.Model. estamos \n",
        "        # obligados a llamar a :\n",
        "        super(DCNN, self).__init__(name=name)   \n",
        "        \n",
        "        self.embedding = layers.Embedding(vocab_size,   # Recibe los números de los tokens y los convierte en vectores y estos vectores permiten que sean ajustado por la RN\n",
        "                                          emb_dim)\n",
        "        # Definimos nuestras redes de convolución. En este caso convoluiciones verticales (1D)\n",
        "        # Definimos 3 familias de filtros diferentes. En este caso, filtros que analizarán 2,3 y 4 palabras juntas\n",
        "        self.bigram = layers.Conv1D(filters=nb_filters,\n",
        "                                    kernel_size=2,      # analizamos las palabras cuando aparecen juntas de dos en dos, ie, filtrará palabras de 2 en 2.\n",
        "                                    padding=\"valid\",    # aqui no toma mucha importancia, lo que hará es añadir ceros en el caso de las primeras y ultimas convoluciones en las que ya no haya elementos. Pero como no hemos puesto el parametro de stride es = 1 por lo que no se podría salir\n",
        "                                    activation=\"relu\")  # trabaja muy bien en este tipo de análisis. La aplicamos para romper la linealidad y que busque más allá de valores lineales.\n",
        "        self.trigram = layers.Conv1D(filters=nb_filters,\n",
        "                                     kernel_size=3,\n",
        "                                     padding=\"valid\",\n",
        "                                     activation=\"relu\")\n",
        "        self.fourgram = layers.Conv1D(filters=nb_filters,\n",
        "                                      kernel_size=4,\n",
        "                                      padding=\"valid\",\n",
        "                                      activation=\"relu\")\n",
        "        # Creamos una capa la cual se encarga de solo quedarse con el máximo de todo estos valores. El \n",
        "        # máximo de cada bi-tri-cuatrigrama\n",
        "        self.pool = layers.GlobalMaxPool1D()\n",
        "        \n",
        "        # Pasamos estos maximos a la primera capa densa\n",
        "        self.dense_1 = layers.Dense(units=FFN_units, activation=\"relu\")\n",
        "        self.dropout = layers.Dropout(rate=dropout_rate)\n",
        "        if nb_classes == 2:\n",
        "            self.last_dense = layers.Dense(units=1,\n",
        "                                           activation=\"sigmoid\")\n",
        "        else:\n",
        "            self.last_dense = layers.Dense(units=nb_classes,\n",
        "                                           activation=\"softmax\")\n",
        "    \n",
        "    def call(self, inputs, training):\n",
        "        x = self.embedding(inputs)\n",
        "        x_1 = self.bigram(x) # (batch_size, nb_filters, seq_len-1) Aplicará la transformación de bigramas a x\n",
        "        x_1 = self.pool(x_1) # (batch_size, nb_filters) Ya que queremos quedarnos con el más grande, el más importante de esos valores de cada uno de los 50 de los mapas de características\n",
        "        x_2 = self.trigram(x) # (batch_size, nb_filters, seq_len-2)\n",
        "        x_2 = self.pool(x_2) # (batch_size, nb_filters)\n",
        "        x_3 = self.fourgram(x) # (batch_size, nb_filters, seq_len-3)\n",
        "        x_3 = self.pool(x_3) # (batch_size, nb_filters)\n",
        "        \n",
        "        # Concatenamos la lista formada por x1,x2 y x3 para formar nuestra macro entrada que pasará a nuestra red neuronal\n",
        "        merged = tf.concat([x_1, x_2, x_3], axis=-1) # (batch_size, 3 * nb_filters)\n",
        "        # -1: representa el último de los ejes de combinacion. En este caso tenemos 2 ejes.\n",
        "        # El primer eje es el de los bloques:\n",
        "        # Recordemos que le damos batches de palabras. Por lo tanto el primer eje sería cada uno de los\n",
        "        # objetos es un comentario.El segundo eje representa el propio valor que acabamos de sacar después\n",
        "        # de cada uno de los max pooling.\n",
        "        # Lo que se obtiene en la variable merge será un tensor cuyo tamaño será el número de filas, el\n",
        "        # mismo que el del tamaño del bloque (batchsize), y el número de columnas será 3 veces * el numero filtros\n",
        "        # 3 veces se debe a que habrá el número de filtros en bigramas, el número de filtros de trigramas, etc\n",
        "        \n",
        "        # Aplicamos la tranformación de la capa densa número 1 a toda esta info ya combinada\n",
        "        merged = self.dense_1(merged)\n",
        "        # Aplicamos la capa de Dropout unica y exclusivamente en fase de training \n",
        "        merged = self.dropout(merged, training)\n",
        "        # Por último aplicamos la capa de salida \n",
        "        output = self.last_dense(merged)\n",
        "        \n",
        "        return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vSix1l4jkIxp"
      },
      "source": [
        "# Fase 4: Entrenamiento"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lhfUFvWEPOIf"
      },
      "source": [
        "# Los siguientes datos se definen a prueba y error buscando mejorar en los resultados\n",
        "\n",
        "# Definimos el tamaño del vocabulario (lo limitamos)\n",
        "VOCAB_SIZE = len(tokenizer.vocab) #\n",
        "\n",
        "EMB_DIM = 200     # Todas y cada una de las palabras se mapeara a un espacio vectorial de dimensión 200 o lo que es lo mismo, cada palabra se identificará de forma única de con un punto de 200 coordenadas.\n",
        "NB_FILTERS = 100  # Número de filtros de la red neuronal convolucional\n",
        "FFN_UNITS = 256   # Número de unidades que la capa de Feed Forward tendrá en la capa oculta\n",
        "NB_CLASSES = 2    # len(set(train_labels))\n",
        "\n",
        "DROPOUT_RATE = 0.2  # Definimos la tasa de olvido\n",
        "\n",
        "NB_EPOCHS = 5     # Número de veces que pasarémos por todo el conjunto de entrenamiento"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VMtdiWmwv6rD"
      },
      "source": [
        "# Creamos la instancia del objeto DCNN\n",
        "Dcnn = DCNN(vocab_size=VOCAB_SIZE,\n",
        "            emb_dim=EMB_DIM,\n",
        "            nb_filters=NB_FILTERS,\n",
        "            FFN_units=FFN_UNITS,\n",
        "            nb_classes=NB_CLASSES,\n",
        "            dropout_rate=DROPOUT_RATE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6apbd7FrwPYo"
      },
      "source": [
        "# Compilamos el modleo de acuerdo al número de clases\n",
        "if NB_CLASSES == 2:\n",
        "    Dcnn.compile(loss=\"binary_crossentropy\",\n",
        "                 optimizer=\"adam\",\n",
        "                 metrics=[\"accuracy\"])\n",
        "else:\n",
        "    Dcnn.compile(loss=\"sparse_categorical_crossentropy\",\n",
        "                 optimizer=\"adam\",\n",
        "                 metrics=[\"sparse_categorical_accuracy\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "78cceSGCw1XC"
      },
      "source": [
        "# Crearemos un sistema de checkpoint en google colab que en caso de que se reinicie la sesión podemos reanudar\n",
        "# desde elúltimo checkpoint que se haya guardado o incluso más adelante , seguir entrenando con más textos \n",
        "# desde donde lo habíamos dejado en lugar de tener que crear la red neuronal desde el inicio.\n",
        "\n",
        "# Definimos una ruta dentro de mi Google Drive donde se guardarán los checkpoints\n",
        "checkpoint_path = \"./drive/My Drive/Curso NLP/BERT/ckpt_bert_tok/\"\n",
        "\n",
        "# Guardamos el modelo que queremos guradar autoamticamente siempre que sea posible\n",
        "ckpt = tf.train.Checkpoint(Dcnn=Dcnn)\n",
        "\n",
        "# Definimos el manager que será el encargado de guardar los checkpoints en la ruta establecida\n",
        "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=1)\n",
        "# max_to_keep: Es para definir que queremos que siempre se guarden los últimos 5 checkpoints.\n",
        "# En este caso si quisieramso guardar todos bastaría con poner max_to_keep = 5 pues son 5 epochs\n",
        "\n",
        "# Las siguientes líneas de código lo que hacen es preguntarle al Checkpoint Manager si hay o no hay \n",
        "# último checkpoint\n",
        "if ckpt_manager.latest_checkpoint:  # esta linea devuelve un None si no hay checkpoint previo (If None:)\n",
        "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
        "    print(\"Último checkpoint restaurado!!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2YIF5trzx7RA"
      },
      "source": [
        "# Función que entrega al método fit. Como se puede apreciar se le pueden entregar uno o más callbacks \n",
        "# y el objetivo es que se ejecuten algunas líneas de código entre cada epoch, al inicio de un epoch,\n",
        "# al final, etc. Hay una serie de metodo que podemos sobreescribir. En este caso on_epoch_end, cuando \n",
        "# termine un epoch lo que quiero es que se ejecute el guardado que hemos dicho en el checkpoint manager.\n",
        "\n",
        "# En otras palabras para que cada que fianlice un epoch haya un guardado de los pesos, de esos parámetros\n",
        "class MyCustomCallback(tf.keras.callbacks.Callback):\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        ckpt_manager.save()\n",
        "        print(\"Checkpoint guardado en {}.\".format(checkpoint_path))\n",
        "# logs=None: No queremos que se muestre ningún log en particular "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WrT8oWZzQNmW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 319
        },
        "outputId": "0697efe8-0cb5-465e-b369-e19b1c0b1fd5"
      },
      "source": [
        "Dcnn.fit(train_dataset,\n",
        "         epochs=NB_EPOCHS,\n",
        "         callbacks=[MyCustomCallback()])    # En este caso solo llamamos un callback, sin embargo, en esta lista podríamos llamar a 2,3, etc."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "  37196/Unknown - 1258s 34ms/step - loss: 0.4292 - accuracy: 0.8019Checkpoint guardado en ./drive/My Drive/Curso de NLP/BERT/ckpt_bert_tok/.\n",
            "37196/37196 [==============================] - 1258s 34ms/step - loss: 0.4292 - accuracy: 0.8019\n",
            "Epoch 2/5\n",
            "37195/37196 [============================>.] - ETA: 0s - loss: 0.3814 - accuracy: 0.8301Checkpoint guardado en ./drive/My Drive/Curso de NLP/BERT/ckpt_bert_tok/.\n",
            "37196/37196 [==============================] - 1223s 33ms/step - loss: 0.3814 - accuracy: 0.8301\n",
            "Epoch 3/5\n",
            "37195/37196 [============================>.] - ETA: 0s - loss: 0.3422 - accuracy: 0.8506Checkpoint guardado en ./drive/My Drive/Curso de NLP/BERT/ckpt_bert_tok/.\n",
            "37196/37196 [==============================] - 1211s 33ms/step - loss: 0.3422 - accuracy: 0.8506\n",
            "Epoch 4/5\n",
            "37195/37196 [============================>.] - ETA: 0s - loss: 0.3017 - accuracy: 0.8710Checkpoint guardado en ./drive/My Drive/Curso de NLP/BERT/ckpt_bert_tok/.\n",
            "37196/37196 [==============================] - 1224s 33ms/step - loss: 0.3017 - accuracy: 0.8710\n",
            "Epoch 5/5\n",
            "37195/37196 [============================>.] - ETA: 0s - loss: 0.2638 - accuracy: 0.8879Checkpoint guardado en ./drive/My Drive/Curso de NLP/BERT/ckpt_bert_tok/.\n",
            "37196/37196 [==============================] - 1221s 33ms/step - loss: 0.2638 - accuracy: 0.8879\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f883ce3d208>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3IiDW919kQQK"
      },
      "source": [
        "# Fase 5: Evaluación"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MthhNfnG1TPV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad592af4-87c9-4bfd-dcf4-9e58856bf99e"
      },
      "source": [
        "results = Dcnn.evaluate(test_dataset)\n",
        "print(results)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4132/4132 [==============================] - 25s 6ms/step - loss: 0.4434 - accuracy: 0.8242\n",
            "[0.4433819651603699, 0.8241847157478333]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vU0FPb-Nv0YO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "3d033039-217a-4c51-cba9-15c3df70787b"
      },
      "source": [
        "print(results)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.4433819651603699, 0.8241847157478333]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17dbJ-bS_Izy"
      },
      "source": [
        "\n",
        "\n",
        "*   Training: 88.5%\n",
        "*   Testing: 84.6%\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z-jrRvtl1xuk"
      },
      "source": [
        "def get_prediction(sentence):\n",
        "    tokens = encode_sentence(sentence)\n",
        "    # Recordemos que cuando entrenamos el modelo nostros le pasabamos bloques de frase y si me quedo \n",
        "    # unicamente con con lo de arriba no sería un bloque por lo añadimos una dimesnion adiconal\n",
        "    inputs = tf.expand_dims(tokens, 0)\n",
        "\n",
        "    output = Dcnn(inputs, training=False)\n",
        "\n",
        "    # El sentimiento lo obtendremos de redondear a la baja el output * 2. Esto ya que el número esta entre 0\n",
        "    # y 1 y este al multiplicarlo * 2 estará entre 0 y 2. Al redondear a la baja toda cosa entre 0 y .999\n",
        "    # será redondeado a 0 y todo valor entre 1 y 1.999 será 1 es imposible obtener 2 así que no hay problema\n",
        "    sentiment = math.floor(output*2)\n",
        "\n",
        "    if sentiment == 0:\n",
        "        print(\"Salida del modelo: {}\\nSentimiento predicho: Negativo.\".format(\n",
        "            output))\n",
        "    elif sentiment == 1:\n",
        "        print(\"Salida del modelo: {}\\nSentimiento predicho: Positivo.\".format(\n",
        "            output))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kk8V2bdvwfCv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "4e982819-db0c-4540-8a17-1279801ac81d"
      },
      "source": [
        "get_prediction(\"This movie was pretty interesting.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Salida del modelo: [[0.99801946]]\n",
            "Sentimiento predicho: Positivo.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NIcwVVB7wFUM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "a5f3bdcb-335b-4df1-8cbb-165b8a6c3af4"
      },
      "source": [
        "get_prediction(\"I'd rather not do that again.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Salida del modelo: [[0.0191755]]\n",
            "Sentimiento predicho: Negativo.\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}