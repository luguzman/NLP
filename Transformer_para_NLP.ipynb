{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Transformer para NLP.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/luguzman/NLP/blob/main/Transformer_para_NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m9JJ7FBw84tG"
      },
      "source": [
        "# Fase 1: Importar las dependencias"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t5DbIHC-F6Hf"
      },
      "source": [
        "**Paper original**: All you need is Attention https://arxiv.org/pdf/1706.03762.pdf"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZbcvtPlp3YWu"
      },
      "source": [
        "import numpy as np\n",
        "import math\n",
        "import re\n",
        "import time\n",
        "from google.colab import drive"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P6o_cpZz3y_-"
      },
      "source": [
        "# Le preguntamos a google colab por la version 2. más reciente\n",
        "try:\n",
        "    %tensorflow_version 2.x\n",
        "except:\n",
        "    pass\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow_datasets as tfds"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BQN8jwx48_yU"
      },
      "source": [
        "# Fase 2: Pre Procesado de Datos\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bPlOT-2mlw0r"
      },
      "source": [
        "## Carga de Ficheros"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dCD9jwXsLwS_"
      },
      "source": [
        "Importamos los ficheros de nuestro Google Drive personal"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eQpbl1pXCR0p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44661fb6-bfcf-4cc3-ec32-4b5f3bcd5d2c"
      },
      "source": [
        "drive.mount(\"/content/drive\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q8Or0sLV5b8t"
      },
      "source": [
        "with open(\"/content/drive/MyDrive/Curso NLP/Transformer/data/europarl-v7.es-en.en\", \n",
        "          mode = \"r\", encoding = \"utf-8\") as f:\n",
        "    europarl_en = f.read()\n",
        "with open(\"/content/drive/MyDrive/Curso NLP/Transformer/data/europarl-v7.es-en.es\", \n",
        "          mode = \"r\", encoding = \"utf-8\") as f:\n",
        "    europarl_es = f.read()\n",
        "with open(\"/content/drive/MyDrive/Curso NLP/Transformer/data/P85-Non-Breaking-Prefix.en\", \n",
        "          mode = \"r\", encoding = \"utf-8\") as f:\n",
        "    non_breaking_prefix_en = f.read()\n",
        "with open(\"/content/drive/MyDrive/Curso NLP/Transformer/data/nonbreaking_prefix.es\", \n",
        "          mode = \"r\", encoding = \"utf-8\") as f:\n",
        "    non_breaking_prefix_es = f.read()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TMAFFdpIyNZd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "d7bde2b2-2d51-4cf5-c8fd-aed6716110dd"
      },
      "source": [
        "europarl_en[:100]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Resumption of the session\\nI declare resumed the session of the European Parliament adjourned on Frid'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BYgCMq6myYIi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "c4aae085-905f-4643-ce85-89d37e92f7e7"
      },
      "source": [
        "europarl_es[:100]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Reanudación del período de sesiones\\nDeclaro reanudado el período de sesiones del Parlamento Europeo,'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TEFw0D2vP_Dl"
      },
      "source": [
        "## Limpieza de datos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PwIBeGXn7LIJ"
      },
      "source": [
        "Vamos a obtener los non_breaking_prefixes como una lista de palabras limpias con un punto al final para que nos sea más fácil de utilizar. Estos non_breaking_prefixes son palabras/letras que después de un punto no termina la oración como i.e (es decir) o inclusive no nos cuesta añadir las letras para un mejor performance. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qrxPJ066G70i",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "e8b1f21b-6e9b-42bb-9dae-ae0b0a492840"
      },
      "source": [
        "non_breaking_prefix_en"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'a\\nb\\nc\\nd\\ne\\nf\\ng\\nh\\ni\\nj\\nk\\nl\\nm\\nn\\no\\np\\nq\\nr\\ns\\nt\\nu\\nv\\nw\\nx\\ny\\nz\\nmessrs\\nmlle\\nmme\\nmr\\nmrs\\nms\\nph\\nprof\\nsr\\nst\\na.m\\np.m\\nvs\\ni.e\\ne.g'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L_TeuktU40Cb"
      },
      "source": [
        "non_breaking_prefix_en = non_breaking_prefix_en.split(\"\\n\")\n",
        "non_breaking_prefix_en = [' ' + pref + '.' for pref in non_breaking_prefix_en]\n",
        "non_breaking_prefix_es = non_breaking_prefix_es.split(\"\\n\")\n",
        "non_breaking_prefix_es = [' ' + pref + '.' for pref in non_breaking_prefix_es]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H9x4mZfKMaxD"
      },
      "source": [
        "Necesitaremos cada palabra y otro símbolo que queramos mantener en minúsculas y separados por espacios para que podamos \"tokenizarlos\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qg-8LLK-WdFp"
      },
      "source": [
        "corpus_en = europarl_en\n",
        "# Añadimos $$$ después de los puntos de frases sin fin\n",
        "for prefix in non_breaking_prefix_en:\n",
        "    corpus_en = corpus_en.replace(prefix, prefix + '$$$')\n",
        "# Sustituimos con expresiones regulares: cualquier punto que le sigue o un texto o un número o una letra \n",
        "# por \".$$$\", ie buscaremos los puntos que están en medio de palabras y que luego luego sin ningun espacio\n",
        "# contengan un número o una letra. Usamos \"\\.\" para especificar que busque un punto ya que en re el punto\n",
        "# significa \"cualquier cosa\". El \"?=\" indica que lo que sigue de la expresión regular no debe ser reemplazado,\n",
        "# por ejemplo 10 a.m sería 10 a.$$$m\n",
        "corpus_en = re.sub(r\"\\.(?=[0-9]|[a-z]|[A-Z])\", \".$$$\", corpus_en)\n",
        "# Eliminamos los marcadores \".$$$\"\n",
        "corpus_en = re.sub(r\"\\.\\$\\$\\$\", '', corpus_en)\n",
        "# Eliminamos espacios múltiples\n",
        "corpus_en = re.sub(r\"  +\", \" \", corpus_en)\n",
        "corpus_en = corpus_en.split('\\n')\n",
        "\n",
        "corpus_es = europarl_es\n",
        "for prefix in non_breaking_prefix_es:\n",
        "    corpus_es = corpus_es.replace(prefix, prefix + '$$$')\n",
        "corpus_es = re.sub(r\"\\.(?=[0-9]|[a-z]|[A-Z])\", \".$$$\", corpus_es)\n",
        "corpus_es = re.sub(r\"\\.\\$\\$\\$\", '', corpus_es)\n",
        "corpus_es = re.sub(r\"  +\", \" \", corpus_es)\n",
        "corpus_es = corpus_es.split('\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s-Y9v8-Tozl2"
      },
      "source": [
        "## Tokenizar el Texto"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p5YXanmOd_xK"
      },
      "source": [
        "# Codificamos/tokenizamos el corpus en inglés e indicamos un tamaño suficiente de palabras a considerar \n",
        "# del vocabulario\n",
        "tokenizer_en = tfds.features.text.SubwordTextEncoder.build_from_corpus(\n",
        "    corpus_en, target_vocab_size=2**13)\n",
        "# Codificamos/tokenizamos el corpus en español\n",
        "tokenizer_es = tfds.features.text.SubwordTextEncoder.build_from_corpus(\n",
        "    corpus_es, target_vocab_size=2**13)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ftIbPzIwCtwL"
      },
      "source": [
        "# Le preguntamos al tokenizer por el tamaño del vocabulario y ñadimos 2:\n",
        "# Un token que indicará inicio de frase y el otro el final de la frase\n",
        "VOCAB_SIZE_EN = tokenizer_en.vocab_size + 2 # = 8198\n",
        "VOCAB_SIZE_ES = tokenizer_es.vocab_size + 2 # = 8225"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oPFe2YJDC9jw"
      },
      "source": [
        "# Dado que el token que agregamos para inicio de la frase ocupara la pisición -2 del VOCAB_SIZE_EN y el de\n",
        "# fianl de la frase -1 del VOCAB_SIZE_EN lo que estamos haciendo para cada oración en corpus_en colocar\n",
        "# el token de inicio de frase + la frase codificada + el token de final de la frase \n",
        "inputs = [[VOCAB_SIZE_EN-2] + tokenizer_en.encode(sentence) + [VOCAB_SIZE_EN-1]\n",
        "          for sentence in corpus_en]\n",
        "outputs = [[VOCAB_SIZE_ES-2] + tokenizer_es.encode(sentence) + [VOCAB_SIZE_ES-1]\n",
        "           for sentence in corpus_es]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bG6AlcFMpC5C"
      },
      "source": [
        "## Eliminamos las frases demasiado largas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F6CD6PLGyQWy"
      },
      "source": [
        "# Dado que no tenemos una computadora tan potente o un servidor externo con más potencia a lo que nos\n",
        "# ofrece google colab removeremos todas las frases que tengan mas de 20 carácteres.\n",
        "MAX_LENGTH = 20\n",
        "# Lo que hacemos es recorrer todas las frases y si su longitud es > 20 se queda con el indice de la frase\n",
        "# para posteriormente eliminar las frases\n",
        "idx_to_remove = [count for count, sent in enumerate(inputs)\n",
        "                 if len(sent) > MAX_LENGTH]\n",
        "# Observemos que eliminamos los indices del ultimo al primero, ya que si borramos los primeros al eliminar una \n",
        "# frase las demás frases estarían cambiando de posición\n",
        "for idx in reversed(idx_to_remove):\n",
        "    del inputs[idx]   # Eliminamos las frases del corpus en ingles\n",
        "    del outputs[idx]  # Eliminamos las frases del corpus en español también ya que si no nos quemos con un corpus más grande\n",
        "\n",
        "# De la misma manero hacemos lo mismo para el corpues en español\n",
        "idx_to_remove = [count for count, sent in enumerate(outputs)\n",
        "                 if len(sent) > MAX_LENGTH]\n",
        "for idx in reversed(idx_to_remove):\n",
        "    del inputs[idx]\n",
        "    del outputs[idx]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SJcYcPUM1OuN"
      },
      "source": [
        "# Creamos un backup por si la sesión de google colab nos saca\r\n",
        "pd.DataFrame(inputs).to_csv(\"/content/drive/MyDrive/Curso NLP/Transformer/data/inputs.csv\", index = False)\r\n",
        "pd.DataFrame(outputs).to_csv(\"/content/drive/MyDrive/Curso NLP/Transformer/data/outputs.csv\", index = False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ypm8h5aZQTZ1"
      },
      "source": [
        "## Creamos las entradas y las salidas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9FP0WPsdM8hl"
      },
      "source": [
        "A medida que entrenamos con bloques, necesitaremos que cada entrada tenga la misma longitud. Rellenamos con el token apropiado, y nos aseguraremos de que este token de relleno no interfiera con nuestro entrenamiento más adelante."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nvDfLDWUONlE"
      },
      "source": [
        "# Hacemos padding en todas las entradas y salidas (frases) para que tengan la misma longitud. \n",
        "inputs = tf.keras.preprocessing.sequence.pad_sequences(inputs,\n",
        "                                                       value=0,\n",
        "                                                       padding='post',\n",
        "                                                       maxlen=MAX_LENGTH)\n",
        "outputs = tf.keras.preprocessing.sequence.pad_sequences(outputs,\n",
        "                                                        value=0,\n",
        "                                                        padding='post',\n",
        "                                                        maxlen=MAX_LENGTH)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wFxMp3TOIYff"
      },
      "source": [
        "# Creamos un dataset mezclando, haciendo un shuffle y todos los ultimos arreglos antes de que se creen\n",
        "# finalmente los conjuntos de entrenamiento.\n",
        "\n",
        "# Podemos jugar con los siguiente hiperparámetros.\n",
        "BATCH_SIZE = 64\n",
        "BUFFER_SIZE = 20000\n",
        "\n",
        "# Juntamos inputs y outputs en una variable\n",
        "dataset = tf.data.Dataset.from_tensor_slices((inputs, outputs))\n",
        "\n",
        "# Mejoramos el modo en que se almacenan los datos mientras se encuentra en la fase de entrenamiento. Esto no\n",
        "# el perfomance del modelo pero si que incrementará la velocidad de entrenamiento y de acceso a los datos\n",
        "dataset = dataset.cache()\n",
        "\n",
        "# Usamos shuffle para indicar que queremos mezclar el dataset con un buffer de tamaño 20 con bloques de 64 \n",
        "# en 64 (frases)\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
        "\n",
        "# Aceleramos el proceso de los datos. Este prefetch de nuevo lo único que hace es que sirve para acceder a los\n",
        "# datos más rápidamente. El parámetro AUTOTUNE es para ajustar la caché venga ya con los primeros\n",
        "# bloques preparados para que el acceso a los datos cada vez no tenga que esperar al siguiente bloque, sino que ya\n",
        "# tenga el siguiente bloque en memoria y por tanto aceleremos la fase de entrenamiento.\n",
        "dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ycT0YqydRcUd"
      },
      "source": [
        "# Fase 3: Construcción del Modelo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-SBoH8G4XyR9"
      },
      "source": [
        "## Embedding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7G9C3ucmJ86I"
      },
      "source": [
        "Fórmula de la Codificación Posicional:\n",
        "\n",
        "$PE_{(pos,2i)} =\\sin(pos/10000^{2i/dmodel})$\n",
        "\n",
        "$PE_{(pos,2i+1)} =\\cos(pos/10000^{2i/dmodel})$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e2wc6sYlX0dr"
      },
      "source": [
        "class PositionalEncoding(layers.Layer):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "    # Creamos el metodo get_angle el cual se encargara de calcular el angulo del seno y coseno, ie, los valores de las funciones expresados arriba.\n",
        "    # pos (es un tensor con posiciones de 0-19 debido a MAX_LENGTH = 19): posición que ocupa la palabra dentro de la frase\n",
        "    # i (vector o array de todas las dimensiones posibles del espacio de emdedding): la dimensión en la que estamos embebiendo\n",
        "    # d_model: la dimension total del modleo del espacio de embeddings\n",
        "    def get_angles(self, pos, i, d_model): # pos: (seq_length, 1) i: (1, d_model)\n",
        "        angles = 1 / np.power(10000., (2*(i//2)) / np.float32(d_model))\n",
        "        return pos * angles # (seq_length, d_model) ya que estamos haciendo una multiplicaciones matricial \n",
        "  \n",
        "  # NOTA : El subíndice utilizado en las formulas es la notación matematica expresar un número par o impar que no es lo mismo a lo que estamos \n",
        "  # expresando en la potencia del denomidor de ambas fórmulas. Además usamos decimales ya que no queremos perder información valiosa\n",
        "  # NOTA: (i//2) lo usamos para quedarnos con la parte entera asi si i = 10 o i = 11 su parte entera será 5, así podemos calcular el ángulo de la\n",
        "  # posición par \"x\" y su consecuente.\n",
        "\n",
        "    # Creamos el método que se va a utilizar cada vez que creemos cada una de estas capas para añadirla a la arquitectura global.\n",
        "    # inputs: Número de entradas\n",
        "    def call(self, inputs):\n",
        "        seq_length = inputs.shape.as_list()[-2]   # Longitud de la secuencia, dado que quiero que sean números los pasamos a formato lista y nos quedamos con la penúltima entrada ay que es la que nos dice cual es la dimensión, ie, cuantas palabras.\n",
        "        d_model = inputs.shape.as_list()[-1]      # Para acceder a las dimensiones del modelos\n",
        "        angles = self.get_angles(np.arange(seq_length)[:, np.newaxis],    # np.newaxis: genera una dimensión vacia adicional. Pasamos de tener una lista a una matriz\n",
        "                                 np.arange(d_model)[np.newaxis, :],       # Pasamos a tener un vector en fila\n",
        "                                 d_model)\n",
        "        \n",
        "        # Corregimos las dimensiones columnas pares \n",
        "        angles[:, 0::2] = np.sin(angles[:, 0::2]) # Indicamos que la secuencia empieza en cero y \":2\" es el llamdo stride que significa el salto es de 2 en 2. Por lo que accederíamos a las posiciones pares \n",
        "        # Corregimos las dimension columnas impares\n",
        "        angles[:, 1::2] = np.cos(angles[:, 1::2])\n",
        "        # Tomamos el angles que ya habíamos generado anteriormente pero le agregaremos una dimención vacia adicional por delante ya que es la\n",
        "        # que corresponde con los batches, con los bloques de entrenamiento. La 2° y 3° dimension corresponderá a los bloques (angles) generados\n",
        "        pos_encoding = angles[np.newaxis, ...]\n",
        "        return inputs + tf.cast(pos_encoding, tf.float32) # Devolvemos los inputs + el posicional encoding que acabos de generar"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lcw8YIQqRhOJ"
      },
      "source": [
        "## Attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3sffhwwvX-wj"
      },
      "source": [
        "### Cálculo de la Atención"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7VBuW6lESLDX"
      },
      "source": [
        "$Attention(Q, K, V ) = \\text{softmax}\\left(\\dfrac{QK^T}{\\sqrt{d_k}}\\right)V $"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2rEoCNJURbrT"
      },
      "source": [
        "# queries: \"Q\"\n",
        "# keys: \"K\"\n",
        "# values: \"V\"\n",
        "# mask: parámetro que podrá ser la máscara de lookahead que no permitirá que el descodificador vea palabras más allá del padding establecido o\n",
        "# podrá ser absolutamente nada si queremos que acceda a toda la frase.\n",
        "def scaled_dot_product_attention(queries, keys, values, mask):\n",
        "    # Realizamos el producto matricial del númerador\n",
        "    product = tf.matmul(queries, keys, transpose_b=True)\n",
        "    \n",
        "    # Declaramos la dimensión de las claves\n",
        "    # tf.shape(keys)[-1]: es para quedarnos con el número de la última dimensión \n",
        "    keys_dim = tf.cast(tf.shape(keys)[-1], tf.float32)  #tf.float32: ya que queremos quedarnos con todos los decimales\n",
        "    scaled_product = product / tf.math.sqrt(keys_dim)   # Dividimos entre todas y cada una de las entradas\n",
        "    \n",
        "    # De forma ocpional se puede aplicar una mascara (lookahead). Lo cual es anular o no cada una de las entradas \n",
        "    # Aplicaremos un pequeño truco y esto será multiplicar los valores donde esta la máscara (los unos basicamente), porque la mascara tiene\n",
        "    # ceros y unos. Vamos a multiplicar los unos por -infinito, ie, un número mu pequeño ya que al resultado de esto posteriormente le aplicaré\n",
        "    # la función softmax, función en la cual -infinito = 0 va creciendo de modo que en 0 vale 0.5 y sigue creciendo hasta +infinito = 1\n",
        "    # De modo que no tengan impacto más adelante como parte del algoritmo. En python no se puede poner -inifinito y por eso usamos -1e9\n",
        "    if mask is not None:\n",
        "        scaled_product += (mask * -1e9)\n",
        "    \n",
        "    # Una vez aplicada la máscara, calculamos la atención como el producto de softmax aplicado a lo anterior por la matriz V\n",
        "    attention = tf.matmul(tf.nn.softmax(scaled_product, axis=-1), values) # axis=-1: para que sea a cada una de la filas de  ultima dimensión tomada (observaciones de la Q que es la que tiene que sumar 1)\n",
        "    \n",
        "    #Nota: con esto tenemos nuestro producto escalar escalado\n",
        "    return attention"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-MjtvXrfYEx7"
      },
      "source": [
        "### Sub capa de atención de encabezado múltiple"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lvq4I9uTX5p7"
      },
      "source": [
        "class MultiHeadAttention(layers.Layer):\n",
        "    \n",
        "    # nb_proj: número de espacios en los que deseo proyectar\n",
        "    def __init__(self, nb_proj):\n",
        "        super(MultiHeadAttention, self).__init__()  # Inicializamos el objeto self de la clase MultiHeadAttention\n",
        "        self.nb_proj = nb_proj\n",
        "    \n",
        "    # Todo lo que vaya a depender de los datos y vaya utilizar es mejor declararlo en build en lugar del init\n",
        "    def build(self, input_shape):\n",
        "        # Definimos la dimension\n",
        "        self.d_model = input_shape[-1]  # La dimensión del embedding se encuentra en la última posición\n",
        "        # realizamo una aserción en caso de no cumplirse dará error y mostrará en consola lo que ocurrió\n",
        "        assert self.d_model % self.nb_proj == 0 \n",
        "        \n",
        "        # Número de projecciones en cada dimensión \n",
        "        self.d_proj = self.d_model // self.nb_proj\n",
        "        \n",
        "        # Definimos 3 capas densas. Por tanto tenemos una neurona para cada una de las dimensiones del espacio vectorial en cuestion.\n",
        "        self.query_lin = layers.Dense(units=self.d_model)\n",
        "        self.key_lin = layers.Dense(units=self.d_model)\n",
        "        self.value_lin = layers.Dense(units=self.d_model)\n",
        "        \n",
        "        self.final_lin = layers.Dense(units=self.d_model)\n",
        "        \n",
        "    def split_proj(self, inputs, batch_size): # inputs: (batch_size, seq_length, d_model)\n",
        "        # Definimos la dimensión de la tramnsformación \n",
        "        shape = (batch_size,    # mantenemos el propio tamaño del bloque\n",
        "                 -1,            # basicamente será la longitud de la propio secuencia, que no se verá alterada para cada una de ellas\n",
        "                 self.nb_proj,  # para cada una de ellas decido el número de proyecciones\n",
        "                 self.d_proj)   # para cada proyección recibo la info en el espacio de la dimensión de proyecciones que hayamos calculado dinamicamente\n",
        "        # redimensionas la entrada\n",
        "        splited_inputs = tf.reshape(inputs, shape=shape) # (batch_size, seq_length, nb_proj, d_proj)\n",
        "        return tf.transpose(splited_inputs, perm=[0, 2, 1, 3]) # (batch_size, nb_proj, seq_length, d_proj)\n",
        "      # Ejemplo: si d_model = 15 y nb_proj = 3, pues basicamente me quedarían 3 submatrices con cada palabra proyectado en un espacio de dimensión 5\n",
        "      # Return: lo que obtenemos es para cada bloque una de sus proyecciones [0], para cada proyección las palabras de la secuencia, para cada palabra de la \n",
        "      # secuencia el espacio lineal proyectado\n",
        "    \n",
        "    # 1.\n",
        "    def call(self, queries, keys, values, mask):\n",
        "      # El tamaño del batch siempre lo va a definir la Q, que es la que siempre voy a querer ir prediciendo. \n",
        "        batch_size = tf.shape(queries)[0] # primera dimensión de query indica el bloque\n",
        "        \n",
        "        # Queremos aplicar 3 transformaciones lineales; las Q, las K y las V\n",
        "        # Declaramos 3 capas densas \n",
        "        queries = self.query_lin(queries)\n",
        "        keys = self.key_lin(keys)\n",
        "        values = self.value_lin(values)\n",
        "        \n",
        "        # Dividmos los queries, keys y values en cada uno de los espacios especificados en el metodo split_proj y vendrán en metodo adecuado\n",
        "        # para palicarle la atención de producto escalar escalado\n",
        "        queries = self.split_proj(queries, batch_size)\n",
        "        keys = self.split_proj(keys, batch_size)\n",
        "        values = self.split_proj(values, batch_size)\n",
        "        \n",
        "        # Calculamos la atención\n",
        "        attention = scaled_dot_product_attention(queries, keys, values, mask)\n",
        "    \n",
        "        # recuperamos para cada bloque la sequencia de tantas proyecciones de dicho tamaño para que sean las dos últimas dimensiones las que\n",
        "        # se junten para volver a juntarlo.\n",
        "        attention = tf.transpose(attention, perm=[0, 2, 1, 3])\n",
        "        \n",
        "        # Concatemos todo lo que habíamos separado\n",
        "        concat_attention = tf.reshape(attention,\n",
        "                                      shape=(batch_size, -1, self.d_model))\n",
        "        # -1: mantemenos la longitud de la segunda dimensión \n",
        "        # self.d_model: que la tercera y última dimensión tiene que ser de tamaño d_model\n",
        "        \n",
        "        # Una última función lineal, una última capa densa será la que se encargará de juntar toda la información y proceder\n",
        "        outputs = self.final_lin(concat_attention)\n",
        "        \n",
        "        return outputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yiyuHe1OeT5N"
      },
      "source": [
        "## Codificación"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UV0ZMH7KT_KZ"
      },
      "source": [
        "# Objetivo crear una estructura de n capas\n",
        "class EncoderLayer(layers.Layer):\n",
        "    \n",
        "    #Definimos el constructor\n",
        "    # FFN_units: Número de unidades en la capa oculta\n",
        "    #n_porj: vamos a pryectar a un espacio n dimensional al hacer el embedding.\n",
        "    # dropout_rate: % de neuronas que no se van activar durante la fase de entrenamiento\n",
        "    def __init__(self, FFN_units, nb_proj, dropout_rate):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.FFN_units = FFN_units\n",
        "        self.nb_proj = nb_proj\n",
        "        self.dropout_rate = dropout_rate\n",
        "    \n",
        "    # El siguiente metodo es llamado cada vez que manda a construir una capa, solo pasandole el tamaño de la entrada\n",
        "    def build(self, input_shape):\n",
        "        self.d_model = input_shape[-1] # tamaño de la dimensión\n",
        "        \n",
        "        self.multi_head_attention = MultiHeadAttention(self.nb_proj) # particiones de la dimensión\n",
        "        self.dropout_1 = layers.Dropout(rate=self.dropout_rate)\n",
        "        self.norm_1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        \n",
        "        self.dense_1 = layers.Dense(units=self.FFN_units, activation=\"relu\")\n",
        "        self.dense_2 = layers.Dense(units=self.d_model)\n",
        "        self.dropout_2 = layers.Dropout(rate=self.dropout_rate)\n",
        "        self.norm_2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "    \n",
        "    # inputs: tamaño de la capa inmediata anterior\n",
        "    # training: para denotar si estamos entrenando, para de ser así activar el dropout_out\n",
        "    def call(self, inputs, mask, training):\n",
        "        attention = self.multi_head_attention(inputs, # q\n",
        "                                              inputs, # k\n",
        "                                              inputs, # v \n",
        "                                              mask)\n",
        "        attention = self.dropout_1(attention, training=training)\n",
        "        attention = self.norm_1(attention + inputs)   # aplicamos la normalización a la suma de attention + las inputs\n",
        "        \n",
        "        # 2° fase: aplicamos 2 capas densas\n",
        "        outputs = self.dense_1(attention)\n",
        "        outputs = self.dense_2(outputs) # aplicamos esta capa ya que el resultado lo que queremos es una dimensión igual a d_model\n",
        "        outputs = self.dropout_2(outputs, training=training)\n",
        "        outputs = self.norm_2(outputs + attention)\n",
        "        \n",
        "        return outputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P-P92KeZih60"
      },
      "source": [
        "# \n",
        "class Encoder(layers.Layer):\n",
        "    \n",
        "    def __init__(self,\n",
        "                 nb_layers, # Número de veces que queremos repetir la operación del EncoderLayer\n",
        "                 FFN_units,\n",
        "                 nb_proj,\n",
        "                 dropout_rate,\n",
        "                 vocab_size,  # Nos lo dará el propio tokenizador\n",
        "                 d_model,\n",
        "                 name=\"encoder\"):\n",
        "        super(Encoder, self).__init__(name=name)\n",
        "        self.nb_layers = nb_layers\n",
        "        self.d_model = d_model\n",
        "        \n",
        "        self.embedding = layers.Embedding(vocab_size, d_model)\n",
        "        self.pos_encoding = PositionalEncoding()\n",
        "        self.dropout = layers.Dropout(rate=dropout_rate)\n",
        "        self.enc_layers = [EncoderLayer(FFN_units,\n",
        "                                        nb_proj,\n",
        "                                        dropout_rate) \n",
        "                           for _ in range(nb_layers)]\n",
        "    \n",
        "    def call(self, inputs, mask, training):\n",
        "        # Fase de codificación\n",
        "        outputs = self.embedding(inputs) # la capa de embedding me proyecta al espacio n dimensional\n",
        "        # En las capas de embedding multiplicamos los pesos por raíz de d_model. Se hace para el modelo sea más robusto, ya que al multplicar los\n",
        "        # pesos por la raíz de d_model crecen un poco más y evitan problemas cercanos a cero para evitarnos tener problemas más adelante de convergencia\n",
        "        outputs *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "        # Realizamos el encoding posicional\n",
        "        outputs = self.pos_encoding(outputs)\n",
        "        outputs = self.dropout(outputs, training)\n",
        "        \n",
        "        # Aplicamos n veces la capa encoder. Sobrescribimos nb_layers veces las salidas \n",
        "        for i in range(self.nb_layers):\n",
        "            outputs = self.enc_layers[i](outputs, mask, training)\n",
        "\n",
        "        return outputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7DthraBEwuvl"
      },
      "source": [
        "## Descodificación"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ZWZyFBnwy8u"
      },
      "source": [
        "class DecoderLayer(layers.Layer):\n",
        "    \n",
        "    def __init__(self, FFN_units, nb_proj, dropout_rate):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "        self.FFN_units = FFN_units # No. de neuronas de la capa oculta\n",
        "        self.nb_proj = nb_proj     # No. de proyecciones\n",
        "        self.dropout_rate = dropout_rate  \n",
        "    \n",
        "    def build(self, input_shape):\n",
        "        self.d_model = input_shape[-1]\n",
        "        \n",
        "        # Self multi head attention\n",
        "        self.multi_head_attention_1 = MultiHeadAttention(self.nb_proj)\n",
        "        self.dropout_1 = layers.Dropout(rate=self.dropout_rate)\n",
        "        self.norm_1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        \n",
        "        # Multi head attention combinado con la salida del encoder \n",
        "        self.multi_head_attention_2 = MultiHeadAttention(self.nb_proj)\n",
        "        self.dropout_2 = layers.Dropout(rate=self.dropout_rate)\n",
        "        self.norm_2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        \n",
        "        # Feed foward\n",
        "        self.dense_1 = layers.Dense(units=self.FFN_units,\n",
        "                                    activation=\"relu\")    # capa totalmente conectada con función relu para presindir de todos los valores negativos\n",
        "        self.dense_2 = layers.Dense(units=self.d_model)\n",
        "        self.dropout_3 = layers.Dropout(rate=self.dropout_rate)\n",
        "        self.norm_3 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        \n",
        "    def call(self, inputs, enc_outputs, mask_1, mask_2, training):\n",
        "        # Aplicamos la 1° multihead attention layer de dos que aplicaremos\n",
        "        attention = self.multi_head_attention_1(inputs, \n",
        "                                                inputs,\n",
        "                                                inputs,\n",
        "                                                mask_1)\n",
        "        # Despues de algo chonho aplicamos un dropout\n",
        "        attention = self.dropout_1(attention, training)\n",
        "        # Normalizamos de acuerdo al paper\n",
        "        attention = self.norm_1(attention + inputs)\n",
        "        \n",
        "        # Aplicamos la 2° multihead attention layer de dos que aplicaremos\n",
        "        attention_2 = self.multi_head_attention_2(attention,\n",
        "                                                  enc_outputs,\n",
        "                                                  enc_outputs,\n",
        "                                                  mask_2)\n",
        "        attention_2 = self.dropout_2(attention_2, training)\n",
        "        attention_2 = self.norm_2(attention_2 + attention)\n",
        "        \n",
        "        outputs = self.dense_1(attention_2)     # Aplicamos una primera capa densa al attention_2\n",
        "        outputs = self.dense_2(outputs)         # una segunda a lo generado\n",
        "        outputs = self.dropout_3(outputs, training)\n",
        "        outputs = self.norm_3(outputs + attention_2)\n",
        "        \n",
        "        return outputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kpzdiWHiwywF"
      },
      "source": [
        "class Decoder(layers.Layer):\n",
        "    \n",
        "    def __init__(self,\n",
        "                 nb_layers,     # Número de veces que queremos repetir la operación del DecoderLayer\n",
        "                 FFN_units,\n",
        "                 nb_proj,       # No de proyecciones \n",
        "                 dropout_rate,  \n",
        "                 vocab_size,    \n",
        "                 d_model,       # Dimensión del espacio vectorial que coincide con el no de columnas\n",
        "                 name=\"decoder\"):\n",
        "        super(Decoder, self).__init__(name=name)\n",
        "        self.d_model = d_model\n",
        "        self.nb_layers = nb_layers\n",
        "        \n",
        "        self.embedding = layers.Embedding(vocab_size, d_model)\n",
        "        self.pos_encoding = PositionalEncoding()\n",
        "        self.dropout = layers.Dropout(rate=dropout_rate)\n",
        "        \n",
        "        self.dec_layers = [DecoderLayer(FFN_units,\n",
        "                                        nb_proj,\n",
        "                                        dropout_rate) \n",
        "                           for _ in range(nb_layers)]\n",
        "    \n",
        "    def call(self, inputs, enc_outputs, mask_1, mask_2, training):\n",
        "        outputs = self.embedding(inputs)    # Utilizamos un capa de embedding para incrustar la entrada de vectores tokenizados a un espacio d-dimensional\n",
        "        outputs *= tf.math.sqrt(tf.cast(self.d_model, tf.float32)) # Tomamos el resultados y multiplicamos entrada a entrada por raiz cuadrada del resultado de castear d_model a un tensor float32 por recomendación del paper\n",
        "        outputs = self.pos_encoding(outputs)  # Usamos el propio posicional encoder a la salida para poder tener relaciones únicas entre las coordenadas y las palabras\n",
        "        outputs = self.dropout(outputs, training) # Con cierta proba no se activan ciertas neuronas\n",
        "        \n",
        "        # Sobrescribimos los outputs; creamos multiples descodificadores\n",
        "        for i in range(self.nb_layers):\n",
        "            outputs = self.dec_layers[i](outputs,\n",
        "                                         enc_outputs,   # salida del decodificador\n",
        "                                         mask_1,  \n",
        "                                         mask_2,\n",
        "                                         training)\n",
        "\n",
        "        return outputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x5sJYkjbz5DD"
      },
      "source": [
        "## Transformer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GqvqNjJPwyh-"
      },
      "source": [
        "# Nota: Transformer no hereda de layer dado que este ya es un modelo y heredad de tf.keras.Model\n",
        "class Transformer(tf.keras.Model):\n",
        "    \n",
        "    def __init__(self,\n",
        "                 vocab_size_enc,    # Dimensiones del vocabulario del encoder\n",
        "                 vocab_size_dec,    # Dimensiones del vocabulario del decoder (ya que dos vocabularios)\n",
        "                 d_model,           # No de columnas (dimensión del espacio vectorial de embedding)\n",
        "                 nb_layers,         # No. de capas. Cuantas veces utilizaremos las capas de encoder y decoder\n",
        "                 FFN_units,         # No. de neuronas de la capa feed forward al final del encoder y del decoder\n",
        "                 nb_proj,           # No. de proyecciones que deseamos que el multihead attention divida nuestro espacio vectorial\n",
        "                 dropout_rate,      # El ratio de neuronas que queramos que no se activen durante la fase de entrenamiento.\n",
        "                 name=\"transformer\"):\n",
        "        super(Transformer, self).__init__(name=name)\n",
        "        \n",
        "        self.encoder = Encoder(nb_layers,\n",
        "                               FFN_units,\n",
        "                               nb_proj,\n",
        "                               dropout_rate,\n",
        "                               vocab_size_enc,\n",
        "                               d_model)\n",
        "        self.decoder = Decoder(nb_layers,\n",
        "                               FFN_units,\n",
        "                               nb_proj,\n",
        "                               dropout_rate,\n",
        "                               vocab_size_dec,\n",
        "                               d_model)\n",
        "        self.last_linear = layers.Dense(units=vocab_size_dec, name=\"lin_ouput\")\n",
        "    \n",
        "    # Creamos la mascara para el padding para los tokens a la hr de hacerle el padding\n",
        "    def create_padding_mask(self, seq): #seq: (batch_size, seq_length) \n",
        "        mask = tf.cast(tf.math.equal(seq, 0), tf.float32)   # Tomamos la secuencia dodne la función tf.math.equal(seq,0) me pondrá un número 1 donde la secuencia tuviera un 0 y un 0 donde no hubiera un 0\n",
        "        return mask[:, tf.newaxis, tf.newaxis, :] \n",
        "        # Esta mascará lo que haremos es expandirla porque ahora será una mascara con todo 0 o 1, con tantas filas como batch_size y tantas columnas como seq_length\n",
        "        # mask[:, tf.newaxis, tf.newaxis, :] la primera se queda al inicio (batch_size), añado 2 nuevas dimensiones (tf.newaxis) y me quedo con los valores de cada secuencia al final (:)\n",
        "        # Lo que estamos haciendo es agregar dos dimensiones esto para k,q,v y las segunda cada uno de los renglones ya que queremos que se aplique a toda una matriz en cuestion, ie, estamos redimensionando \n",
        "\n",
        "    # La función de la sig función será desactivar los elementos últimos, cuyos no queremos que aparezcan en la frase \n",
        "    # esta mascara debe tener unos en el tringulo superior \n",
        "    def create_look_ahead_mask(self, seq):\n",
        "        seq_len = tf.shape(seq)[1]\n",
        "        look_ahead_mask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0) # creamos un matriz triangular inferior. Con -1,0 le decimos que no queremos que la columna j sea mayor que el renglon i\n",
        "        return look_ahead_mask\n",
        "        # Conclusión: Por lo tanto la mascara impide que conozca las palabras que va a haber en el futuro.\n",
        "        # Notemos que no he añadido ningun tipo de padding por tanto pensaríamos que hace falta aplicar la función paddin_mask pero no nos hará falta del todo\n",
        "        # gracias a la función tf.maximum\n",
        "    \n",
        "    def call(self, enc_inputs, dec_inputs, training):\n",
        "        # Generamos la capa del encoder \n",
        "        enc_mask = self.create_padding_mask(enc_inputs)   # Solo nos preocupa que sea del tamaño adecuado \n",
        "        # Generamos la primera capa del decoder\n",
        "        dec_mask_1 = tf.maximum(\n",
        "            self.create_padding_mask(dec_inputs),\n",
        "            self.create_look_ahead_mask(dec_inputs)\n",
        "        )\n",
        "        # Creamos la segunda capa del decoder\n",
        "        dec_mask_2 = self.create_padding_mask(enc_inputs)\n",
        "        \n",
        "        \n",
        "        enc_outputs = self.encoder(enc_inputs, enc_mask, training)\n",
        "        dec_outputs = self.decoder(dec_inputs,\n",
        "                                   enc_outputs,\n",
        "                                   dec_mask_1,\n",
        "                                   dec_mask_2,\n",
        "                                   training)\n",
        "        \n",
        "        outputs = self.last_linear(dec_outputs)\n",
        "        \n",
        "        return outputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9yaFLhINRyUy"
      },
      "source": [
        "# Ejemplo de como trabajan las siguientes funciones\r\n",
        "def create_padding_mask( seq): #seq: (batch_size, seq_length) \r\n",
        "  mask = tf.cast(tf.math.equal(seq, 0), tf.float32)  \r\n",
        "  return mask[:, tf.newaxis, tf.newaxis, :] \r\n",
        "    \r\n",
        "def create_look_ahead_mask( seq):\r\n",
        "  seq_len = tf.shape(seq)[1]\r\n",
        "  look_ahead_mask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0) # creamos un matriz triangular inferior. Con -1,0 le decimos que no queremos que la columna j sea mayor que el renglon i\r\n",
        "  return look_ahead_mask\r\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UVMV5_-QSBOq",
        "outputId": "f9646e84-0cd4-4377-dc0e-af8e92c575bc"
      },
      "source": [
        "seq = tf.cast([[234,510,0,129,6,0,0,0]], tf.int32)\r\n",
        "# Lo siguiente crea un tensor donde a la propia frase le ha añadido una dimensión y de hecho 2 dimensiones extra en medio para ser capaz de \r\n",
        "# recrear esa palabra en un espacio de dimensión superior. Espacio donde por cierto, la propia función por construcción como yo lo que hago \r\n",
        "# es buscar donde están los 0´s, me ha colocado 1´s donde había 0´s.\r\n",
        "print(create_padding_mask(seq))\r\n",
        "# Lo siguiente crea una matriz con número de filas y columnas igual a la longitud de la secuencia y donde me es imposible acceder a todo lo\r\n",
        "# que viene después, ie a los númeor que están más allá de la diagonal\r\n",
        "print(create_look_ahead_mask(seq))\r\n",
        "# Lo siguiente hace que se me añadan los 0´s en cuestión y se me va a ampliar al tamaño adecuado pero al hacer el máximo entre este y el \r\n",
        "# create_look_ahead_mask me va aguardar los 1´s de ese triangulo superior cosa que no guardaba el padding. Podemos apreciar que ademas de que\r\n",
        "# tengamos unos arriba de la diagonal también me bloquea ya donde hay 0´s\r\n",
        "print(tf.maximum(create_padding_mask(seq), create_look_ahead_mask(seq)))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor([[[[0. 0. 1. 0. 0. 1. 1. 1.]]]], shape=(1, 1, 1, 8), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[0. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [0. 0. 1. 1. 1. 1. 1. 1.]\n",
            " [0. 0. 0. 1. 1. 1. 1. 1.]\n",
            " [0. 0. 0. 0. 1. 1. 1. 1.]\n",
            " [0. 0. 0. 0. 0. 1. 1. 1.]\n",
            " [0. 0. 0. 0. 0. 0. 1. 1.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0.]], shape=(8, 8), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[[[0. 1. 1. 1. 1. 1. 1. 1.]\n",
            "   [0. 0. 1. 1. 1. 1. 1. 1.]\n",
            "   [0. 0. 1. 1. 1. 1. 1. 1.]\n",
            "   [0. 0. 1. 0. 1. 1. 1. 1.]\n",
            "   [0. 0. 1. 0. 0. 1. 1. 1.]\n",
            "   [0. 0. 1. 0. 0. 1. 1. 1.]\n",
            "   [0. 0. 1. 0. 0. 1. 1. 1.]\n",
            "   [0. 0. 1. 0. 0. 1. 1. 1.]]]], shape=(1, 1, 8, 8), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-c-LRThUPrso"
      },
      "source": [
        "# Entrenamiento"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qiOdqQ5qPs8z"
      },
      "source": [
        "tf.keras.backend.clear_session()\n",
        "\n",
        "# Hiper Parámetros\n",
        "D_MODEL = 128 # 512\n",
        "NB_LAYERS = 4 # 6\n",
        "FFN_UNITS = 512 # 2048\n",
        "NB_PROJ = 8 # 8\n",
        "DROPOUT_RATE = 0.1 # 0.1\n",
        "\n",
        "transformer = Transformer(vocab_size_enc=VOCAB_SIZE_EN,\n",
        "                          vocab_size_dec=VOCAB_SIZE_ES,\n",
        "                          d_model=D_MODEL,\n",
        "                          nb_layers=NB_LAYERS,\n",
        "                          FFN_units=FFN_UNITS,\n",
        "                          nb_proj=NB_PROJ,\n",
        "                          dropout_rate=DROPOUT_RATE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "46xg4Wrg1Wgl"
      },
      "source": [
        "# Trabajaremos con SparseCategoricalCrossentropy ya que queremos maximizar la probabilidad de una palabra se encuentre a lado de otra\n",
        "# y SparseCategoricalCrossentropy es el objeto standar cuando lo que tenemos son probabilidades y de hecho no son probas hasta que apliquemos softmax\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True,\n",
        "                                                            reduction=\"none\")\n",
        "# from_logits=True: definimos que las salidas son numero reales preparados para aplicarseles la función softmax \n",
        "# reduction: no queremos que aplique ninguna reducción, media, etc. No queremos un único número\n",
        "\n",
        "def loss_function(target, pred):\n",
        "    mask = tf.math.logical_not(tf.math.equal(target, 0)) # Buscamos las posiciones donde la matriz target tenga token iguales a 0, marcaremos los valores que no fueran 0´s\n",
        "    loss_ = loss_object(target, pred) \n",
        "    \n",
        "    mask = tf.cast(mask, dtype=loss_.dtype)   # para poder enmascarar, quitar esos lugares donde la pérdida no me interesa porque aporta 0 a lo que es el modelo\n",
        "    loss_ *= mask # Filtramos que loss solo tiene valores no nulos \n",
        "    \n",
        "    return tf.reduce_mean(loss_)\n",
        "\n",
        "# Nos quedamos con la pérdida y la precisión en el entrenamiento\n",
        "train_loss = tf.keras.metrics.Mean(name=\"train_loss\")\n",
        "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name=\"train_accuracy\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Goque362343"
      },
      "source": [
        "# Optimizador del gradiente descendente\n",
        "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "    \n",
        "    def __init__(self, d_model, warmup_steps=4000):\n",
        "        super(CustomSchedule, self).__init__()\n",
        "        \n",
        "        self.d_model = tf.cast(d_model, tf.float32)\n",
        "        self.warmup_steps = warmup_steps\n",
        "    \n",
        "    def __call__(self, step):\n",
        "        arg1 = tf.math.rsqrt(step)    # Formula que considerará los primeors 4000 pasos\n",
        "        arg2 = step * (self.warmup_steps**-1.5) # Formula que considerará el learning rate después de los 4000 pasos\n",
        "        \n",
        "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)  # Por lo tanto el learning rate será variable\n",
        "\n",
        "leaning_rate = CustomSchedule(D_MODEL)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(leaning_rate,\n",
        "                                     beta_1=0.9,\n",
        "                                     beta_2=0.98,\n",
        "                                     epsilon=1e-9)\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nb_32PIU5Zkh"
      },
      "source": [
        "checkpoint_path = \"/content/drive/MyDrive/Curso NLP/Transformer/checkpoints/\"\n",
        "\n",
        "ckpt = tf.train.Checkpoint(transformer=transformer,\n",
        "                           optimizer=optimizer)\n",
        "\n",
        "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
        "# max_to_keep: máximo núemro de checkpoints a almacenar\n",
        "\n",
        "# Si después de crear todos estos vinculos el manager detecta que ya hay un checkpoint en el sistema\n",
        "# el manager se encargará de cargarlo\n",
        "if ckpt_manager.latest_checkpoint:\n",
        "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
        "    print(\"Último checkpoint restaurado!!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lhFK5kUx602K",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "bb4e7c4b-c112-478e-f76c-8ab979724455"
      },
      "source": [
        "EPOCHS = 10\n",
        "for epoch in range(EPOCHS):\n",
        "    print(\"Inicio del epoch {}\".format(epoch+1))\n",
        "    start = time.time()\n",
        "    \n",
        "    # Reseteamos train_loss y train_accuracy en cada epoch \n",
        "    train_loss.reset_states()\n",
        "    train_accuracy.reset_states()\n",
        "    \n",
        "    # batch: identificador de la fila; lote número 0, lote número 1, etc\n",
        "    # enc_inputs: frase de inicio. Entrada del codificador\n",
        "    # targets_ frase objetivo. Contra que voy a comparar la salida del descodificador.\n",
        "    for (batch, (enc_inputs, targets)) in enumerate(dataset):\n",
        "        dec_inputs = targets[:, :-1]      # El último es un token de fin de frase. Así lo construimos \n",
        "        dec_outputs_real = targets[:, 1:] # Todas las frases del bloque a excepción del primer token que es de inicio de frase\n",
        "        with tf.GradientTape() as tape:   # Construimos un objeto que nos permitira registrar todo lo que le ocurra al modelo con respecto a las predicciones\n",
        "            predictions = transformer(enc_inputs, dec_inputs, True) # True: dado que estamos en fase de entrenamiento\n",
        "            loss = loss_function(dec_outputs_real, predictions) # Aplicamos la función de pérdidas\n",
        "        \n",
        "        # Gracias a que creamos el objeto tape podemos pedir que obtenga del objeto guardado el gradiente de \n",
        "        # la función lde pérdidas con respecto a las variables del transformer\n",
        "        gradients = tape.gradient(loss, transformer.trainable_variables)\n",
        "        # Aplicamos los gradientes de nuestro optimizador respecto a las varibales que se pueden modificar/entrenar\n",
        "        # Es decir le decimos cuales son las varibales más culpables de que estemos comentiendo ese error\n",
        "        optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
        "        \n",
        "        train_loss(loss)    # Evaluamos la pérdida\n",
        "        train_accuracy(dec_outputs_real, predictions) # Evaluamos la presición\n",
        "        \n",
        "        # Si el bloque es multiplo de 50 mostramos la siguiente info para llevar registro\n",
        "        if batch % 50 == 0:\n",
        "            print(\"Epoch {} Lote {} Pérdida {:.4f} Precisión {:.4f}\".format(\n",
        "                epoch+1, batch, train_loss.result(), train_accuracy.result()))\n",
        "    \n",
        "    # No queremos iniciar otra epoca hasta que hallamos guardado todos los pesos de la época inmediata \n",
        "    # anterior, por lo tanto haremos un checkpoint\n",
        "    ckpt_save_path = ckpt_manager.save()\n",
        "    print(\"Guardando checkpoint para el epoch {} en {}\".format(epoch+1,\n",
        "                                                        ckpt_save_path))\n",
        "    print(\"Tiempo que ha tardado 1 epoch: {} segs\\n\".format(time.time() - start))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Inicio del epoch 1\n",
            "Epoch 1 Lote 0 Pérdida 5.7712 Precisión 0.0000\n",
            "Epoch 1 Lote 50 Pérdida 6.0703 Precisión 0.0008\n",
            "Epoch 1 Lote 100 Pérdida 6.0209 Precisión 0.0249\n",
            "Epoch 1 Lote 150 Pérdida 5.9672 Precisión 0.0341\n",
            "Epoch 1 Lote 200 Pérdida 5.8880 Precisión 0.0388\n",
            "Epoch 1 Lote 250 Pérdida 5.7829 Precisión 0.0431\n",
            "Epoch 1 Lote 300 Pérdida 5.6780 Precisión 0.0512\n",
            "Epoch 1 Lote 350 Pérdida 5.5599 Precisión 0.0575\n",
            "Epoch 1 Lote 400 Pérdida 5.4499 Precisión 0.0622\n",
            "Epoch 1 Lote 450 Pérdida 5.3427 Precisión 0.0662\n",
            "Epoch 1 Lote 500 Pérdida 5.2409 Precisión 0.0704\n",
            "Epoch 1 Lote 550 Pérdida 5.1469 Precisión 0.0750\n",
            "Epoch 1 Lote 600 Pérdida 5.0560 Precisión 0.0796\n",
            "Epoch 1 Lote 650 Pérdida 4.9704 Precisión 0.0841\n",
            "Epoch 1 Lote 700 Pérdida 4.8874 Precisión 0.0882\n",
            "Epoch 1 Lote 750 Pérdida 4.8114 Precisión 0.0924\n",
            "Epoch 1 Lote 800 Pérdida 4.7401 Precisión 0.0967\n",
            "Epoch 1 Lote 850 Pérdida 4.6730 Precisión 0.1008\n",
            "Epoch 1 Lote 900 Pérdida 4.6113 Precisión 0.1048\n",
            "Epoch 1 Lote 950 Pérdida 4.5494 Precisión 0.1086\n",
            "Epoch 1 Lote 1000 Pérdida 4.4932 Precisión 0.1124\n",
            "Epoch 1 Lote 1050 Pérdida 4.4374 Precisión 0.1158\n",
            "Epoch 1 Lote 1100 Pérdida 4.3844 Precisión 0.1191\n",
            "Epoch 1 Lote 1150 Pérdida 4.3329 Precisión 0.1224\n",
            "Epoch 1 Lote 1200 Pérdida 4.2865 Precisión 0.1255\n",
            "Epoch 1 Lote 1250 Pérdida 4.2397 Precisión 0.1285\n",
            "Epoch 1 Lote 1300 Pérdida 4.1947 Precisión 0.1312\n",
            "Epoch 1 Lote 1350 Pérdida 4.1520 Precisión 0.1339\n",
            "Epoch 1 Lote 1400 Pérdida 4.1133 Precisión 0.1366\n",
            "Epoch 1 Lote 1450 Pérdida 4.0745 Precisión 0.1392\n",
            "Epoch 1 Lote 1500 Pérdida 4.0383 Precisión 0.1416\n",
            "Epoch 1 Lote 1550 Pérdida 4.0026 Precisión 0.1439\n",
            "Epoch 1 Lote 1600 Pérdida 3.9673 Precisión 0.1462\n",
            "Epoch 1 Lote 1650 Pérdida 3.9335 Precisión 0.1485\n",
            "Epoch 1 Lote 1700 Pérdida 3.9013 Precisión 0.1508\n",
            "Epoch 1 Lote 1750 Pérdida 3.8685 Precisión 0.1529\n",
            "Epoch 1 Lote 1800 Pérdida 3.8372 Precisión 0.1549\n",
            "Epoch 1 Lote 1850 Pérdida 3.8068 Precisión 0.1569\n",
            "Epoch 1 Lote 1900 Pérdida 3.7779 Precisión 0.1587\n",
            "Epoch 1 Lote 1950 Pérdida 3.7506 Precisión 0.1605\n",
            "Epoch 1 Lote 2000 Pérdida 3.7238 Precisión 0.1624\n",
            "Epoch 1 Lote 2050 Pérdida 3.6972 Precisión 0.1642\n",
            "Epoch 1 Lote 2100 Pérdida 3.6713 Precisión 0.1661\n",
            "Epoch 1 Lote 2150 Pérdida 3.6463 Precisión 0.1679\n",
            "Epoch 1 Lote 2200 Pérdida 3.6211 Precisión 0.1698\n",
            "Epoch 1 Lote 2250 Pérdida 3.5963 Precisión 0.1715\n",
            "Epoch 1 Lote 2300 Pérdida 3.5718 Precisión 0.1733\n",
            "Epoch 1 Lote 2350 Pérdida 3.5482 Precisión 0.1751\n",
            "Epoch 1 Lote 2400 Pérdida 3.5251 Precisión 0.1768\n",
            "Epoch 1 Lote 2450 Pérdida 3.5016 Precisión 0.1786\n",
            "Epoch 1 Lote 2500 Pérdida 3.4787 Precisión 0.1805\n",
            "Epoch 1 Lote 2550 Pérdida 3.4551 Precisión 0.1822\n",
            "Epoch 1 Lote 2600 Pérdida 3.4322 Precisión 0.1841\n",
            "Epoch 1 Lote 2650 Pérdida 3.4102 Precisión 0.1859\n",
            "Epoch 1 Lote 2700 Pérdida 3.3890 Precisión 0.1878\n",
            "Epoch 1 Lote 2750 Pérdida 3.3689 Precisión 0.1897\n",
            "Epoch 1 Lote 2800 Pérdida 3.3484 Precisión 0.1916\n",
            "Epoch 1 Lote 2850 Pérdida 3.3280 Precisión 0.1934\n",
            "Epoch 1 Lote 2900 Pérdida 3.3086 Precisión 0.1952\n",
            "Epoch 1 Lote 2950 Pérdida 3.2898 Precisión 0.1970\n",
            "Epoch 1 Lote 3000 Pérdida 3.2707 Precisión 0.1990\n",
            "Epoch 1 Lote 3050 Pérdida 3.2522 Precisión 0.2009\n",
            "Epoch 1 Lote 3100 Pérdida 3.2344 Precisión 0.2028\n",
            "Epoch 1 Lote 3150 Pérdida 3.2168 Precisión 0.2046\n",
            "Epoch 1 Lote 3200 Pérdida 3.1991 Precisión 0.2065\n",
            "Epoch 1 Lote 3250 Pérdida 3.1820 Precisión 0.2083\n",
            "Epoch 1 Lote 3300 Pérdida 3.1649 Precisión 0.2102\n",
            "Epoch 1 Lote 3350 Pérdida 3.1481 Precisión 0.2120\n",
            "Epoch 1 Lote 3400 Pérdida 3.1314 Precisión 0.2138\n",
            "Epoch 1 Lote 3450 Pérdida 3.1151 Precisión 0.2156\n",
            "Epoch 1 Lote 3500 Pérdida 3.0987 Precisión 0.2174\n",
            "Epoch 1 Lote 3550 Pérdida 3.0830 Precisión 0.2192\n",
            "Epoch 1 Lote 3600 Pérdida 3.0669 Precisión 0.2208\n",
            "Epoch 1 Lote 3650 Pérdida 3.0511 Precisión 0.2225\n",
            "Epoch 1 Lote 3700 Pérdida 3.0356 Precisión 0.2242\n",
            "Epoch 1 Lote 3750 Pérdida 3.0201 Precisión 0.2259\n",
            "Epoch 1 Lote 3800 Pérdida 3.0056 Precisión 0.2276\n",
            "Epoch 1 Lote 3850 Pérdida 2.9907 Precisión 0.2292\n",
            "Epoch 1 Lote 3900 Pérdida 2.9762 Precisión 0.2309\n",
            "Epoch 1 Lote 3950 Pérdida 2.9616 Precisión 0.2325\n",
            "Epoch 1 Lote 4000 Pérdida 2.9472 Precisión 0.2341\n",
            "Epoch 1 Lote 4050 Pérdida 2.9330 Precisión 0.2358\n",
            "Epoch 1 Lote 4100 Pérdida 2.9189 Precisión 0.2374\n",
            "Epoch 1 Lote 4150 Pérdida 2.9043 Precisión 0.2391\n",
            "Epoch 1 Lote 4200 Pérdida 2.8902 Precisión 0.2408\n",
            "Epoch 1 Lote 4250 Pérdida 2.8767 Precisión 0.2424\n",
            "Epoch 1 Lote 4300 Pérdida 2.8634 Precisión 0.2441\n",
            "Epoch 1 Lote 4350 Pérdida 2.8503 Precisión 0.2457\n",
            "Epoch 1 Lote 4400 Pérdida 2.8371 Precisión 0.2473\n",
            "Epoch 1 Lote 4450 Pérdida 2.8239 Precisión 0.2489\n",
            "Epoch 1 Lote 4500 Pérdida 2.8110 Precisión 0.2504\n",
            "Epoch 1 Lote 4550 Pérdida 2.7981 Precisión 0.2520\n",
            "Epoch 1 Lote 4600 Pérdida 2.7854 Precisión 0.2536\n",
            "Epoch 1 Lote 4650 Pérdida 2.7728 Precisión 0.2551\n",
            "Epoch 1 Lote 4700 Pérdida 2.7614 Precisión 0.2565\n",
            "Epoch 1 Lote 4750 Pérdida 2.7508 Precisión 0.2578\n",
            "Epoch 1 Lote 4800 Pérdida 2.7407 Precisión 0.2590\n",
            "Epoch 1 Lote 4850 Pérdida 2.7307 Precisión 0.2601\n",
            "Epoch 1 Lote 4900 Pérdida 2.7215 Precisión 0.2612\n",
            "Epoch 1 Lote 4950 Pérdida 2.7125 Precisión 0.2622\n",
            "Epoch 1 Lote 5000 Pérdida 2.7041 Precisión 0.2632\n",
            "Epoch 1 Lote 5050 Pérdida 2.6956 Precisión 0.2641\n",
            "Epoch 1 Lote 5100 Pérdida 2.6872 Precisión 0.2650\n",
            "Epoch 1 Lote 5150 Pérdida 2.6791 Precisión 0.2659\n",
            "Epoch 1 Lote 5200 Pérdida 2.6708 Precisión 0.2667\n",
            "Epoch 1 Lote 5250 Pérdida 2.6632 Precisión 0.2675\n",
            "Epoch 1 Lote 5300 Pérdida 2.6554 Precisión 0.2683\n",
            "Epoch 1 Lote 5350 Pérdida 2.6480 Precisión 0.2692\n",
            "Epoch 1 Lote 5400 Pérdida 2.6405 Precisión 0.2700\n",
            "Epoch 1 Lote 5450 Pérdida 2.6328 Precisión 0.2708\n",
            "Epoch 1 Lote 5500 Pérdida 2.6254 Precisión 0.2716\n",
            "Epoch 1 Lote 5550 Pérdida 2.6182 Precisión 0.2725\n",
            "Epoch 1 Lote 5600 Pérdida 2.6109 Precisión 0.2733\n",
            "Epoch 1 Lote 5650 Pérdida 2.6037 Precisión 0.2740\n",
            "Epoch 1 Lote 5700 Pérdida 2.5967 Precisión 0.2747\n",
            "Epoch 1 Lote 5750 Pérdida 2.5896 Precisión 0.2755\n",
            "Epoch 1 Lote 5800 Pérdida 2.5829 Precisión 0.2763\n",
            "Epoch 1 Lote 5850 Pérdida 2.5760 Precisión 0.2770\n",
            "Epoch 1 Lote 5900 Pérdida 2.5691 Precisión 0.2777\n",
            "Epoch 1 Lote 5950 Pérdida 2.5625 Precisión 0.2784\n",
            "Epoch 1 Lote 6000 Pérdida 2.5556 Precisión 0.2790\n",
            "Epoch 1 Lote 6050 Pérdida 2.5487 Precisión 0.2797\n",
            "Epoch 1 Lote 6100 Pérdida 2.5418 Precisión 0.2804\n",
            "Epoch 1 Lote 6150 Pérdida 2.5352 Precisión 0.2810\n",
            "Epoch 1 Lote 6200 Pérdida 2.5286 Precisión 0.2817\n",
            "Epoch 1 Lote 6250 Pérdida 2.5220 Precisión 0.2824\n",
            "Epoch 1 Lote 6300 Pérdida 2.5156 Precisión 0.2831\n",
            "Epoch 1 Lote 6350 Pérdida 2.5088 Precisión 0.2837\n",
            "Epoch 1 Lote 6400 Pérdida 2.5024 Precisión 0.2844\n",
            "Guardando checkpoint para el epoch 1 en ./drive/My Drive/Curso de NLP/Transformer/ckpt/ckpt-1\n",
            "Tiempo que ha tardado 1 epoch: 1600.0238528251648 segs\n",
            "\n",
            "Inicio del epoch 2\n",
            "Epoch 2 Lote 0 Pérdida 1.7629 Precisión 0.3660\n",
            "Epoch 2 Lote 50 Pérdida 1.7098 Precisión 0.3701\n",
            "Epoch 2 Lote 100 Pérdida 1.7051 Precisión 0.3714\n",
            "Epoch 2 Lote 150 Pérdida 1.6910 Precisión 0.3723\n",
            "Epoch 2 Lote 200 Pérdida 1.6840 Precisión 0.3730\n",
            "Epoch 2 Lote 250 Pérdida 1.6751 Precisión 0.3736\n",
            "Epoch 2 Lote 300 Pérdida 1.6703 Precisión 0.3742\n",
            "Epoch 2 Lote 350 Pérdida 1.6614 Precisión 0.3752\n",
            "Epoch 2 Lote 400 Pérdida 1.6598 Precisión 0.3753\n",
            "Epoch 2 Lote 450 Pérdida 1.6600 Precisión 0.3753\n",
            "Epoch 2 Lote 500 Pérdida 1.6546 Precisión 0.3755\n",
            "Epoch 2 Lote 550 Pérdida 1.6527 Precisión 0.3757\n",
            "Epoch 2 Lote 600 Pérdida 1.6495 Precisión 0.3760\n",
            "Epoch 2 Lote 650 Pérdida 1.6469 Precisión 0.3759\n",
            "Epoch 2 Lote 700 Pérdida 1.6414 Precisión 0.3760\n",
            "Epoch 2 Lote 750 Pérdida 1.6359 Precisión 0.3773\n",
            "Epoch 2 Lote 800 Pérdida 1.6291 Precisión 0.3784\n",
            "Epoch 2 Lote 850 Pérdida 1.6239 Precisión 0.3798\n",
            "Epoch 2 Lote 900 Pérdida 1.6168 Precisión 0.3809\n",
            "Epoch 2 Lote 950 Pérdida 1.6089 Precisión 0.3824\n",
            "Epoch 2 Lote 1000 Pérdida 1.6021 Precisión 0.3839\n",
            "Epoch 2 Lote 1050 Pérdida 1.5962 Precisión 0.3848\n",
            "Epoch 2 Lote 1100 Pérdida 1.5898 Precisión 0.3855\n",
            "Epoch 2 Lote 1150 Pérdida 1.5835 Precisión 0.3866\n",
            "Epoch 2 Lote 1200 Pérdida 1.5770 Precisión 0.3876\n",
            "Epoch 2 Lote 1250 Pérdida 1.5699 Precisión 0.3884\n",
            "Epoch 2 Lote 1300 Pérdida 1.5621 Precisión 0.3893\n",
            "Epoch 2 Lote 1350 Pérdida 1.5550 Precisión 0.3901\n",
            "Epoch 2 Lote 1400 Pérdida 1.5477 Precisión 0.3910\n",
            "Epoch 2 Lote 1450 Pérdida 1.5403 Precisión 0.3918\n",
            "Epoch 2 Lote 1500 Pérdida 1.5338 Precisión 0.3927\n",
            "Epoch 2 Lote 1550 Pérdida 1.5281 Precisión 0.3934\n",
            "Epoch 2 Lote 1600 Pérdida 1.5212 Precisión 0.3943\n",
            "Epoch 2 Lote 1650 Pérdida 1.5148 Precisión 0.3952\n",
            "Epoch 2 Lote 1700 Pérdida 1.5089 Precisión 0.3960\n",
            "Epoch 2 Lote 1750 Pérdida 1.5025 Precisión 0.3966\n",
            "Epoch 2 Lote 1800 Pérdida 1.4967 Precisión 0.3973\n",
            "Epoch 2 Lote 1850 Pérdida 1.4911 Precisión 0.3978\n",
            "Epoch 2 Lote 1900 Pérdida 1.4856 Precisión 0.3984\n",
            "Epoch 2 Lote 1950 Pérdida 1.4799 Precisión 0.3989\n",
            "Epoch 2 Lote 2000 Pérdida 1.4753 Precisión 0.3995\n",
            "Epoch 2 Lote 2050 Pérdida 1.4698 Precisión 0.4002\n",
            "Epoch 2 Lote 2100 Pérdida 1.4645 Precisión 0.4007\n",
            "Epoch 2 Lote 2150 Pérdida 1.4594 Precisión 0.4013\n",
            "Epoch 2 Lote 2200 Pérdida 1.4545 Precisión 0.4018\n",
            "Epoch 2 Lote 2250 Pérdida 1.4496 Precisión 0.4023\n",
            "Epoch 2 Lote 2300 Pérdida 1.4447 Precisión 0.4028\n",
            "Epoch 2 Lote 2350 Pérdida 1.4406 Precisión 0.4034\n",
            "Epoch 2 Lote 2400 Pérdida 1.4357 Precisión 0.4039\n",
            "Epoch 2 Lote 2450 Pérdida 1.4304 Precisión 0.4045\n",
            "Epoch 2 Lote 2500 Pérdida 1.4253 Precisión 0.4051\n",
            "Epoch 2 Lote 2550 Pérdida 1.4203 Precisión 0.4057\n",
            "Epoch 2 Lote 2600 Pérdida 1.4156 Precisión 0.4063\n",
            "Epoch 2 Lote 2650 Pérdida 1.4114 Precisión 0.4068\n",
            "Epoch 2 Lote 2700 Pérdida 1.4075 Precisión 0.4074\n",
            "Epoch 2 Lote 2750 Pérdida 1.4035 Precisión 0.4080\n",
            "Epoch 2 Lote 2800 Pérdida 1.4000 Precisión 0.4084\n",
            "Epoch 2 Lote 2850 Pérdida 1.3959 Precisión 0.4088\n",
            "Epoch 2 Lote 2900 Pérdida 1.3923 Precisión 0.4093\n",
            "Epoch 2 Lote 2950 Pérdida 1.3889 Precisión 0.4098\n",
            "Epoch 2 Lote 3000 Pérdida 1.3859 Precisión 0.4102\n",
            "Epoch 2 Lote 3050 Pérdida 1.3825 Precisión 0.4107\n",
            "Epoch 2 Lote 3100 Pérdida 1.3798 Precisión 0.4112\n",
            "Epoch 2 Lote 3150 Pérdida 1.3770 Precisión 0.4118\n",
            "Epoch 2 Lote 3200 Pérdida 1.3742 Precisión 0.4123\n",
            "Epoch 2 Lote 3250 Pérdida 1.3711 Precisión 0.4129\n",
            "Epoch 2 Lote 3300 Pérdida 1.3683 Precisión 0.4135\n",
            "Epoch 2 Lote 3350 Pérdida 1.3660 Precisión 0.4140\n",
            "Epoch 2 Lote 3400 Pérdida 1.3635 Precisión 0.4144\n",
            "Epoch 2 Lote 3450 Pérdida 1.3610 Precisión 0.4149\n",
            "Epoch 2 Lote 3500 Pérdida 1.3585 Precisión 0.4155\n",
            "Epoch 2 Lote 3550 Pérdida 1.3559 Precisión 0.4160\n",
            "Epoch 2 Lote 3600 Pérdida 1.3534 Precisión 0.4165\n",
            "Epoch 2 Lote 3650 Pérdida 1.3505 Precisión 0.4171\n",
            "Epoch 2 Lote 3700 Pérdida 1.3477 Precisión 0.4176\n",
            "Epoch 2 Lote 3750 Pérdida 1.3448 Precisión 0.4181\n",
            "Epoch 2 Lote 3800 Pérdida 1.3421 Precisión 0.4186\n",
            "Epoch 2 Lote 3850 Pérdida 1.3393 Precisión 0.4191\n",
            "Epoch 2 Lote 3900 Pérdida 1.3372 Precisión 0.4196\n",
            "Epoch 2 Lote 3950 Pérdida 1.3345 Precisión 0.4201\n",
            "Epoch 2 Lote 4000 Pérdida 1.3321 Precisión 0.4206\n",
            "Epoch 2 Lote 4050 Pérdida 1.3295 Precisión 0.4212\n",
            "Epoch 2 Lote 4100 Pérdida 1.3267 Precisión 0.4217\n",
            "Epoch 2 Lote 4150 Pérdida 1.3240 Precisión 0.4223\n",
            "Epoch 2 Lote 4200 Pérdida 1.3212 Precisión 0.4229\n",
            "Epoch 2 Lote 4250 Pérdida 1.3186 Precisión 0.4234\n",
            "Epoch 2 Lote 4300 Pérdida 1.3161 Precisión 0.4240\n",
            "Epoch 2 Lote 4350 Pérdida 1.3135 Precisión 0.4245\n",
            "Epoch 2 Lote 4400 Pérdida 1.3108 Precisión 0.4251\n",
            "Epoch 2 Lote 4450 Pérdida 1.3083 Precisión 0.4257\n",
            "Epoch 2 Lote 4500 Pérdida 1.3060 Precisión 0.4263\n",
            "Epoch 2 Lote 4550 Pérdida 1.3035 Precisión 0.4268\n",
            "Epoch 2 Lote 4600 Pérdida 1.3014 Precisión 0.4273\n",
            "Epoch 2 Lote 4650 Pérdida 1.2989 Precisión 0.4278\n",
            "Epoch 2 Lote 4700 Pérdida 1.2973 Precisión 0.4282\n",
            "Epoch 2 Lote 4750 Pérdida 1.2966 Precisión 0.4284\n",
            "Epoch 2 Lote 4800 Pérdida 1.2960 Precisión 0.4286\n",
            "Epoch 2 Lote 4850 Pérdida 1.2962 Precisión 0.4287\n",
            "Epoch 2 Lote 4900 Pérdida 1.2965 Precisión 0.4287\n",
            "Epoch 2 Lote 4950 Pérdida 1.2973 Precisión 0.4286\n",
            "Epoch 2 Lote 5000 Pérdida 1.2980 Precisión 0.4286\n",
            "Epoch 2 Lote 5050 Pérdida 1.2988 Precisión 0.4285\n",
            "Epoch 2 Lote 5100 Pérdida 1.3000 Precisión 0.4284\n",
            "Epoch 2 Lote 5150 Pérdida 1.3011 Precisión 0.4284\n",
            "Epoch 2 Lote 5200 Pérdida 1.3026 Precisión 0.4282\n",
            "Epoch 2 Lote 5250 Pérdida 1.3038 Precisión 0.4280\n",
            "Epoch 2 Lote 5300 Pérdida 1.3048 Precisión 0.4279\n",
            "Epoch 2 Lote 5350 Pérdida 1.3063 Precisión 0.4277\n",
            "Epoch 2 Lote 5400 Pérdida 1.3076 Precisión 0.4277\n",
            "Epoch 2 Lote 5450 Pérdida 1.3088 Precisión 0.4275\n",
            "Epoch 2 Lote 5500 Pérdida 1.3097 Precisión 0.4274\n",
            "Epoch 2 Lote 5550 Pérdida 1.3107 Precisión 0.4273\n",
            "Epoch 2 Lote 5600 Pérdida 1.3119 Precisión 0.4271\n",
            "Epoch 2 Lote 5650 Pérdida 1.3130 Precisión 0.4270\n",
            "Epoch 2 Lote 5700 Pérdida 1.3141 Precisión 0.4268\n",
            "Epoch 2 Lote 5750 Pérdida 1.3152 Precisión 0.4267\n",
            "Epoch 2 Lote 5800 Pérdida 1.3164 Precisión 0.4265\n",
            "Epoch 2 Lote 5850 Pérdida 1.3173 Precisión 0.4264\n",
            "Epoch 2 Lote 5900 Pérdida 1.3183 Precisión 0.4262\n",
            "Epoch 2 Lote 5950 Pérdida 1.3193 Precisión 0.4260\n",
            "Epoch 2 Lote 6000 Pérdida 1.3201 Precisión 0.4258\n",
            "Epoch 2 Lote 6050 Pérdida 1.3208 Precisión 0.4256\n",
            "Epoch 2 Lote 6100 Pérdida 1.3217 Precisión 0.4254\n",
            "Epoch 2 Lote 6150 Pérdida 1.3222 Precisión 0.4253\n",
            "Epoch 2 Lote 6200 Pérdida 1.3227 Precisión 0.4251\n",
            "Epoch 2 Lote 6250 Pérdida 1.3235 Precisión 0.4249\n",
            "Epoch 2 Lote 6300 Pérdida 1.3240 Precisión 0.4248\n",
            "Epoch 2 Lote 6350 Pérdida 1.3245 Precisión 0.4247\n",
            "Epoch 2 Lote 6400 Pérdida 1.3250 Precisión 0.4246\n",
            "Guardando checkpoint para el epoch 2 en ./drive/My Drive/Curso de NLP/Transformer/ckpt/ckpt-2\n",
            "Tiempo que ha tardado 1 epoch: 1566.3543708324432 segs\n",
            "\n",
            "Inicio del epoch 3\n",
            "Epoch 3 Lote 0 Pérdida 1.6048 Precisión 0.3882\n",
            "Epoch 3 Lote 50 Pérdida 1.3913 Precisión 0.4109\n",
            "Epoch 3 Lote 100 Pérdida 1.3957 Precisión 0.4129\n",
            "Epoch 3 Lote 150 Pérdida 1.3935 Precisión 0.4118\n",
            "Epoch 3 Lote 200 Pérdida 1.3940 Precisión 0.4134\n",
            "Epoch 3 Lote 250 Pérdida 1.3900 Precisión 0.4133\n",
            "Epoch 3 Lote 300 Pérdida 1.3877 Precisión 0.4142\n",
            "Epoch 3 Lote 350 Pérdida 1.3832 Precisión 0.4146\n",
            "Epoch 3 Lote 400 Pérdida 1.3779 Precisión 0.4149\n",
            "Epoch 3 Lote 450 Pérdida 1.3767 Precisión 0.4147\n",
            "Epoch 3 Lote 500 Pérdida 1.3732 Precisión 0.4154\n",
            "Epoch 3 Lote 550 Pérdida 1.3686 Precisión 0.4155\n",
            "Epoch 3 Lote 600 Pérdida 1.3685 Precisión 0.4151\n",
            "Epoch 3 Lote 650 Pérdida 1.3670 Precisión 0.4152\n",
            "Epoch 3 Lote 700 Pérdida 1.3643 Precisión 0.4152\n",
            "Epoch 3 Lote 750 Pérdida 1.3610 Precisión 0.4161\n",
            "Epoch 3 Lote 800 Pérdida 1.3571 Precisión 0.4169\n",
            "Epoch 3 Lote 850 Pérdida 1.3522 Precisión 0.4181\n",
            "Epoch 3 Lote 900 Pérdida 1.3448 Precisión 0.4193\n",
            "Epoch 3 Lote 950 Pérdida 1.3386 Precisión 0.4206\n",
            "Epoch 3 Lote 1000 Pérdida 1.3315 Precisión 0.4215\n",
            "Epoch 3 Lote 1050 Pérdida 1.3248 Precisión 0.4227\n",
            "Epoch 3 Lote 1100 Pérdida 1.3169 Precisión 0.4235\n",
            "Epoch 3 Lote 1150 Pérdida 1.3110 Precisión 0.4242\n",
            "Epoch 3 Lote 1200 Pérdida 1.3049 Precisión 0.4252\n",
            "Epoch 3 Lote 1250 Pérdida 1.2990 Precisión 0.4259\n",
            "Epoch 3 Lote 1300 Pérdida 1.2928 Precisión 0.4265\n",
            "Epoch 3 Lote 1350 Pérdida 1.2868 Precisión 0.4273\n",
            "Epoch 3 Lote 1400 Pérdida 1.2804 Precisión 0.4281\n",
            "Epoch 3 Lote 1450 Pérdida 1.2748 Precisión 0.4291\n",
            "Epoch 3 Lote 1500 Pérdida 1.2695 Precisión 0.4298\n",
            "Epoch 3 Lote 1550 Pérdida 1.2639 Precisión 0.4306\n",
            "Epoch 3 Lote 1600 Pérdida 1.2589 Precisión 0.4314\n",
            "Epoch 3 Lote 1650 Pérdida 1.2545 Precisión 0.4322\n",
            "Epoch 3 Lote 1700 Pérdida 1.2497 Precisión 0.4330\n",
            "Epoch 3 Lote 1750 Pérdida 1.2447 Precisión 0.4336\n",
            "Epoch 3 Lote 1800 Pérdida 1.2400 Precisión 0.4342\n",
            "Epoch 3 Lote 1850 Pérdida 1.2357 Precisión 0.4346\n",
            "Epoch 3 Lote 1900 Pérdida 1.2311 Precisión 0.4350\n",
            "Epoch 3 Lote 1950 Pérdida 1.2268 Precisión 0.4354\n",
            "Epoch 3 Lote 2000 Pérdida 1.2229 Precisión 0.4358\n",
            "Epoch 3 Lote 2050 Pérdida 1.2190 Precisión 0.4363\n",
            "Epoch 3 Lote 2100 Pérdida 1.2148 Precisión 0.4368\n",
            "Epoch 3 Lote 2150 Pérdida 1.2109 Precisión 0.4372\n",
            "Epoch 3 Lote 2200 Pérdida 1.2074 Precisión 0.4377\n",
            "Epoch 3 Lote 2250 Pérdida 1.2027 Precisión 0.4381\n",
            "Epoch 3 Lote 2300 Pérdida 1.1992 Precisión 0.4385\n",
            "Epoch 3 Lote 2350 Pérdida 1.1954 Precisión 0.4389\n",
            "Epoch 3 Lote 2400 Pérdida 1.1919 Precisión 0.4394\n",
            "Epoch 3 Lote 2450 Pérdida 1.1880 Precisión 0.4399\n",
            "Epoch 3 Lote 2500 Pérdida 1.1840 Precisión 0.4403\n",
            "Epoch 3 Lote 2550 Pérdida 1.1798 Precisión 0.4407\n",
            "Epoch 3 Lote 2600 Pérdida 1.1756 Precisión 0.4411\n",
            "Epoch 3 Lote 2650 Pérdida 1.1728 Precisión 0.4416\n",
            "Epoch 3 Lote 2700 Pérdida 1.1696 Precisión 0.4420\n",
            "Epoch 3 Lote 2750 Pérdida 1.1667 Precisión 0.4423\n",
            "Epoch 3 Lote 2800 Pérdida 1.1645 Precisión 0.4426\n",
            "Epoch 3 Lote 2850 Pérdida 1.1619 Precisión 0.4429\n",
            "Epoch 3 Lote 2900 Pérdida 1.1597 Precisión 0.4433\n",
            "Epoch 3 Lote 2950 Pérdida 1.1572 Precisión 0.4437\n",
            "Epoch 3 Lote 3000 Pérdida 1.1547 Precisión 0.4442\n",
            "Epoch 3 Lote 3050 Pérdida 1.1527 Precisión 0.4446\n",
            "Epoch 3 Lote 3100 Pérdida 1.1511 Precisión 0.4450\n",
            "Epoch 3 Lote 3150 Pérdida 1.1489 Precisión 0.4454\n",
            "Epoch 3 Lote 3200 Pérdida 1.1476 Precisión 0.4458\n",
            "Epoch 3 Lote 3250 Pérdida 1.1457 Precisión 0.4462\n",
            "Epoch 3 Lote 3300 Pérdida 1.1439 Precisión 0.4466\n",
            "Epoch 3 Lote 3350 Pérdida 1.1423 Precisión 0.4469\n",
            "Epoch 3 Lote 3400 Pérdida 1.1406 Precisión 0.4473\n",
            "Epoch 3 Lote 3450 Pérdida 1.1386 Precisión 0.4477\n",
            "Epoch 3 Lote 3500 Pérdida 1.1370 Precisión 0.4481\n",
            "Epoch 3 Lote 3550 Pérdida 1.1355 Precisión 0.4485\n",
            "Epoch 3 Lote 3600 Pérdida 1.1335 Precisión 0.4489\n",
            "Epoch 3 Lote 3650 Pérdida 1.1319 Precisión 0.4493\n",
            "Epoch 3 Lote 3700 Pérdida 1.1301 Precisión 0.4496\n",
            "Epoch 3 Lote 3750 Pérdida 1.1281 Precisión 0.4500\n",
            "Epoch 3 Lote 3800 Pérdida 1.1264 Precisión 0.4504\n",
            "Epoch 3 Lote 3850 Pérdida 1.1244 Precisión 0.4508\n",
            "Epoch 3 Lote 3900 Pérdida 1.1228 Precisión 0.4512\n",
            "Epoch 3 Lote 3950 Pérdida 1.1210 Precisión 0.4515\n",
            "Epoch 3 Lote 4000 Pérdida 1.1197 Precisión 0.4519\n",
            "Epoch 3 Lote 4050 Pérdida 1.1178 Precisión 0.4523\n",
            "Epoch 3 Lote 4100 Pérdida 1.1161 Precisión 0.4527\n",
            "Epoch 3 Lote 4150 Pérdida 1.1143 Precisión 0.4531\n",
            "Epoch 3 Lote 4200 Pérdida 1.1124 Precisión 0.4536\n",
            "Epoch 3 Lote 4250 Pérdida 1.1106 Precisión 0.4540\n",
            "Epoch 3 Lote 4300 Pérdida 1.1085 Precisión 0.4545\n",
            "Epoch 3 Lote 4350 Pérdida 1.1068 Precisión 0.4550\n",
            "Epoch 3 Lote 4400 Pérdida 1.1054 Precisión 0.4555\n",
            "Epoch 3 Lote 4450 Pérdida 1.1034 Precisión 0.4559\n",
            "Epoch 3 Lote 4500 Pérdida 1.1019 Precisión 0.4564\n",
            "Epoch 3 Lote 4550 Pérdida 1.1001 Precisión 0.4569\n",
            "Epoch 3 Lote 4600 Pérdida 1.0985 Precisión 0.4574\n",
            "Epoch 3 Lote 4650 Pérdida 1.0971 Precisión 0.4578\n",
            "Epoch 3 Lote 4700 Pérdida 1.0967 Precisión 0.4581\n",
            "Epoch 3 Lote 4750 Pérdida 1.0966 Precisión 0.4582\n",
            "Epoch 3 Lote 4800 Pérdida 1.0972 Precisión 0.4582\n",
            "Epoch 3 Lote 4850 Pérdida 1.0981 Precisión 0.4582\n",
            "Epoch 3 Lote 4900 Pérdida 1.0989 Precisión 0.4580\n",
            "Epoch 3 Lote 4950 Pérdida 1.1002 Precisión 0.4579\n",
            "Epoch 3 Lote 5000 Pérdida 1.1017 Precisión 0.4577\n",
            "Epoch 3 Lote 5050 Pérdida 1.1033 Precisión 0.4576\n",
            "Epoch 3 Lote 5100 Pérdida 1.1050 Precisión 0.4574\n",
            "Epoch 3 Lote 5150 Pérdida 1.1069 Precisión 0.4571\n",
            "Epoch 3 Lote 5200 Pérdida 1.1087 Precisión 0.4569\n",
            "Epoch 3 Lote 5250 Pérdida 1.1103 Precisión 0.4567\n",
            "Epoch 3 Lote 5300 Pérdida 1.1123 Precisión 0.4564\n",
            "Epoch 3 Lote 5350 Pérdida 1.1141 Precisión 0.4562\n",
            "Epoch 3 Lote 5400 Pérdida 1.1160 Precisión 0.4560\n",
            "Epoch 3 Lote 5450 Pérdida 1.1180 Precisión 0.4557\n",
            "Epoch 3 Lote 5500 Pérdida 1.1197 Precisión 0.4555\n",
            "Epoch 3 Lote 5550 Pérdida 1.1214 Precisión 0.4553\n",
            "Epoch 3 Lote 5600 Pérdida 1.1231 Precisión 0.4550\n",
            "Epoch 3 Lote 5650 Pérdida 1.1248 Precisión 0.4548\n",
            "Epoch 3 Lote 5700 Pérdida 1.1266 Precisión 0.4546\n",
            "Epoch 3 Lote 5750 Pérdida 1.1285 Precisión 0.4543\n",
            "Epoch 3 Lote 5800 Pérdida 1.1301 Precisión 0.4541\n",
            "Epoch 3 Lote 5850 Pérdida 1.1319 Precisión 0.4538\n",
            "Epoch 3 Lote 5900 Pérdida 1.1333 Precisión 0.4535\n",
            "Epoch 3 Lote 5950 Pérdida 1.1349 Precisión 0.4532\n",
            "Epoch 3 Lote 6000 Pérdida 1.1363 Precisión 0.4529\n",
            "Epoch 3 Lote 6050 Pérdida 1.1376 Precisión 0.4526\n",
            "Epoch 3 Lote 6100 Pérdida 1.1389 Precisión 0.4524\n",
            "Epoch 3 Lote 6150 Pérdida 1.1403 Precisión 0.4521\n",
            "Epoch 3 Lote 6200 Pérdida 1.1415 Precisión 0.4519\n",
            "Epoch 3 Lote 6250 Pérdida 1.1426 Precisión 0.4516\n",
            "Epoch 3 Lote 6300 Pérdida 1.1438 Precisión 0.4514\n",
            "Epoch 3 Lote 6350 Pérdida 1.1450 Precisión 0.4512\n",
            "Epoch 3 Lote 6400 Pérdida 1.1462 Precisión 0.4510\n",
            "Guardando checkpoint para el epoch 3 en ./drive/My Drive/Curso de NLP/Transformer/ckpt/ckpt-3\n",
            "Tiempo que ha tardado 1 epoch: 1573.3830065727234 segs\n",
            "\n",
            "Inicio del epoch 4\n",
            "Epoch 4 Lote 0 Pérdida 1.2992 Precisión 0.4334\n",
            "Epoch 4 Lote 50 Pérdida 1.2857 Precisión 0.4269\n",
            "Epoch 4 Lote 100 Pérdida 1.2828 Precisión 0.4251\n",
            "Epoch 4 Lote 150 Pérdida 1.2827 Precisión 0.4247\n",
            "Epoch 4 Lote 200 Pérdida 1.2789 Precisión 0.4258\n",
            "Epoch 4 Lote 250 Pérdida 1.2760 Precisión 0.4270\n",
            "Epoch 4 Lote 300 Pérdida 1.2746 Precisión 0.4271\n",
            "Epoch 4 Lote 350 Pérdida 1.2750 Precisión 0.4284\n",
            "Epoch 4 Lote 400 Pérdida 1.2732 Precisión 0.4289\n",
            "Epoch 4 Lote 450 Pérdida 1.2719 Precisión 0.4292\n",
            "Epoch 4 Lote 500 Pérdida 1.2710 Precisión 0.4295\n",
            "Epoch 4 Lote 550 Pérdida 1.2685 Precisión 0.4298\n",
            "Epoch 4 Lote 600 Pérdida 1.2669 Precisión 0.4297\n",
            "Epoch 4 Lote 650 Pérdida 1.2673 Precisión 0.4296\n",
            "Epoch 4 Lote 700 Pérdida 1.2642 Precisión 0.4301\n",
            "Epoch 4 Lote 750 Pérdida 1.2608 Precisión 0.4305\n",
            "Epoch 4 Lote 800 Pérdida 1.2562 Precisión 0.4312\n",
            "Epoch 4 Lote 850 Pérdida 1.2510 Precisión 0.4323\n",
            "Epoch 4 Lote 900 Pérdida 1.2446 Precisión 0.4335\n",
            "Epoch 4 Lote 950 Pérdida 1.2382 Precisión 0.4346\n",
            "Epoch 4 Lote 1000 Pérdida 1.2323 Precisión 0.4359\n",
            "Epoch 4 Lote 1050 Pérdida 1.2264 Precisión 0.4370\n",
            "Epoch 4 Lote 1100 Pérdida 1.2207 Precisión 0.4378\n",
            "Epoch 4 Lote 1150 Pérdida 1.2141 Precisión 0.4387\n",
            "Epoch 4 Lote 1200 Pérdida 1.2071 Precisión 0.4395\n",
            "Epoch 4 Lote 1250 Pérdida 1.2015 Precisión 0.4401\n",
            "Epoch 4 Lote 1300 Pérdida 1.1962 Precisión 0.4411\n",
            "Epoch 4 Lote 1350 Pérdida 1.1909 Precisión 0.4419\n",
            "Epoch 4 Lote 1400 Pérdida 1.1848 Precisión 0.4424\n",
            "Epoch 4 Lote 1450 Pérdida 1.1791 Precisión 0.4433\n",
            "Epoch 4 Lote 1500 Pérdida 1.1739 Precisión 0.4442\n",
            "Epoch 4 Lote 1550 Pérdida 1.1690 Precisión 0.4451\n",
            "Epoch 4 Lote 1600 Pérdida 1.1628 Precisión 0.4459\n",
            "Epoch 4 Lote 1650 Pérdida 1.1573 Precisión 0.4468\n",
            "Epoch 4 Lote 1700 Pérdida 1.1531 Precisión 0.4475\n",
            "Epoch 4 Lote 1750 Pérdida 1.1480 Precisión 0.4482\n",
            "Epoch 4 Lote 1800 Pérdida 1.1431 Precisión 0.4486\n",
            "Epoch 4 Lote 1850 Pérdida 1.1388 Precisión 0.4492\n",
            "Epoch 4 Lote 1900 Pérdida 1.1349 Precisión 0.4496\n",
            "Epoch 4 Lote 1950 Pérdida 1.1304 Precisión 0.4500\n",
            "Epoch 4 Lote 2000 Pérdida 1.1263 Precisión 0.4505\n",
            "Epoch 4 Lote 2050 Pérdida 1.1226 Precisión 0.4508\n",
            "Epoch 4 Lote 2100 Pérdida 1.1193 Precisión 0.4512\n",
            "Epoch 4 Lote 2150 Pérdida 1.1153 Precisión 0.4517\n",
            "Epoch 4 Lote 2200 Pérdida 1.1117 Precisión 0.4520\n",
            "Epoch 4 Lote 2250 Pérdida 1.1082 Precisión 0.4524\n",
            "Epoch 4 Lote 2300 Pérdida 1.1050 Precisión 0.4528\n",
            "Epoch 4 Lote 2350 Pérdida 1.1015 Precisión 0.4532\n",
            "Epoch 4 Lote 2400 Pérdida 1.0977 Precisión 0.4537\n",
            "Epoch 4 Lote 2450 Pérdida 1.0940 Precisión 0.4541\n",
            "Epoch 4 Lote 2500 Pérdida 1.0906 Precisión 0.4546\n",
            "Epoch 4 Lote 2550 Pérdida 1.0872 Precisión 0.4548\n",
            "Epoch 4 Lote 2600 Pérdida 1.0836 Precisión 0.4551\n",
            "Epoch 4 Lote 2650 Pérdida 1.0802 Precisión 0.4556\n",
            "Epoch 4 Lote 2700 Pérdida 1.0776 Precisión 0.4559\n",
            "Epoch 4 Lote 2750 Pérdida 1.0751 Precisión 0.4562\n",
            "Epoch 4 Lote 2800 Pérdida 1.0726 Precisión 0.4567\n",
            "Epoch 4 Lote 2850 Pérdida 1.0703 Precisión 0.4570\n",
            "Epoch 4 Lote 2900 Pérdida 1.0679 Precisión 0.4574\n",
            "Epoch 4 Lote 2950 Pérdida 1.0655 Precisión 0.4577\n",
            "Epoch 4 Lote 3000 Pérdida 1.0636 Precisión 0.4581\n",
            "Epoch 4 Lote 3050 Pérdida 1.0620 Precisión 0.4584\n",
            "Epoch 4 Lote 3100 Pérdida 1.0600 Precisión 0.4588\n",
            "Epoch 4 Lote 3150 Pérdida 1.0585 Precisión 0.4593\n",
            "Epoch 4 Lote 3200 Pérdida 1.0569 Precisión 0.4596\n",
            "Epoch 4 Lote 3250 Pérdida 1.0551 Precisión 0.4600\n",
            "Epoch 4 Lote 3300 Pérdida 1.0532 Precisión 0.4604\n",
            "Epoch 4 Lote 3350 Pérdida 1.0520 Precisión 0.4606\n",
            "Epoch 4 Lote 3400 Pérdida 1.0506 Precisión 0.4610\n",
            "Epoch 4 Lote 3450 Pérdida 1.0490 Precisión 0.4614\n",
            "Epoch 4 Lote 3500 Pérdida 1.0474 Precisión 0.4618\n",
            "Epoch 4 Lote 3550 Pérdida 1.0461 Precisión 0.4622\n",
            "Epoch 4 Lote 3600 Pérdida 1.0446 Precisión 0.4625\n",
            "Epoch 4 Lote 3650 Pérdida 1.0431 Precisión 0.4628\n",
            "Epoch 4 Lote 3700 Pérdida 1.0418 Precisión 0.4631\n",
            "Epoch 4 Lote 3750 Pérdida 1.0401 Precisión 0.4634\n",
            "Epoch 4 Lote 3800 Pérdida 1.0386 Precisión 0.4638\n",
            "Epoch 4 Lote 3850 Pérdida 1.0372 Precisión 0.4642\n",
            "Epoch 4 Lote 3900 Pérdida 1.0355 Precisión 0.4646\n",
            "Epoch 4 Lote 3950 Pérdida 1.0340 Precisión 0.4649\n",
            "Epoch 4 Lote 4000 Pérdida 1.0328 Precisión 0.4653\n",
            "Epoch 4 Lote 4050 Pérdida 1.0310 Precisión 0.4656\n",
            "Epoch 4 Lote 4100 Pérdida 1.0297 Precisión 0.4660\n",
            "Epoch 4 Lote 4150 Pérdida 1.0278 Precisión 0.4664\n",
            "Epoch 4 Lote 4200 Pérdida 1.0261 Precisión 0.4668\n",
            "Epoch 4 Lote 4250 Pérdida 1.0244 Precisión 0.4673\n",
            "Epoch 4 Lote 4300 Pérdida 1.0227 Precisión 0.4677\n",
            "Epoch 4 Lote 4350 Pérdida 1.0215 Precisión 0.4681\n",
            "Epoch 4 Lote 4400 Pérdida 1.0201 Precisión 0.4686\n",
            "Epoch 4 Lote 4450 Pérdida 1.0185 Precisión 0.4690\n",
            "Epoch 4 Lote 4500 Pérdida 1.0170 Precisión 0.4694\n",
            "Epoch 4 Lote 4550 Pérdida 1.0156 Precisión 0.4699\n",
            "Epoch 4 Lote 4600 Pérdida 1.0145 Precisión 0.4702\n",
            "Epoch 4 Lote 4650 Pérdida 1.0131 Precisión 0.4706\n",
            "Epoch 4 Lote 4700 Pérdida 1.0125 Precisión 0.4708\n",
            "Epoch 4 Lote 4750 Pérdida 1.0127 Precisión 0.4709\n",
            "Epoch 4 Lote 4800 Pérdida 1.0135 Precisión 0.4709\n",
            "Epoch 4 Lote 4850 Pérdida 1.0143 Precisión 0.4709\n",
            "Epoch 4 Lote 4900 Pérdida 1.0157 Precisión 0.4709\n",
            "Epoch 4 Lote 4950 Pérdida 1.0169 Precisión 0.4707\n",
            "Epoch 4 Lote 5000 Pérdida 1.0187 Precisión 0.4705\n",
            "Epoch 4 Lote 5050 Pérdida 1.0205 Precisión 0.4703\n",
            "Epoch 4 Lote 5100 Pérdida 1.0226 Precisión 0.4700\n",
            "Epoch 4 Lote 5150 Pérdida 1.0248 Precisión 0.4697\n",
            "Epoch 4 Lote 5200 Pérdida 1.0268 Precisión 0.4694\n",
            "Epoch 4 Lote 5250 Pérdida 1.0289 Precisión 0.4691\n",
            "Epoch 4 Lote 5300 Pérdida 1.0310 Precisión 0.4688\n",
            "Epoch 4 Lote 5350 Pérdida 1.0333 Precisión 0.4686\n",
            "Epoch 4 Lote 5400 Pérdida 1.0354 Precisión 0.4683\n",
            "Epoch 4 Lote 5450 Pérdida 1.0374 Precisión 0.4681\n",
            "Epoch 4 Lote 5500 Pérdida 1.0391 Precisión 0.4679\n",
            "Epoch 4 Lote 5550 Pérdida 1.0407 Precisión 0.4676\n",
            "Epoch 4 Lote 5600 Pérdida 1.0424 Precisión 0.4674\n",
            "Epoch 4 Lote 5650 Pérdida 1.0444 Precisión 0.4671\n",
            "Epoch 4 Lote 5700 Pérdida 1.0461 Precisión 0.4668\n",
            "Epoch 4 Lote 5750 Pérdida 1.0480 Precisión 0.4666\n",
            "Epoch 4 Lote 5800 Pérdida 1.0499 Precisión 0.4663\n",
            "Epoch 4 Lote 5850 Pérdida 1.0518 Precisión 0.4660\n",
            "Epoch 4 Lote 5900 Pérdida 1.0536 Precisión 0.4657\n",
            "Epoch 4 Lote 5950 Pérdida 1.0550 Precisión 0.4654\n",
            "Epoch 4 Lote 6000 Pérdida 1.0567 Precisión 0.4651\n",
            "Epoch 4 Lote 6050 Pérdida 1.0584 Precisión 0.4648\n",
            "Epoch 4 Lote 6100 Pérdida 1.0598 Precisión 0.4645\n",
            "Epoch 4 Lote 6150 Pérdida 1.0614 Precisión 0.4642\n",
            "Epoch 4 Lote 6200 Pérdida 1.0628 Precisión 0.4639\n",
            "Epoch 4 Lote 6250 Pérdida 1.0644 Precisión 0.4637\n",
            "Epoch 4 Lote 6300 Pérdida 1.0659 Precisión 0.4634\n",
            "Epoch 4 Lote 6350 Pérdida 1.0671 Precisión 0.4632\n",
            "Epoch 4 Lote 6400 Pérdida 1.0685 Precisión 0.4629\n",
            "Guardando checkpoint para el epoch 4 en ./drive/My Drive/Curso de NLP/Transformer/ckpt/ckpt-4\n",
            "Tiempo que ha tardado 1 epoch: 1561.301760673523 segs\n",
            "\n",
            "Inicio del epoch 5\n",
            "Epoch 5 Lote 0 Pérdida 1.3921 Precisión 0.3849\n",
            "Epoch 5 Lote 50 Pérdida 1.2153 Precisión 0.4345\n",
            "Epoch 5 Lote 100 Pérdida 1.2133 Precisión 0.4358\n",
            "Epoch 5 Lote 150 Pérdida 1.2188 Precisión 0.4357\n",
            "Epoch 5 Lote 200 Pérdida 1.2193 Precisión 0.4360\n",
            "Epoch 5 Lote 250 Pérdida 1.2244 Precisión 0.4362\n",
            "Epoch 5 Lote 300 Pérdida 1.2246 Precisión 0.4374\n",
            "Epoch 5 Lote 350 Pérdida 1.2235 Precisión 0.4379\n",
            "Epoch 5 Lote 400 Pérdida 1.2197 Precisión 0.4379\n",
            "Epoch 5 Lote 450 Pérdida 1.2138 Precisión 0.4383\n",
            "Epoch 5 Lote 500 Pérdida 1.2117 Precisión 0.4384\n",
            "Epoch 5 Lote 550 Pérdida 1.2102 Precisión 0.4383\n",
            "Epoch 5 Lote 600 Pérdida 1.2099 Precisión 0.4384\n",
            "Epoch 5 Lote 650 Pérdida 1.2078 Precisión 0.4381\n",
            "Epoch 5 Lote 700 Pérdida 1.2057 Precisión 0.4381\n",
            "Epoch 5 Lote 750 Pérdida 1.2026 Precisión 0.4389\n",
            "Epoch 5 Lote 800 Pérdida 1.2001 Precisión 0.4398\n",
            "Epoch 5 Lote 850 Pérdida 1.1947 Precisión 0.4408\n",
            "Epoch 5 Lote 900 Pérdida 1.1886 Precisión 0.4420\n",
            "Epoch 5 Lote 950 Pérdida 1.1802 Precisión 0.4432\n",
            "Epoch 5 Lote 1000 Pérdida 1.1740 Precisión 0.4442\n",
            "Epoch 5 Lote 1050 Pérdida 1.1683 Precisión 0.4452\n",
            "Epoch 5 Lote 1100 Pérdida 1.1617 Precisión 0.4459\n",
            "Epoch 5 Lote 1150 Pérdida 1.1560 Precisión 0.4467\n",
            "Epoch 5 Lote 1200 Pérdida 1.1507 Precisión 0.4477\n",
            "Epoch 5 Lote 1250 Pérdida 1.1448 Precisión 0.4489\n",
            "Epoch 5 Lote 1300 Pérdida 1.1381 Precisión 0.4497\n",
            "Epoch 5 Lote 1350 Pérdida 1.1323 Precisión 0.4505\n",
            "Epoch 5 Lote 1400 Pérdida 1.1260 Precisión 0.4514\n",
            "Epoch 5 Lote 1450 Pérdida 1.1210 Precisión 0.4522\n",
            "Epoch 5 Lote 1500 Pérdida 1.1168 Precisión 0.4531\n",
            "Epoch 5 Lote 1550 Pérdida 1.1125 Precisión 0.4537\n",
            "Epoch 5 Lote 1600 Pérdida 1.1077 Precisión 0.4545\n",
            "Epoch 5 Lote 1650 Pérdida 1.1025 Precisión 0.4552\n",
            "Epoch 5 Lote 1700 Pérdida 1.0975 Precisión 0.4560\n",
            "Epoch 5 Lote 1750 Pérdida 1.0931 Precisión 0.4565\n",
            "Epoch 5 Lote 1800 Pérdida 1.0886 Precisión 0.4570\n",
            "Epoch 5 Lote 1850 Pérdida 1.0842 Precisión 0.4574\n",
            "Epoch 5 Lote 1900 Pérdida 1.0801 Precisión 0.4579\n",
            "Epoch 5 Lote 1950 Pérdida 1.0767 Precisión 0.4583\n",
            "Epoch 5 Lote 2000 Pérdida 1.0729 Precisión 0.4586\n",
            "Epoch 5 Lote 2050 Pérdida 1.0687 Precisión 0.4591\n",
            "Epoch 5 Lote 2100 Pérdida 1.0650 Precisión 0.4596\n",
            "Epoch 5 Lote 2150 Pérdida 1.0609 Precisión 0.4598\n",
            "Epoch 5 Lote 2200 Pérdida 1.0571 Precisión 0.4602\n",
            "Epoch 5 Lote 2250 Pérdida 1.0537 Precisión 0.4607\n",
            "Epoch 5 Lote 2300 Pérdida 1.0508 Precisión 0.4611\n",
            "Epoch 5 Lote 2350 Pérdida 1.0479 Precisión 0.4615\n",
            "Epoch 5 Lote 2400 Pérdida 1.0443 Precisión 0.4620\n",
            "Epoch 5 Lote 2450 Pérdida 1.0409 Precisión 0.4623\n",
            "Epoch 5 Lote 2500 Pérdida 1.0370 Precisión 0.4626\n",
            "Epoch 5 Lote 2550 Pérdida 1.0334 Precisión 0.4631\n",
            "Epoch 5 Lote 2600 Pérdida 1.0301 Precisión 0.4634\n",
            "Epoch 5 Lote 2650 Pérdida 1.0271 Precisión 0.4638\n",
            "Epoch 5 Lote 2700 Pérdida 1.0240 Precisión 0.4642\n",
            "Epoch 5 Lote 2750 Pérdida 1.0218 Precisión 0.4646\n",
            "Epoch 5 Lote 2800 Pérdida 1.0193 Precisión 0.4649\n",
            "Epoch 5 Lote 2850 Pérdida 1.0173 Precisión 0.4653\n",
            "Epoch 5 Lote 2900 Pérdida 1.0153 Precisión 0.4656\n",
            "Epoch 5 Lote 2950 Pérdida 1.0130 Precisión 0.4660\n",
            "Epoch 5 Lote 3000 Pérdida 1.0113 Precisión 0.4663\n",
            "Epoch 5 Lote 3050 Pérdida 1.0092 Precisión 0.4667\n",
            "Epoch 5 Lote 3100 Pérdida 1.0076 Precisión 0.4669\n",
            "Epoch 5 Lote 3150 Pérdida 1.0058 Precisión 0.4673\n",
            "Epoch 5 Lote 3200 Pérdida 1.0040 Precisión 0.4677\n",
            "Epoch 5 Lote 3250 Pérdida 1.0027 Precisión 0.4680\n",
            "Epoch 5 Lote 3300 Pérdida 1.0008 Precisión 0.4684\n",
            "Epoch 5 Lote 3350 Pérdida 0.9994 Precisión 0.4687\n",
            "Epoch 5 Lote 3400 Pérdida 0.9983 Precisión 0.4690\n",
            "Epoch 5 Lote 3450 Pérdida 0.9969 Precisión 0.4694\n",
            "Epoch 5 Lote 3500 Pérdida 0.9955 Precisión 0.4698\n",
            "Epoch 5 Lote 3550 Pérdida 0.9942 Precisión 0.4701\n",
            "Epoch 5 Lote 3600 Pérdida 0.9931 Precisión 0.4705\n",
            "Epoch 5 Lote 3650 Pérdida 0.9915 Precisión 0.4708\n",
            "Epoch 5 Lote 3700 Pérdida 0.9902 Precisión 0.4712\n",
            "Epoch 5 Lote 3750 Pérdida 0.9888 Precisión 0.4716\n",
            "Epoch 5 Lote 3800 Pérdida 0.9870 Precisión 0.4720\n",
            "Epoch 5 Lote 3850 Pérdida 0.9856 Precisión 0.4723\n",
            "Epoch 5 Lote 3900 Pérdida 0.9841 Precisión 0.4726\n",
            "Epoch 5 Lote 3950 Pérdida 0.9827 Precisión 0.4730\n",
            "Epoch 5 Lote 4000 Pérdida 0.9816 Precisión 0.4734\n",
            "Epoch 5 Lote 4050 Pérdida 0.9800 Precisión 0.4737\n",
            "Epoch 5 Lote 4100 Pérdida 0.9785 Precisión 0.4741\n",
            "Epoch 5 Lote 4150 Pérdida 0.9771 Precisión 0.4745\n",
            "Epoch 5 Lote 4200 Pérdida 0.9757 Precisión 0.4748\n",
            "Epoch 5 Lote 4250 Pérdida 0.9741 Precisión 0.4752\n",
            "Epoch 5 Lote 4300 Pérdida 0.9728 Precisión 0.4756\n",
            "Epoch 5 Lote 4350 Pérdida 0.9713 Precisión 0.4761\n",
            "Epoch 5 Lote 4400 Pérdida 0.9700 Precisión 0.4765\n",
            "Epoch 5 Lote 4450 Pérdida 0.9685 Precisión 0.4769\n",
            "Epoch 5 Lote 4500 Pérdida 0.9670 Precisión 0.4772\n",
            "Epoch 5 Lote 4550 Pérdida 0.9656 Precisión 0.4776\n",
            "Epoch 5 Lote 4600 Pérdida 0.9644 Precisión 0.4780\n",
            "Epoch 5 Lote 4650 Pérdida 0.9633 Precisión 0.4783\n",
            "Epoch 5 Lote 4700 Pérdida 0.9628 Precisión 0.4785\n",
            "Epoch 5 Lote 4750 Pérdida 0.9632 Precisión 0.4786\n",
            "Epoch 5 Lote 4800 Pérdida 0.9638 Precisión 0.4787\n",
            "Epoch 5 Lote 4850 Pérdida 0.9648 Precisión 0.4787\n",
            "Epoch 5 Lote 4900 Pérdida 0.9664 Precisión 0.4785\n",
            "Epoch 5 Lote 4950 Pérdida 0.9682 Precisión 0.4784\n",
            "Epoch 5 Lote 5000 Pérdida 0.9699 Precisión 0.4782\n",
            "Epoch 5 Lote 5050 Pérdida 0.9717 Precisión 0.4779\n",
            "Epoch 5 Lote 5100 Pérdida 0.9736 Precisión 0.4777\n",
            "Epoch 5 Lote 5150 Pérdida 0.9758 Precisión 0.4774\n",
            "Epoch 5 Lote 5200 Pérdida 0.9780 Precisión 0.4771\n",
            "Epoch 5 Lote 5250 Pérdida 0.9801 Precisión 0.4768\n",
            "Epoch 5 Lote 5300 Pérdida 0.9822 Precisión 0.4765\n",
            "Epoch 5 Lote 5350 Pérdida 0.9842 Precisión 0.4763\n",
            "Epoch 5 Lote 5400 Pérdida 0.9861 Precisión 0.4760\n",
            "Epoch 5 Lote 5450 Pérdida 0.9881 Precisión 0.4757\n",
            "Epoch 5 Lote 5500 Pérdida 0.9900 Precisión 0.4755\n",
            "Epoch 5 Lote 5550 Pérdida 0.9919 Precisión 0.4753\n",
            "Epoch 5 Lote 5600 Pérdida 0.9937 Precisión 0.4750\n",
            "Epoch 5 Lote 5650 Pérdida 0.9956 Precisión 0.4747\n",
            "Epoch 5 Lote 5700 Pérdida 0.9975 Precisión 0.4745\n",
            "Epoch 5 Lote 5750 Pérdida 0.9996 Precisión 0.4742\n",
            "Epoch 5 Lote 5800 Pérdida 1.0013 Precisión 0.4739\n",
            "Epoch 5 Lote 5850 Pérdida 1.0035 Precisión 0.4736\n",
            "Epoch 5 Lote 5900 Pérdida 1.0053 Precisión 0.4732\n",
            "Epoch 5 Lote 5950 Pérdida 1.0074 Precisión 0.4729\n",
            "Epoch 5 Lote 6000 Pérdida 1.0092 Precisión 0.4726\n",
            "Epoch 5 Lote 6050 Pérdida 1.0108 Precisión 0.4723\n",
            "Epoch 5 Lote 6100 Pérdida 1.0122 Precisión 0.4720\n",
            "Epoch 5 Lote 6150 Pérdida 1.0137 Precisión 0.4717\n",
            "Epoch 5 Lote 6200 Pérdida 1.0151 Precisión 0.4714\n",
            "Epoch 5 Lote 6250 Pérdida 1.0166 Precisión 0.4711\n",
            "Epoch 5 Lote 6300 Pérdida 1.0180 Precisión 0.4709\n",
            "Epoch 5 Lote 6350 Pérdida 1.0193 Precisión 0.4706\n",
            "Epoch 5 Lote 6400 Pérdida 1.0207 Precisión 0.4703\n",
            "Guardando checkpoint para el epoch 5 en ./drive/My Drive/Curso de NLP/Transformer/ckpt/ckpt-5\n",
            "Tiempo que ha tardado 1 epoch: 1557.1833729743958 segs\n",
            "\n",
            "Inicio del epoch 6\n",
            "Epoch 6 Lote 0 Pérdida 1.2198 Precisión 0.4441\n",
            "Epoch 6 Lote 50 Pérdida 1.2064 Precisión 0.4410\n",
            "Epoch 6 Lote 100 Pérdida 1.1974 Precisión 0.4463\n",
            "Epoch 6 Lote 150 Pérdida 1.1932 Precisión 0.4446\n",
            "Epoch 6 Lote 200 Pérdida 1.1833 Precisión 0.4430\n",
            "Epoch 6 Lote 250 Pérdida 1.1782 Precisión 0.4436\n",
            "Epoch 6 Lote 300 Pérdida 1.1809 Precisión 0.4436\n",
            "Epoch 6 Lote 350 Pérdida 1.1797 Precisión 0.4439\n",
            "Epoch 6 Lote 400 Pérdida 1.1765 Precisión 0.4441\n",
            "Epoch 6 Lote 450 Pérdida 1.1763 Precisión 0.4442\n",
            "Epoch 6 Lote 500 Pérdida 1.1748 Precisión 0.4446\n",
            "Epoch 6 Lote 550 Pérdida 1.1722 Precisión 0.4446\n",
            "Epoch 6 Lote 600 Pérdida 1.1711 Precisión 0.4443\n",
            "Epoch 6 Lote 650 Pérdida 1.1719 Precisión 0.4441\n",
            "Epoch 6 Lote 700 Pérdida 1.1692 Precisión 0.4448\n",
            "Epoch 6 Lote 750 Pérdida 1.1666 Precisión 0.4453\n",
            "Epoch 6 Lote 800 Pérdida 1.1627 Precisión 0.4460\n",
            "Epoch 6 Lote 850 Pérdida 1.1571 Precisión 0.4472\n",
            "Epoch 6 Lote 900 Pérdida 1.1518 Precisión 0.4484\n",
            "Epoch 6 Lote 950 Pérdida 1.1458 Precisión 0.4495\n",
            "Epoch 6 Lote 1000 Pérdida 1.1386 Precisión 0.4506\n",
            "Epoch 6 Lote 1050 Pérdida 1.1320 Precisión 0.4516\n",
            "Epoch 6 Lote 1100 Pérdida 1.1250 Precisión 0.4524\n",
            "Epoch 6 Lote 1150 Pérdida 1.1184 Precisión 0.4533\n",
            "Epoch 6 Lote 1200 Pérdida 1.1126 Precisión 0.4541\n",
            "Epoch 6 Lote 1250 Pérdida 1.1076 Precisión 0.4550\n",
            "Epoch 6 Lote 1300 Pérdida 1.1017 Precisión 0.4560\n",
            "Epoch 6 Lote 1350 Pérdida 1.0959 Precisión 0.4566\n",
            "Epoch 6 Lote 1400 Pérdida 1.0899 Precisión 0.4574\n",
            "Epoch 6 Lote 1450 Pérdida 1.0852 Precisión 0.4583\n",
            "Epoch 6 Lote 1500 Pérdida 1.0797 Precisión 0.4590\n",
            "Epoch 6 Lote 1550 Pérdida 1.0746 Precisión 0.4599\n",
            "Epoch 6 Lote 1600 Pérdida 1.0699 Precisión 0.4606\n",
            "Epoch 6 Lote 1650 Pérdida 1.0651 Precisión 0.4613\n",
            "Epoch 6 Lote 1700 Pérdida 1.0602 Precisión 0.4620\n",
            "Epoch 6 Lote 1750 Pérdida 1.0556 Precisión 0.4625\n",
            "Epoch 6 Lote 1800 Pérdida 1.0512 Precisión 0.4632\n",
            "Epoch 6 Lote 1850 Pérdida 1.0470 Precisión 0.4637\n",
            "Epoch 6 Lote 1900 Pérdida 1.0430 Precisión 0.4642\n",
            "Epoch 6 Lote 1950 Pérdida 1.0394 Precisión 0.4646\n",
            "Epoch 6 Lote 2000 Pérdida 1.0354 Precisión 0.4649\n",
            "Epoch 6 Lote 2050 Pérdida 1.0317 Precisión 0.4652\n",
            "Epoch 6 Lote 2100 Pérdida 1.0279 Precisión 0.4655\n",
            "Epoch 6 Lote 2150 Pérdida 1.0245 Precisión 0.4659\n",
            "Epoch 6 Lote 2200 Pérdida 1.0212 Precisión 0.4662\n",
            "Epoch 6 Lote 2250 Pérdida 1.0178 Precisión 0.4666\n",
            "Epoch 6 Lote 2300 Pérdida 1.0144 Precisión 0.4669\n",
            "Epoch 6 Lote 2350 Pérdida 1.0112 Precisión 0.4673\n",
            "Epoch 6 Lote 2400 Pérdida 1.0075 Precisión 0.4678\n",
            "Epoch 6 Lote 2450 Pérdida 1.0041 Precisión 0.4682\n",
            "Epoch 6 Lote 2500 Pérdida 1.0006 Precisión 0.4686\n",
            "Epoch 6 Lote 2550 Pérdida 0.9971 Precisión 0.4690\n",
            "Epoch 6 Lote 2600 Pérdida 0.9932 Precisión 0.4694\n",
            "Epoch 6 Lote 2650 Pérdida 0.9905 Precisión 0.4697\n",
            "Epoch 6 Lote 2700 Pérdida 0.9878 Precisión 0.4700\n",
            "Epoch 6 Lote 2750 Pérdida 0.9853 Precisión 0.4704\n",
            "Epoch 6 Lote 2800 Pérdida 0.9832 Precisión 0.4708\n",
            "Epoch 6 Lote 2850 Pérdida 0.9815 Precisión 0.4711\n",
            "Epoch 6 Lote 2900 Pérdida 0.9794 Precisión 0.4714\n",
            "Epoch 6 Lote 2950 Pérdida 0.9772 Precisión 0.4718\n",
            "Epoch 6 Lote 3000 Pérdida 0.9751 Precisión 0.4721\n",
            "Epoch 6 Lote 3050 Pérdida 0.9735 Precisión 0.4724\n",
            "Epoch 6 Lote 3100 Pérdida 0.9717 Precisión 0.4728\n",
            "Epoch 6 Lote 3150 Pérdida 0.9701 Precisión 0.4731\n",
            "Epoch 6 Lote 3200 Pérdida 0.9683 Precisión 0.4735\n",
            "Epoch 6 Lote 3250 Pérdida 0.9671 Precisión 0.4738\n",
            "Epoch 6 Lote 3300 Pérdida 0.9657 Precisión 0.4743\n",
            "Epoch 6 Lote 3350 Pérdida 0.9645 Precisión 0.4745\n",
            "Epoch 6 Lote 3400 Pérdida 0.9636 Precisión 0.4749\n",
            "Epoch 6 Lote 3450 Pérdida 0.9625 Precisión 0.4751\n",
            "Epoch 6 Lote 3500 Pérdida 0.9609 Precisión 0.4755\n",
            "Epoch 6 Lote 3550 Pérdida 0.9597 Precisión 0.4759\n",
            "Epoch 6 Lote 3600 Pérdida 0.9583 Precisión 0.4762\n",
            "Epoch 6 Lote 3650 Pérdida 0.9568 Precisión 0.4765\n",
            "Epoch 6 Lote 3700 Pérdida 0.9553 Precisión 0.4768\n",
            "Epoch 6 Lote 3750 Pérdida 0.9541 Precisión 0.4771\n",
            "Epoch 6 Lote 3800 Pérdida 0.9526 Precisión 0.4774\n",
            "Epoch 6 Lote 3850 Pérdida 0.9515 Precisión 0.4777\n",
            "Epoch 6 Lote 3900 Pérdida 0.9500 Precisión 0.4781\n",
            "Epoch 6 Lote 3950 Pérdida 0.9489 Precisión 0.4784\n",
            "Epoch 6 Lote 4000 Pérdida 0.9472 Precisión 0.4788\n",
            "Epoch 6 Lote 4050 Pérdida 0.9460 Precisión 0.4791\n",
            "Epoch 6 Lote 4100 Pérdida 0.9443 Precisión 0.4794\n",
            "Epoch 6 Lote 4150 Pérdida 0.9432 Precisión 0.4799\n",
            "Epoch 6 Lote 4200 Pérdida 0.9416 Precisión 0.4803\n",
            "Epoch 6 Lote 4250 Pérdida 0.9401 Precisión 0.4806\n",
            "Epoch 6 Lote 4300 Pérdida 0.9387 Precisión 0.4811\n",
            "Epoch 6 Lote 4350 Pérdida 0.9375 Precisión 0.4815\n",
            "Epoch 6 Lote 4400 Pérdida 0.9362 Precisión 0.4819\n",
            "Epoch 6 Lote 4450 Pérdida 0.9350 Precisión 0.4823\n",
            "Epoch 6 Lote 4500 Pérdida 0.9336 Precisión 0.4827\n",
            "Epoch 6 Lote 4550 Pérdida 0.9323 Precisión 0.4831\n",
            "Epoch 6 Lote 4600 Pérdida 0.9312 Precisión 0.4835\n",
            "Epoch 6 Lote 4650 Pérdida 0.9302 Precisión 0.4838\n",
            "Epoch 6 Lote 4700 Pérdida 0.9297 Precisión 0.4840\n",
            "Epoch 6 Lote 4750 Pérdida 0.9300 Precisión 0.4840\n",
            "Epoch 6 Lote 4800 Pérdida 0.9308 Precisión 0.4841\n",
            "Epoch 6 Lote 4850 Pérdida 0.9320 Precisión 0.4840\n",
            "Epoch 6 Lote 4900 Pérdida 0.9333 Precisión 0.4839\n",
            "Epoch 6 Lote 4950 Pérdida 0.9349 Precisión 0.4837\n",
            "Epoch 6 Lote 5000 Pérdida 0.9365 Precisión 0.4835\n",
            "Epoch 6 Lote 5050 Pérdida 0.9385 Precisión 0.4832\n",
            "Epoch 6 Lote 5100 Pérdida 0.9406 Precisión 0.4830\n",
            "Epoch 6 Lote 5150 Pérdida 0.9428 Precisión 0.4828\n",
            "Epoch 6 Lote 5200 Pérdida 0.9449 Precisión 0.4825\n",
            "Epoch 6 Lote 5250 Pérdida 0.9471 Precisión 0.4822\n",
            "Epoch 6 Lote 5300 Pérdida 0.9494 Precisión 0.4819\n",
            "Epoch 6 Lote 5350 Pérdida 0.9515 Precisión 0.4816\n",
            "Epoch 6 Lote 5400 Pérdida 0.9535 Precisión 0.4814\n",
            "Epoch 6 Lote 5450 Pérdida 0.9556 Precisión 0.4811\n",
            "Epoch 6 Lote 5500 Pérdida 0.9574 Precisión 0.4808\n",
            "Epoch 6 Lote 5550 Pérdida 0.9594 Precisión 0.4806\n",
            "Epoch 6 Lote 5600 Pérdida 0.9613 Precisión 0.4803\n",
            "Epoch 6 Lote 5650 Pérdida 0.9631 Precisión 0.4800\n",
            "Epoch 6 Lote 5700 Pérdida 0.9651 Precisión 0.4797\n",
            "Epoch 6 Lote 5750 Pérdida 0.9670 Precisión 0.4794\n",
            "Epoch 6 Lote 5800 Pérdida 0.9690 Precisión 0.4791\n",
            "Epoch 6 Lote 5850 Pérdida 0.9709 Precisión 0.4788\n",
            "Epoch 6 Lote 5900 Pérdida 0.9728 Precisión 0.4785\n",
            "Epoch 6 Lote 5950 Pérdida 0.9748 Precisión 0.4781\n",
            "Epoch 6 Lote 6000 Pérdida 0.9764 Precisión 0.4778\n",
            "Epoch 6 Lote 6050 Pérdida 0.9782 Precisión 0.4774\n",
            "Epoch 6 Lote 6100 Pérdida 0.9799 Precisión 0.4772\n",
            "Epoch 6 Lote 6150 Pérdida 0.9812 Precisión 0.4769\n",
            "Epoch 6 Lote 6200 Pérdida 0.9828 Precisión 0.4766\n",
            "Epoch 6 Lote 6250 Pérdida 0.9841 Precisión 0.4763\n",
            "Epoch 6 Lote 6300 Pérdida 0.9857 Precisión 0.4760\n",
            "Epoch 6 Lote 6350 Pérdida 0.9872 Precisión 0.4758\n",
            "Epoch 6 Lote 6400 Pérdida 0.9885 Precisión 0.4755\n",
            "Guardando checkpoint para el epoch 6 en ./drive/My Drive/Curso de NLP/Transformer/ckpt/ckpt-6\n",
            "Tiempo que ha tardado 1 epoch: 1584.3481879234314 segs\n",
            "\n",
            "Inicio del epoch 7\n",
            "Epoch 7 Lote 0 Pérdida 1.0520 Precisión 0.4523\n",
            "Epoch 7 Lote 50 Pérdida 1.1865 Precisión 0.4419\n",
            "Epoch 7 Lote 100 Pérdida 1.1674 Precisión 0.4457\n",
            "Epoch 7 Lote 150 Pérdida 1.1529 Precisión 0.4477\n",
            "Epoch 7 Lote 200 Pérdida 1.1563 Precisión 0.4478\n",
            "Epoch 7 Lote 250 Pérdida 1.1578 Precisión 0.4473\n",
            "Epoch 7 Lote 300 Pérdida 1.1557 Precisión 0.4485\n",
            "Epoch 7 Lote 350 Pérdida 1.1567 Precisión 0.4490\n",
            "Epoch 7 Lote 400 Pérdida 1.1558 Precisión 0.4492\n",
            "Epoch 7 Lote 450 Pérdida 1.1511 Precisión 0.4489\n",
            "Epoch 7 Lote 500 Pérdida 1.1495 Precisión 0.4488\n",
            "Epoch 7 Lote 550 Pérdida 1.1458 Precisión 0.4486\n",
            "Epoch 7 Lote 600 Pérdida 1.1415 Precisión 0.4485\n",
            "Epoch 7 Lote 650 Pérdida 1.1426 Precisión 0.4487\n",
            "Epoch 7 Lote 700 Pérdida 1.1411 Precisión 0.4486\n",
            "Epoch 7 Lote 750 Pérdida 1.1376 Precisión 0.4493\n",
            "Epoch 7 Lote 800 Pérdida 1.1338 Precisión 0.4504\n",
            "Epoch 7 Lote 850 Pérdida 1.1278 Precisión 0.4515\n",
            "Epoch 7 Lote 900 Pérdida 1.1212 Precisión 0.4527\n",
            "Epoch 7 Lote 950 Pérdida 1.1146 Precisión 0.4541\n",
            "Epoch 7 Lote 1000 Pérdida 1.1096 Precisión 0.4551\n",
            "Epoch 7 Lote 1050 Pérdida 1.1040 Precisión 0.4559\n",
            "Epoch 7 Lote 1100 Pérdida 1.0975 Precisión 0.4567\n",
            "Epoch 7 Lote 1150 Pérdida 1.0910 Precisión 0.4576\n",
            "Epoch 7 Lote 1200 Pérdida 1.0850 Precisión 0.4585\n",
            "Epoch 7 Lote 1250 Pérdida 1.0795 Precisión 0.4592\n",
            "Epoch 7 Lote 1300 Pérdida 1.0741 Precisión 0.4601\n",
            "Epoch 7 Lote 1350 Pérdida 1.0682 Precisión 0.4610\n",
            "Epoch 7 Lote 1400 Pérdida 1.0626 Precisión 0.4618\n",
            "Epoch 7 Lote 1450 Pérdida 1.0567 Precisión 0.4626\n",
            "Epoch 7 Lote 1500 Pérdida 1.0521 Precisión 0.4634\n",
            "Epoch 7 Lote 1550 Pérdida 1.0470 Precisión 0.4642\n",
            "Epoch 7 Lote 1600 Pérdida 1.0425 Precisión 0.4648\n",
            "Epoch 7 Lote 1650 Pérdida 1.0381 Precisión 0.4656\n",
            "Epoch 7 Lote 1700 Pérdida 1.0335 Precisión 0.4664\n",
            "Epoch 7 Lote 1750 Pérdida 1.0293 Precisión 0.4671\n",
            "Epoch 7 Lote 1800 Pérdida 1.0251 Precisión 0.4675\n",
            "Epoch 7 Lote 1850 Pérdida 1.0209 Precisión 0.4679\n",
            "Epoch 7 Lote 1900 Pérdida 1.0165 Precisión 0.4683\n",
            "Epoch 7 Lote 1950 Pérdida 1.0126 Precisión 0.4687\n",
            "Epoch 7 Lote 2000 Pérdida 1.0091 Precisión 0.4689\n",
            "Epoch 7 Lote 2050 Pérdida 1.0054 Precisión 0.4694\n",
            "Epoch 7 Lote 2100 Pérdida 1.0018 Precisión 0.4698\n",
            "Epoch 7 Lote 2150 Pérdida 0.9980 Precisión 0.4701\n",
            "Epoch 7 Lote 2200 Pérdida 0.9947 Precisión 0.4704\n",
            "Epoch 7 Lote 2250 Pérdida 0.9913 Precisión 0.4708\n",
            "Epoch 7 Lote 2300 Pérdida 0.9878 Precisión 0.4713\n",
            "Epoch 7 Lote 2350 Pérdida 0.9840 Precisión 0.4716\n",
            "Epoch 7 Lote 2400 Pérdida 0.9805 Precisión 0.4721\n",
            "Epoch 7 Lote 2450 Pérdida 0.9769 Precisión 0.4724\n",
            "Epoch 7 Lote 2500 Pérdida 0.9734 Precisión 0.4728\n",
            "Epoch 7 Lote 2550 Pérdida 0.9698 Precisión 0.4731\n",
            "Epoch 7 Lote 2600 Pérdida 0.9666 Precisión 0.4735\n",
            "Epoch 7 Lote 2650 Pérdida 0.9636 Precisión 0.4737\n",
            "Epoch 7 Lote 2700 Pérdida 0.9616 Precisión 0.4742\n",
            "Epoch 7 Lote 2750 Pérdida 0.9592 Precisión 0.4747\n",
            "Epoch 7 Lote 2800 Pérdida 0.9569 Precisión 0.4749\n",
            "Epoch 7 Lote 2850 Pérdida 0.9548 Precisión 0.4752\n",
            "Epoch 7 Lote 2900 Pérdida 0.9526 Precisión 0.4755\n",
            "Epoch 7 Lote 2950 Pérdida 0.9509 Precisión 0.4758\n",
            "Epoch 7 Lote 3000 Pérdida 0.9493 Precisión 0.4762\n",
            "Epoch 7 Lote 3050 Pérdida 0.9475 Precisión 0.4766\n",
            "Epoch 7 Lote 3100 Pérdida 0.9457 Precisión 0.4770\n",
            "Epoch 7 Lote 3150 Pérdida 0.9441 Precisión 0.4773\n",
            "Epoch 7 Lote 3200 Pérdida 0.9427 Precisión 0.4777\n",
            "Epoch 7 Lote 3250 Pérdida 0.9412 Precisión 0.4781\n",
            "Epoch 7 Lote 3300 Pérdida 0.9397 Precisión 0.4783\n",
            "Epoch 7 Lote 3350 Pérdida 0.9384 Precisión 0.4787\n",
            "Epoch 7 Lote 3400 Pérdida 0.9371 Precisión 0.4791\n",
            "Epoch 7 Lote 3450 Pérdida 0.9358 Precisión 0.4794\n",
            "Epoch 7 Lote 3500 Pérdida 0.9346 Precisión 0.4797\n",
            "Epoch 7 Lote 3550 Pérdida 0.9333 Precisión 0.4801\n",
            "Epoch 7 Lote 3600 Pérdida 0.9320 Precisión 0.4804\n",
            "Epoch 7 Lote 3650 Pérdida 0.9306 Precisión 0.4807\n",
            "Epoch 7 Lote 3700 Pérdida 0.9290 Precisión 0.4810\n",
            "Epoch 7 Lote 3750 Pérdida 0.9277 Precisión 0.4813\n",
            "Epoch 7 Lote 3800 Pérdida 0.9263 Precisión 0.4816\n",
            "Epoch 7 Lote 3850 Pérdida 0.9251 Precisión 0.4818\n",
            "Epoch 7 Lote 3900 Pérdida 0.9238 Precisión 0.4821\n",
            "Epoch 7 Lote 3950 Pérdida 0.9225 Precisión 0.4824\n",
            "Epoch 7 Lote 4000 Pérdida 0.9212 Precisión 0.4827\n",
            "Epoch 7 Lote 4050 Pérdida 0.9198 Precisión 0.4831\n",
            "Epoch 7 Lote 4100 Pérdida 0.9183 Precisión 0.4835\n",
            "Epoch 7 Lote 4150 Pérdida 0.9169 Precisión 0.4838\n",
            "Epoch 7 Lote 4200 Pérdida 0.9156 Precisión 0.4842\n",
            "Epoch 7 Lote 4250 Pérdida 0.9144 Precisión 0.4847\n",
            "Epoch 7 Lote 4300 Pérdida 0.9131 Precisión 0.4851\n",
            "Epoch 7 Lote 4350 Pérdida 0.9118 Precisión 0.4854\n",
            "Epoch 7 Lote 4400 Pérdida 0.9105 Precisión 0.4858\n",
            "Epoch 7 Lote 4450 Pérdida 0.9092 Precisión 0.4861\n",
            "Epoch 7 Lote 4500 Pérdida 0.9080 Precisión 0.4866\n",
            "Epoch 7 Lote 4550 Pérdida 0.9068 Precisión 0.4870\n",
            "Epoch 7 Lote 4600 Pérdida 0.9058 Precisión 0.4874\n",
            "Epoch 7 Lote 4650 Pérdida 0.9048 Precisión 0.4877\n",
            "Epoch 7 Lote 4700 Pérdida 0.9047 Precisión 0.4879\n",
            "Epoch 7 Lote 4750 Pérdida 0.9050 Precisión 0.4880\n",
            "Epoch 7 Lote 4800 Pérdida 0.9058 Precisión 0.4880\n",
            "Epoch 7 Lote 4850 Pérdida 0.9067 Precisión 0.4880\n",
            "Epoch 7 Lote 4900 Pérdida 0.9084 Precisión 0.4878\n",
            "Epoch 7 Lote 4950 Pérdida 0.9099 Precisión 0.4877\n",
            "Epoch 7 Lote 5000 Pérdida 0.9115 Precisión 0.4875\n",
            "Epoch 7 Lote 5050 Pérdida 0.9134 Precisión 0.4873\n",
            "Epoch 7 Lote 5100 Pérdida 0.9152 Precisión 0.4871\n",
            "Epoch 7 Lote 5150 Pérdida 0.9173 Precisión 0.4868\n",
            "Epoch 7 Lote 5200 Pérdida 0.9196 Precisión 0.4865\n",
            "Epoch 7 Lote 5250 Pérdida 0.9216 Precisión 0.4862\n",
            "Epoch 7 Lote 5300 Pérdida 0.9239 Precisión 0.4859\n",
            "Epoch 7 Lote 5350 Pérdida 0.9259 Precisión 0.4856\n",
            "Epoch 7 Lote 5400 Pérdida 0.9280 Precisión 0.4853\n",
            "Epoch 7 Lote 5450 Pérdida 0.9301 Precisión 0.4851\n",
            "Epoch 7 Lote 5500 Pérdida 0.9321 Precisión 0.4848\n",
            "Epoch 7 Lote 5550 Pérdida 0.9343 Precisión 0.4845\n",
            "Epoch 7 Lote 5600 Pérdida 0.9362 Precisión 0.4843\n",
            "Epoch 7 Lote 5650 Pérdida 0.9381 Precisión 0.4840\n",
            "Epoch 7 Lote 5700 Pérdida 0.9402 Precisión 0.4837\n",
            "Epoch 7 Lote 5750 Pérdida 0.9423 Precisión 0.4834\n",
            "Epoch 7 Lote 5800 Pérdida 0.9446 Precisión 0.4831\n",
            "Epoch 7 Lote 5850 Pérdida 0.9463 Precisión 0.4827\n",
            "Epoch 7 Lote 5900 Pérdida 0.9484 Precisión 0.4824\n",
            "Epoch 7 Lote 5950 Pérdida 0.9502 Precisión 0.4821\n",
            "Epoch 7 Lote 6000 Pérdida 0.9521 Precisión 0.4816\n",
            "Epoch 7 Lote 6050 Pérdida 0.9536 Precisión 0.4813\n",
            "Epoch 7 Lote 6100 Pérdida 0.9553 Precisión 0.4810\n",
            "Epoch 7 Lote 6150 Pérdida 0.9569 Precisión 0.4807\n",
            "Epoch 7 Lote 6200 Pérdida 0.9584 Precisión 0.4804\n",
            "Epoch 7 Lote 6250 Pérdida 0.9598 Precisión 0.4801\n",
            "Epoch 7 Lote 6300 Pérdida 0.9613 Precisión 0.4799\n",
            "Epoch 7 Lote 6350 Pérdida 0.9628 Precisión 0.4796\n",
            "Epoch 7 Lote 6400 Pérdida 0.9640 Precisión 0.4794\n",
            "Guardando checkpoint para el epoch 7 en ./drive/My Drive/Curso de NLP/Transformer/ckpt/ckpt-7\n",
            "Tiempo que ha tardado 1 epoch: 1587.6691868305206 segs\n",
            "\n",
            "Inicio del epoch 8\n",
            "Epoch 8 Lote 0 Pérdida 1.1441 Precisión 0.4786\n",
            "Epoch 8 Lote 50 Pérdida 1.1260 Precisión 0.4545\n",
            "Epoch 8 Lote 100 Pérdida 1.1411 Precisión 0.4527\n",
            "Epoch 8 Lote 150 Pérdida 1.1424 Precisión 0.4529\n",
            "Epoch 8 Lote 200 Pérdida 1.1385 Precisión 0.4521\n",
            "Epoch 8 Lote 250 Pérdida 1.1377 Precisión 0.4521\n",
            "Epoch 8 Lote 300 Pérdida 1.1389 Precisión 0.4525\n",
            "Epoch 8 Lote 350 Pérdida 1.1361 Precisión 0.4526\n",
            "Epoch 8 Lote 400 Pérdida 1.1288 Precisión 0.4526\n",
            "Epoch 8 Lote 450 Pérdida 1.1257 Precisión 0.4529\n",
            "Epoch 8 Lote 500 Pérdida 1.1228 Precisión 0.4530\n",
            "Epoch 8 Lote 550 Pérdida 1.1205 Precisión 0.4529\n",
            "Epoch 8 Lote 600 Pérdida 1.1193 Precisión 0.4531\n",
            "Epoch 8 Lote 650 Pérdida 1.1187 Precisión 0.4527\n",
            "Epoch 8 Lote 700 Pérdida 1.1189 Precisión 0.4529\n",
            "Epoch 8 Lote 750 Pérdida 1.1155 Precisión 0.4533\n",
            "Epoch 8 Lote 800 Pérdida 1.1118 Precisión 0.4542\n",
            "Epoch 8 Lote 850 Pérdida 1.1068 Precisión 0.4553\n",
            "Epoch 8 Lote 900 Pérdida 1.1006 Precisión 0.4566\n",
            "Epoch 8 Lote 950 Pérdida 1.0939 Precisión 0.4580\n",
            "Epoch 8 Lote 1000 Pérdida 1.0877 Precisión 0.4593\n",
            "Epoch 8 Lote 1050 Pérdida 1.0821 Precisión 0.4604\n",
            "Epoch 8 Lote 1100 Pérdida 1.0763 Precisión 0.4610\n",
            "Epoch 8 Lote 1150 Pérdida 1.0706 Precisión 0.4615\n",
            "Epoch 8 Lote 1200 Pérdida 1.0651 Precisión 0.4622\n",
            "Epoch 8 Lote 1250 Pérdida 1.0581 Precisión 0.4629\n",
            "Epoch 8 Lote 1300 Pérdida 1.0522 Precisión 0.4636\n",
            "Epoch 8 Lote 1350 Pérdida 1.0466 Precisión 0.4644\n",
            "Epoch 8 Lote 1400 Pérdida 1.0414 Precisión 0.4653\n",
            "Epoch 8 Lote 1450 Pérdida 1.0366 Precisión 0.4663\n",
            "Epoch 8 Lote 1500 Pérdida 1.0317 Precisión 0.4671\n",
            "Epoch 8 Lote 1550 Pérdida 1.0271 Precisión 0.4679\n",
            "Epoch 8 Lote 1600 Pérdida 1.0220 Precisión 0.4686\n",
            "Epoch 8 Lote 1650 Pérdida 1.0170 Precisión 0.4694\n",
            "Epoch 8 Lote 1700 Pérdida 1.0125 Precisión 0.4700\n",
            "Epoch 8 Lote 1750 Pérdida 1.0084 Precisión 0.4705\n",
            "Epoch 8 Lote 1800 Pérdida 1.0037 Precisión 0.4709\n",
            "Epoch 8 Lote 1850 Pérdida 0.9992 Precisión 0.4714\n",
            "Epoch 8 Lote 1900 Pérdida 0.9953 Precisión 0.4716\n",
            "Epoch 8 Lote 1950 Pérdida 0.9911 Precisión 0.4721\n",
            "Epoch 8 Lote 2000 Pérdida 0.9870 Precisión 0.4726\n",
            "Epoch 8 Lote 2050 Pérdida 0.9830 Precisión 0.4728\n",
            "Epoch 8 Lote 2100 Pérdida 0.9791 Precisión 0.4731\n",
            "Epoch 8 Lote 2150 Pérdida 0.9755 Precisión 0.4736\n",
            "Epoch 8 Lote 2200 Pérdida 0.9720 Precisión 0.4741\n",
            "Epoch 8 Lote 2250 Pérdida 0.9686 Precisión 0.4746\n",
            "Epoch 8 Lote 2300 Pérdida 0.9653 Precisión 0.4750\n",
            "Epoch 8 Lote 2350 Pérdida 0.9621 Precisión 0.4753\n",
            "Epoch 8 Lote 2400 Pérdida 0.9588 Precisión 0.4757\n",
            "Epoch 8 Lote 2450 Pérdida 0.9555 Precisión 0.4761\n",
            "Epoch 8 Lote 2500 Pérdida 0.9520 Precisión 0.4765\n",
            "Epoch 8 Lote 2550 Pérdida 0.9483 Precisión 0.4769\n",
            "Epoch 8 Lote 2600 Pérdida 0.9448 Precisión 0.4773\n",
            "Epoch 8 Lote 2650 Pérdida 0.9423 Precisión 0.4776\n",
            "Epoch 8 Lote 2700 Pérdida 0.9396 Precisión 0.4779\n",
            "Epoch 8 Lote 2750 Pérdida 0.9375 Precisión 0.4782\n",
            "Epoch 8 Lote 2800 Pérdida 0.9350 Precisión 0.4784\n",
            "Epoch 8 Lote 2850 Pérdida 0.9332 Precisión 0.4787\n",
            "Epoch 8 Lote 2900 Pérdida 0.9313 Precisión 0.4790\n",
            "Epoch 8 Lote 2950 Pérdida 0.9294 Precisión 0.4794\n",
            "Epoch 8 Lote 3000 Pérdida 0.9278 Precisión 0.4797\n",
            "Epoch 8 Lote 3050 Pérdida 0.9262 Precisión 0.4800\n",
            "Epoch 8 Lote 3100 Pérdida 0.9244 Precisión 0.4804\n",
            "Epoch 8 Lote 3150 Pérdida 0.9229 Precisión 0.4808\n",
            "Epoch 8 Lote 3200 Pérdida 0.9216 Precisión 0.4812\n",
            "Epoch 8 Lote 3250 Pérdida 0.9200 Precisión 0.4815\n",
            "Epoch 8 Lote 3300 Pérdida 0.9187 Precisión 0.4819\n",
            "Epoch 8 Lote 3350 Pérdida 0.9176 Precisión 0.4821\n",
            "Epoch 8 Lote 3400 Pérdida 0.9161 Precisión 0.4824\n",
            "Epoch 8 Lote 3450 Pérdida 0.9148 Precisión 0.4828\n",
            "Epoch 8 Lote 3500 Pérdida 0.9139 Precisión 0.4831\n",
            "Epoch 8 Lote 3550 Pérdida 0.9127 Precisión 0.4835\n",
            "Epoch 8 Lote 3600 Pérdida 0.9111 Precisión 0.4838\n",
            "Epoch 8 Lote 3650 Pérdida 0.9100 Precisión 0.4841\n",
            "Epoch 8 Lote 3700 Pérdida 0.9086 Precisión 0.4844\n",
            "Epoch 8 Lote 3750 Pérdida 0.9075 Precisión 0.4847\n",
            "Epoch 8 Lote 3800 Pérdida 0.9062 Precisión 0.4850\n",
            "Epoch 8 Lote 3850 Pérdida 0.9050 Precisión 0.4853\n",
            "Epoch 8 Lote 3900 Pérdida 0.9039 Precisión 0.4856\n",
            "Epoch 8 Lote 3950 Pérdida 0.9025 Precisión 0.4859\n",
            "Epoch 8 Lote 4000 Pérdida 0.9011 Precisión 0.4863\n",
            "Epoch 8 Lote 4050 Pérdida 0.8999 Precisión 0.4867\n",
            "Epoch 8 Lote 4100 Pérdida 0.8985 Precisión 0.4870\n",
            "Epoch 8 Lote 4150 Pérdida 0.8970 Precisión 0.4875\n",
            "Epoch 8 Lote 4200 Pérdida 0.8956 Precisión 0.4879\n",
            "Epoch 8 Lote 4250 Pérdida 0.8941 Precisión 0.4883\n",
            "Epoch 8 Lote 4300 Pérdida 0.8929 Precisión 0.4887\n",
            "Epoch 8 Lote 4350 Pérdida 0.8917 Precisión 0.4891\n",
            "Epoch 8 Lote 4400 Pérdida 0.8905 Precisión 0.4894\n",
            "Epoch 8 Lote 4450 Pérdida 0.8890 Precisión 0.4898\n",
            "Epoch 8 Lote 4500 Pérdida 0.8878 Precisión 0.4902\n",
            "Epoch 8 Lote 4550 Pérdida 0.8867 Precisión 0.4905\n",
            "Epoch 8 Lote 4600 Pérdida 0.8856 Precisión 0.4909\n",
            "Epoch 8 Lote 4650 Pérdida 0.8848 Precisión 0.4912\n",
            "Epoch 8 Lote 4700 Pérdida 0.8844 Precisión 0.4914\n",
            "Epoch 8 Lote 4750 Pérdida 0.8846 Precisión 0.4914\n",
            "Epoch 8 Lote 4800 Pérdida 0.8852 Precisión 0.4914\n",
            "Epoch 8 Lote 4850 Pérdida 0.8862 Precisión 0.4914\n",
            "Epoch 8 Lote 4900 Pérdida 0.8877 Precisión 0.4913\n",
            "Epoch 8 Lote 4950 Pérdida 0.8893 Precisión 0.4911\n",
            "Epoch 8 Lote 5000 Pérdida 0.8912 Precisión 0.4908\n",
            "Epoch 8 Lote 5050 Pérdida 0.8931 Precisión 0.4906\n",
            "Epoch 8 Lote 5100 Pérdida 0.8951 Precisión 0.4903\n",
            "Epoch 8 Lote 5150 Pérdida 0.8972 Precisión 0.4901\n",
            "Epoch 8 Lote 5200 Pérdida 0.8992 Precisión 0.4898\n",
            "Epoch 8 Lote 5250 Pérdida 0.9014 Precisión 0.4895\n",
            "Epoch 8 Lote 5300 Pérdida 0.9035 Precisión 0.4892\n",
            "Epoch 8 Lote 5350 Pérdida 0.9058 Precisión 0.4889\n",
            "Epoch 8 Lote 5400 Pérdida 0.9079 Precisión 0.4886\n",
            "Epoch 8 Lote 5450 Pérdida 0.9101 Precisión 0.4884\n",
            "Epoch 8 Lote 5500 Pérdida 0.9122 Precisión 0.4881\n",
            "Epoch 8 Lote 5550 Pérdida 0.9141 Precisión 0.4878\n",
            "Epoch 8 Lote 5600 Pérdida 0.9160 Precisión 0.4875\n",
            "Epoch 8 Lote 5650 Pérdida 0.9178 Precisión 0.4872\n",
            "Epoch 8 Lote 5700 Pérdida 0.9199 Precisión 0.4869\n",
            "Epoch 8 Lote 5750 Pérdida 0.9219 Precisión 0.4867\n",
            "Epoch 8 Lote 5800 Pérdida 0.9240 Precisión 0.4864\n",
            "Epoch 8 Lote 5850 Pérdida 0.9260 Precisión 0.4860\n",
            "Epoch 8 Lote 5900 Pérdida 0.9280 Precisión 0.4857\n",
            "Epoch 8 Lote 5950 Pérdida 0.9297 Precisión 0.4853\n",
            "Epoch 8 Lote 6000 Pérdida 0.9317 Precisión 0.4850\n",
            "Epoch 8 Lote 6050 Pérdida 0.9333 Precisión 0.4847\n",
            "Epoch 8 Lote 6100 Pérdida 0.9349 Precisión 0.4844\n",
            "Epoch 8 Lote 6150 Pérdida 0.9366 Precisión 0.4840\n",
            "Epoch 8 Lote 6200 Pérdida 0.9380 Precisión 0.4838\n",
            "Epoch 8 Lote 6250 Pérdida 0.9396 Precisión 0.4835\n",
            "Epoch 8 Lote 6300 Pérdida 0.9412 Precisión 0.4832\n",
            "Epoch 8 Lote 6350 Pérdida 0.9427 Precisión 0.4829\n",
            "Epoch 8 Lote 6400 Pérdida 0.9441 Precisión 0.4826\n",
            "Guardando checkpoint para el epoch 8 en ./drive/My Drive/Curso de NLP/Transformer/ckpt/ckpt-8\n",
            "Tiempo que ha tardado 1 epoch: 1596.9727466106415 segs\n",
            "\n",
            "Inicio del epoch 9\n",
            "Epoch 9 Lote 0 Pérdida 1.1461 Precisión 0.4918\n",
            "Epoch 9 Lote 50 Pérdida 1.1300 Precisión 0.4559\n",
            "Epoch 9 Lote 100 Pérdida 1.1237 Precisión 0.4566\n",
            "Epoch 9 Lote 150 Pérdida 1.1208 Precisión 0.4559\n",
            "Epoch 9 Lote 200 Pérdida 1.1144 Precisión 0.4564\n",
            "Epoch 9 Lote 250 Pérdida 1.1202 Precisión 0.4561\n",
            "Epoch 9 Lote 300 Pérdida 1.1205 Precisión 0.4555\n",
            "Epoch 9 Lote 350 Pérdida 1.1178 Precisión 0.4558\n",
            "Epoch 9 Lote 400 Pérdida 1.1115 Precisión 0.4553\n",
            "Epoch 9 Lote 450 Pérdida 1.1108 Precisión 0.4553\n",
            "Epoch 9 Lote 500 Pérdida 1.1059 Precisión 0.4555\n",
            "Epoch 9 Lote 550 Pérdida 1.1054 Precisión 0.4552\n",
            "Epoch 9 Lote 600 Pérdida 1.1044 Precisión 0.4554\n",
            "Epoch 9 Lote 650 Pérdida 1.1039 Precisión 0.4548\n",
            "Epoch 9 Lote 700 Pérdida 1.1041 Precisión 0.4552\n",
            "Epoch 9 Lote 750 Pérdida 1.1015 Precisión 0.4557\n",
            "Epoch 9 Lote 800 Pérdida 1.0966 Precisión 0.4569\n",
            "Epoch 9 Lote 850 Pérdida 1.0900 Precisión 0.4580\n",
            "Epoch 9 Lote 900 Pérdida 1.0846 Precisión 0.4594\n",
            "Epoch 9 Lote 950 Pérdida 1.0776 Precisión 0.4603\n",
            "Epoch 9 Lote 1000 Pérdida 1.0701 Precisión 0.4616\n",
            "Epoch 9 Lote 1050 Pérdida 1.0647 Precisión 0.4623\n",
            "Epoch 9 Lote 1100 Pérdida 1.0576 Precisión 0.4630\n",
            "Epoch 9 Lote 1150 Pérdida 1.0511 Precisión 0.4637\n",
            "Epoch 9 Lote 1200 Pérdida 1.0456 Precisión 0.4647\n",
            "Epoch 9 Lote 1250 Pérdida 1.0400 Precisión 0.4654\n",
            "Epoch 9 Lote 1300 Pérdida 1.0336 Precisión 0.4663\n",
            "Epoch 9 Lote 1350 Pérdida 1.0276 Precisión 0.4671\n",
            "Epoch 9 Lote 1400 Pérdida 1.0226 Precisión 0.4681\n",
            "Epoch 9 Lote 1450 Pérdida 1.0181 Precisión 0.4688\n",
            "Epoch 9 Lote 1500 Pérdida 1.0133 Precisión 0.4695\n",
            "Epoch 9 Lote 1550 Pérdida 1.0079 Precisión 0.4704\n",
            "Epoch 9 Lote 1600 Pérdida 1.0034 Precisión 0.4711\n",
            "Epoch 9 Lote 1650 Pérdida 0.9992 Precisión 0.4718\n",
            "Epoch 9 Lote 1700 Pérdida 0.9946 Precisión 0.4725\n",
            "Epoch 9 Lote 1750 Pérdida 0.9897 Precisión 0.4732\n",
            "Epoch 9 Lote 1800 Pérdida 0.9858 Precisión 0.4736\n",
            "Epoch 9 Lote 1850 Pérdida 0.9816 Precisión 0.4741\n",
            "Epoch 9 Lote 1900 Pérdida 0.9770 Precisión 0.4744\n",
            "Epoch 9 Lote 1950 Pérdida 0.9733 Precisión 0.4748\n",
            "Epoch 9 Lote 2000 Pérdida 0.9699 Precisión 0.4751\n",
            "Epoch 9 Lote 2050 Pérdida 0.9661 Precisión 0.4755\n",
            "Epoch 9 Lote 2100 Pérdida 0.9623 Precisión 0.4760\n",
            "Epoch 9 Lote 2150 Pérdida 0.9592 Precisión 0.4764\n",
            "Epoch 9 Lote 2200 Pérdida 0.9555 Precisión 0.4768\n",
            "Epoch 9 Lote 2250 Pérdida 0.9518 Precisión 0.4773\n",
            "Epoch 9 Lote 2300 Pérdida 0.9486 Precisión 0.4777\n",
            "Epoch 9 Lote 2350 Pérdida 0.9455 Precisión 0.4781\n",
            "Epoch 9 Lote 2400 Pérdida 0.9421 Precisión 0.4784\n",
            "Epoch 9 Lote 2450 Pérdida 0.9391 Precisión 0.4789\n",
            "Epoch 9 Lote 2500 Pérdida 0.9356 Precisión 0.4792\n",
            "Epoch 9 Lote 2550 Pérdida 0.9321 Precisión 0.4796\n",
            "Epoch 9 Lote 2600 Pérdida 0.9285 Precisión 0.4800\n",
            "Epoch 9 Lote 2650 Pérdida 0.9257 Precisión 0.4803\n",
            "Epoch 9 Lote 2700 Pérdida 0.9232 Precisión 0.4807\n",
            "Epoch 9 Lote 2750 Pérdida 0.9211 Precisión 0.4809\n",
            "Epoch 9 Lote 2800 Pérdida 0.9188 Precisión 0.4813\n",
            "Epoch 9 Lote 2850 Pérdida 0.9165 Precisión 0.4815\n",
            "Epoch 9 Lote 2900 Pérdida 0.9145 Precisión 0.4818\n",
            "Epoch 9 Lote 2950 Pérdida 0.9123 Precisión 0.4821\n",
            "Epoch 9 Lote 3000 Pérdida 0.9105 Precisión 0.4824\n",
            "Epoch 9 Lote 3050 Pérdida 0.9089 Precisión 0.4828\n",
            "Epoch 9 Lote 3100 Pérdida 0.9067 Precisión 0.4832\n",
            "Epoch 9 Lote 3150 Pérdida 0.9055 Precisión 0.4835\n",
            "Epoch 9 Lote 3200 Pérdida 0.9043 Precisión 0.4838\n",
            "Epoch 9 Lote 3250 Pérdida 0.9027 Precisión 0.4840\n",
            "Epoch 9 Lote 3300 Pérdida 0.9017 Precisión 0.4845\n",
            "Epoch 9 Lote 3350 Pérdida 0.9006 Precisión 0.4848\n",
            "Epoch 9 Lote 3400 Pérdida 0.8997 Precisión 0.4851\n",
            "Epoch 9 Lote 3450 Pérdida 0.8987 Precisión 0.4855\n",
            "Epoch 9 Lote 3500 Pérdida 0.8975 Precisión 0.4859\n",
            "Epoch 9 Lote 3550 Pérdida 0.8961 Precisión 0.4861\n",
            "Epoch 9 Lote 3600 Pérdida 0.8949 Precisión 0.4863\n",
            "Epoch 9 Lote 3650 Pérdida 0.8934 Precisión 0.4867\n",
            "Epoch 9 Lote 3700 Pérdida 0.8920 Precisión 0.4869\n",
            "Epoch 9 Lote 3750 Pérdida 0.8906 Precisión 0.4872\n",
            "Epoch 9 Lote 3800 Pérdida 0.8893 Precisión 0.4875\n",
            "Epoch 9 Lote 3850 Pérdida 0.8880 Precisión 0.4878\n",
            "Epoch 9 Lote 3900 Pérdida 0.8866 Precisión 0.4882\n",
            "Epoch 9 Lote 3950 Pérdida 0.8854 Precisión 0.4885\n",
            "Epoch 9 Lote 4000 Pérdida 0.8842 Precisión 0.4889\n",
            "Epoch 9 Lote 4050 Pérdida 0.8830 Precisión 0.4892\n",
            "Epoch 9 Lote 4100 Pérdida 0.8818 Precisión 0.4896\n",
            "Epoch 9 Lote 4150 Pérdida 0.8802 Precisión 0.4900\n",
            "Epoch 9 Lote 4200 Pérdida 0.8788 Precisión 0.4904\n",
            "Epoch 9 Lote 4250 Pérdida 0.8773 Precisión 0.4909\n",
            "Epoch 9 Lote 4300 Pérdida 0.8761 Precisión 0.4912\n",
            "Epoch 9 Lote 4350 Pérdida 0.8750 Precisión 0.4916\n",
            "Epoch 9 Lote 4400 Pérdida 0.8737 Precisión 0.4921\n",
            "Epoch 9 Lote 4450 Pérdida 0.8727 Precisión 0.4924\n",
            "Epoch 9 Lote 4500 Pérdida 0.8715 Precisión 0.4928\n",
            "Epoch 9 Lote 4550 Pérdida 0.8704 Precisión 0.4931\n",
            "Epoch 9 Lote 4600 Pérdida 0.8692 Precisión 0.4935\n",
            "Epoch 9 Lote 4650 Pérdida 0.8682 Precisión 0.4938\n",
            "Epoch 9 Lote 4700 Pérdida 0.8678 Precisión 0.4940\n",
            "Epoch 9 Lote 4750 Pérdida 0.8680 Precisión 0.4940\n",
            "Epoch 9 Lote 4800 Pérdida 0.8687 Precisión 0.4940\n",
            "Epoch 9 Lote 4850 Pérdida 0.8701 Precisión 0.4939\n",
            "Epoch 9 Lote 4900 Pérdida 0.8717 Precisión 0.4938\n",
            "Epoch 9 Lote 4950 Pérdida 0.8733 Precisión 0.4937\n",
            "Epoch 9 Lote 5000 Pérdida 0.8750 Precisión 0.4935\n",
            "Epoch 9 Lote 5050 Pérdida 0.8768 Precisión 0.4933\n",
            "Epoch 9 Lote 5100 Pérdida 0.8790 Precisión 0.4930\n",
            "Epoch 9 Lote 5150 Pérdida 0.8808 Precisión 0.4927\n",
            "Epoch 9 Lote 5200 Pérdida 0.8828 Precisión 0.4925\n",
            "Epoch 9 Lote 5250 Pérdida 0.8850 Precisión 0.4922\n",
            "Epoch 9 Lote 5300 Pérdida 0.8873 Precisión 0.4919\n",
            "Epoch 9 Lote 5350 Pérdida 0.8897 Precisión 0.4916\n",
            "Epoch 9 Lote 5400 Pérdida 0.8920 Precisión 0.4913\n",
            "Epoch 9 Lote 5450 Pérdida 0.8939 Precisión 0.4910\n",
            "Epoch 9 Lote 5500 Pérdida 0.8960 Precisión 0.4908\n",
            "Epoch 9 Lote 5550 Pérdida 0.8980 Precisión 0.4905\n",
            "Epoch 9 Lote 5600 Pérdida 0.9003 Precisión 0.4902\n",
            "Epoch 9 Lote 5650 Pérdida 0.9024 Precisión 0.4899\n",
            "Epoch 9 Lote 5700 Pérdida 0.9044 Precisión 0.4896\n",
            "Epoch 9 Lote 5750 Pérdida 0.9064 Precisión 0.4893\n",
            "Epoch 9 Lote 5800 Pérdida 0.9082 Precisión 0.4890\n",
            "Epoch 9 Lote 5850 Pérdida 0.9102 Precisión 0.4887\n",
            "Epoch 9 Lote 5900 Pérdida 0.9121 Precisión 0.4884\n",
            "Epoch 9 Lote 5950 Pérdida 0.9140 Precisión 0.4880\n",
            "Epoch 9 Lote 6000 Pérdida 0.9158 Precisión 0.4876\n",
            "Epoch 9 Lote 6050 Pérdida 0.9177 Precisión 0.4873\n",
            "Epoch 9 Lote 6100 Pérdida 0.9194 Precisión 0.4870\n",
            "Epoch 9 Lote 6150 Pérdida 0.9209 Precisión 0.4868\n",
            "Epoch 9 Lote 6200 Pérdida 0.9224 Precisión 0.4865\n",
            "Epoch 9 Lote 6250 Pérdida 0.9238 Precisión 0.4861\n",
            "Epoch 9 Lote 6300 Pérdida 0.9254 Precisión 0.4858\n",
            "Epoch 9 Lote 6350 Pérdida 0.9270 Precisión 0.4855\n",
            "Epoch 9 Lote 6400 Pérdida 0.9282 Precisión 0.4852\n",
            "Guardando checkpoint para el epoch 9 en ./drive/My Drive/Curso de NLP/Transformer/ckpt/ckpt-9\n",
            "Tiempo que ha tardado 1 epoch: 1600.5086560249329 segs\n",
            "\n",
            "Inicio del epoch 10\n",
            "Epoch 10 Lote 0 Pérdida 1.0560 Precisión 0.4844\n",
            "Epoch 10 Lote 50 Pérdida 1.1411 Precisión 0.4537\n",
            "Epoch 10 Lote 100 Pérdida 1.1236 Precisión 0.4542\n",
            "Epoch 10 Lote 150 Pérdida 1.1083 Precisión 0.4565\n",
            "Epoch 10 Lote 200 Pérdida 1.1112 Precisión 0.4565\n",
            "Epoch 10 Lote 250 Pérdida 1.1082 Precisión 0.4562\n",
            "Epoch 10 Lote 300 Pérdida 1.1054 Precisión 0.4573\n",
            "Epoch 10 Lote 350 Pérdida 1.1002 Precisión 0.4573\n",
            "Epoch 10 Lote 400 Pérdida 1.0985 Precisión 0.4578\n",
            "Epoch 10 Lote 450 Pérdida 1.0959 Precisión 0.4577\n",
            "Epoch 10 Lote 500 Pérdida 1.0934 Precisión 0.4579\n",
            "Epoch 10 Lote 550 Pérdida 1.0909 Precisión 0.4573\n",
            "Epoch 10 Lote 600 Pérdida 1.0891 Precisión 0.4571\n",
            "Epoch 10 Lote 650 Pérdida 1.0904 Precisión 0.4570\n",
            "Epoch 10 Lote 700 Pérdida 1.0868 Precisión 0.4573\n",
            "Epoch 10 Lote 750 Pérdida 1.0838 Precisión 0.4580\n",
            "Epoch 10 Lote 800 Pérdida 1.0800 Precisión 0.4588\n",
            "Epoch 10 Lote 850 Pérdida 1.0755 Precisión 0.4601\n",
            "Epoch 10 Lote 900 Pérdida 1.0704 Precisión 0.4612\n",
            "Epoch 10 Lote 950 Pérdida 1.0621 Precisión 0.4626\n",
            "Epoch 10 Lote 1000 Pérdida 1.0562 Precisión 0.4638\n",
            "Epoch 10 Lote 1050 Pérdida 1.0486 Precisión 0.4647\n",
            "Epoch 10 Lote 1100 Pérdida 1.0431 Precisión 0.4657\n",
            "Epoch 10 Lote 1150 Pérdida 1.0371 Precisión 0.4663\n",
            "Epoch 10 Lote 1200 Pérdida 1.0323 Precisión 0.4670\n",
            "Epoch 10 Lote 1250 Pérdida 1.0264 Precisión 0.4677\n",
            "Epoch 10 Lote 1300 Pérdida 1.0204 Precisión 0.4685\n",
            "Epoch 10 Lote 1350 Pérdida 1.0136 Precisión 0.4694\n",
            "Epoch 10 Lote 1400 Pérdida 1.0082 Precisión 0.4702\n",
            "Epoch 10 Lote 1450 Pérdida 1.0025 Precisión 0.4711\n",
            "Epoch 10 Lote 1500 Pérdida 0.9978 Precisión 0.4718\n",
            "Epoch 10 Lote 1550 Pérdida 0.9922 Precisión 0.4724\n",
            "Epoch 10 Lote 1600 Pérdida 0.9878 Precisión 0.4734\n",
            "Epoch 10 Lote 1650 Pérdida 0.9835 Precisión 0.4741\n",
            "Epoch 10 Lote 1700 Pérdida 0.9790 Precisión 0.4749\n",
            "Epoch 10 Lote 1750 Pérdida 0.9749 Precisión 0.4755\n",
            "Epoch 10 Lote 1800 Pérdida 0.9706 Precisión 0.4761\n",
            "Epoch 10 Lote 1850 Pérdida 0.9668 Precisión 0.4765\n",
            "Epoch 10 Lote 1900 Pérdida 0.9629 Precisión 0.4769\n",
            "Epoch 10 Lote 1950 Pérdida 0.9590 Precisión 0.4772\n",
            "Epoch 10 Lote 2000 Pérdida 0.9553 Precisión 0.4777\n",
            "Epoch 10 Lote 2050 Pérdida 0.9515 Precisión 0.4781\n",
            "Epoch 10 Lote 2100 Pérdida 0.9477 Precisión 0.4786\n",
            "Epoch 10 Lote 2150 Pérdida 0.9441 Precisión 0.4790\n",
            "Epoch 10 Lote 2200 Pérdida 0.9408 Precisión 0.4795\n",
            "Epoch 10 Lote 2250 Pérdida 0.9374 Precisión 0.4797\n",
            "Epoch 10 Lote 2300 Pérdida 0.9343 Precisión 0.4802\n",
            "Epoch 10 Lote 2350 Pérdida 0.9304 Precisión 0.4806\n",
            "Epoch 10 Lote 2400 Pérdida 0.9270 Precisión 0.4809\n",
            "Epoch 10 Lote 2450 Pérdida 0.9235 Precisión 0.4813\n",
            "Epoch 10 Lote 2500 Pérdida 0.9203 Precisión 0.4816\n",
            "Epoch 10 Lote 2550 Pérdida 0.9170 Precisión 0.4819\n",
            "Epoch 10 Lote 2600 Pérdida 0.9139 Precisión 0.4821\n",
            "Epoch 10 Lote 2650 Pérdida 0.9111 Precisión 0.4825\n",
            "Epoch 10 Lote 2700 Pérdida 0.9091 Precisión 0.4828\n",
            "Epoch 10 Lote 2750 Pérdida 0.9066 Precisión 0.4832\n",
            "Epoch 10 Lote 2800 Pérdida 0.9044 Precisión 0.4835\n",
            "Epoch 10 Lote 2850 Pérdida 0.9024 Precisión 0.4840\n",
            "Epoch 10 Lote 2900 Pérdida 0.9005 Precisión 0.4843\n",
            "Epoch 10 Lote 2950 Pérdida 0.8987 Precisión 0.4846\n",
            "Epoch 10 Lote 3000 Pérdida 0.8970 Precisión 0.4849\n",
            "Epoch 10 Lote 3050 Pérdida 0.8955 Precisión 0.4852\n",
            "Epoch 10 Lote 3100 Pérdida 0.8940 Precisión 0.4855\n",
            "Epoch 10 Lote 3150 Pérdida 0.8928 Precisión 0.4858\n",
            "Epoch 10 Lote 3200 Pérdida 0.8914 Precisión 0.4861\n",
            "Epoch 10 Lote 3250 Pérdida 0.8897 Precisión 0.4864\n",
            "Epoch 10 Lote 3300 Pérdida 0.8881 Precisión 0.4868\n",
            "Epoch 10 Lote 3350 Pérdida 0.8866 Precisión 0.4871\n",
            "Epoch 10 Lote 3400 Pérdida 0.8856 Precisión 0.4873\n",
            "Epoch 10 Lote 3450 Pérdida 0.8843 Precisión 0.4877\n",
            "Epoch 10 Lote 3500 Pérdida 0.8833 Precisión 0.4881\n",
            "Epoch 10 Lote 3550 Pérdida 0.8820 Precisión 0.4883\n",
            "Epoch 10 Lote 3600 Pérdida 0.8807 Precisión 0.4886\n",
            "Epoch 10 Lote 3650 Pérdida 0.8792 Precisión 0.4889\n",
            "Epoch 10 Lote 3700 Pérdida 0.8780 Precisión 0.4892\n",
            "Epoch 10 Lote 3750 Pérdida 0.8767 Precisión 0.4895\n",
            "Epoch 10 Lote 3800 Pérdida 0.8755 Precisión 0.4898\n",
            "Epoch 10 Lote 3850 Pérdida 0.8742 Precisión 0.4901\n",
            "Epoch 10 Lote 3900 Pérdida 0.8728 Precisión 0.4904\n",
            "Epoch 10 Lote 3950 Pérdida 0.8715 Precisión 0.4908\n",
            "Epoch 10 Lote 4000 Pérdida 0.8702 Precisión 0.4912\n",
            "Epoch 10 Lote 4050 Pérdida 0.8691 Precisión 0.4915\n",
            "Epoch 10 Lote 4100 Pérdida 0.8678 Precisión 0.4919\n",
            "Epoch 10 Lote 4150 Pérdida 0.8665 Precisión 0.4923\n",
            "Epoch 10 Lote 4200 Pérdida 0.8647 Precisión 0.4927\n",
            "Epoch 10 Lote 4250 Pérdida 0.8631 Precisión 0.4932\n",
            "Epoch 10 Lote 4300 Pérdida 0.8618 Precisión 0.4935\n",
            "Epoch 10 Lote 4350 Pérdida 0.8609 Precisión 0.4939\n",
            "Epoch 10 Lote 4400 Pérdida 0.8597 Precisión 0.4943\n",
            "Epoch 10 Lote 4450 Pérdida 0.8584 Precisión 0.4946\n",
            "Epoch 10 Lote 4500 Pérdida 0.8572 Precisión 0.4951\n",
            "Epoch 10 Lote 4550 Pérdida 0.8562 Precisión 0.4955\n",
            "Epoch 10 Lote 4600 Pérdida 0.8550 Precisión 0.4959\n",
            "Epoch 10 Lote 4650 Pérdida 0.8542 Precisión 0.4962\n",
            "Epoch 10 Lote 4700 Pérdida 0.8538 Precisión 0.4964\n",
            "Epoch 10 Lote 4750 Pérdida 0.8539 Precisión 0.4965\n",
            "Epoch 10 Lote 4800 Pérdida 0.8545 Precisión 0.4965\n",
            "Epoch 10 Lote 4850 Pérdida 0.8558 Precisión 0.4964\n",
            "Epoch 10 Lote 4900 Pérdida 0.8572 Precisión 0.4963\n",
            "Epoch 10 Lote 4950 Pérdida 0.8589 Precisión 0.4961\n",
            "Epoch 10 Lote 5000 Pérdida 0.8608 Precisión 0.4958\n",
            "Epoch 10 Lote 5050 Pérdida 0.8628 Precisión 0.4956\n",
            "Epoch 10 Lote 5100 Pérdida 0.8650 Precisión 0.4953\n",
            "Epoch 10 Lote 5150 Pérdida 0.8669 Precisión 0.4950\n",
            "Epoch 10 Lote 5200 Pérdida 0.8690 Precisión 0.4948\n",
            "Epoch 10 Lote 5250 Pérdida 0.8710 Precisión 0.4944\n",
            "Epoch 10 Lote 5300 Pérdida 0.8734 Precisión 0.4942\n",
            "Epoch 10 Lote 5350 Pérdida 0.8754 Precisión 0.4938\n",
            "Epoch 10 Lote 5400 Pérdida 0.8777 Precisión 0.4936\n",
            "Epoch 10 Lote 5450 Pérdida 0.8798 Precisión 0.4933\n",
            "Epoch 10 Lote 5500 Pérdida 0.8818 Precisión 0.4930\n",
            "Epoch 10 Lote 5550 Pérdida 0.8839 Precisión 0.4927\n",
            "Epoch 10 Lote 5600 Pérdida 0.8857 Precisión 0.4924\n",
            "Epoch 10 Lote 5650 Pérdida 0.8878 Precisión 0.4921\n",
            "Epoch 10 Lote 5700 Pérdida 0.8897 Precisión 0.4918\n",
            "Epoch 10 Lote 5750 Pérdida 0.8917 Precisión 0.4915\n",
            "Epoch 10 Lote 5800 Pérdida 0.8937 Precisión 0.4912\n",
            "Epoch 10 Lote 5850 Pérdida 0.8958 Precisión 0.4909\n",
            "Epoch 10 Lote 5900 Pérdida 0.8977 Precisión 0.4905\n",
            "Epoch 10 Lote 5950 Pérdida 0.8996 Precisión 0.4902\n",
            "Epoch 10 Lote 6000 Pérdida 0.9014 Precisión 0.4899\n",
            "Epoch 10 Lote 6050 Pérdida 0.9032 Precisión 0.4895\n",
            "Epoch 10 Lote 6100 Pérdida 0.9048 Precisión 0.4892\n",
            "Epoch 10 Lote 6150 Pérdida 0.9064 Precisión 0.4889\n",
            "Epoch 10 Lote 6200 Pérdida 0.9080 Precisión 0.4886\n",
            "Epoch 10 Lote 6250 Pérdida 0.9096 Precisión 0.4883\n",
            "Epoch 10 Lote 6300 Pérdida 0.9110 Precisión 0.4880\n",
            "Epoch 10 Lote 6350 Pérdida 0.9126 Precisión 0.4877\n",
            "Epoch 10 Lote 6400 Pérdida 0.9140 Precisión 0.4874\n",
            "Guardando checkpoint para el epoch 10 en ./drive/My Drive/Curso de NLP/Transformer/ckpt/ckpt-10\n",
            "Tiempo que ha tardado 1 epoch: 1607.4231536388397 segs\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nmzyRwDrRGdq"
      },
      "source": [
        "# Evaluación"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cNHwJJrz3lPB"
      },
      "source": [
        "# Esta función traduce frases en inglés a tokens en castellano\n",
        "def evaluate(inp_sentence):\n",
        "  # Tokenizamos la frase\n",
        "    inp_sentence = \\\n",
        "        [VOCAB_SIZE_EN-2] + tokenizer_en.encode(inp_sentence) + [VOCAB_SIZE_EN-1]\n",
        "    # añadimos un eje en la posición 0, ie, añadimos uan dimensión a la frase esto porque solo es una frase\n",
        "    # y habíamos estado entrenando con lotes de frases\n",
        "    enc_input = tf.expand_dims(inp_sentence, axis=0)\n",
        "    \n",
        "    # Añadimos una dimensión igualmente al output para que sea un lote de predicciones \n",
        "    output = tf.expand_dims([VOCAB_SIZE_ES-2], axis=0)\n",
        "    \n",
        "    # Habrá que hacer varias iteraciones de nuestro transformer pues recordemos que a cada iteración, solo\n",
        "    # nos devuelve la siguiente palabra. Recordemos que ademas nosotros hemos definido un máximo de longitud\n",
        "    # de la frase = 20 por lo que a lo mucho obtendremos 20 palabras\n",
        "    for _ in range(MAX_LENGTH):\n",
        "        # hacemos la predicción:\n",
        "        predictions = transformer(enc_input, output, False) #(1, seq_length, VOCAB_SIZE_ES)\n",
        "        # De todas las prediccipnes me quedo con predicitions para todos los lotes(:), ie un solo lote porque\n",
        "        # solo hay uno me quedo con la última palabra predicha por el transformer (-1) y me quedo con todas las\n",
        "        # palabras en castellano (:)\n",
        "        prediction = predictions[:, -1:, :]\n",
        "        # Obtenemos el indice de esa palabra más probable de todo VOCAB_SIZE_ES\n",
        "        predicted_id = tf.cast(tf.argmax(prediction, axis=-1), tf.int32)\n",
        "        \n",
        "        # si el tokenizador considera que es final de frase pues hemos finalizado\n",
        "        if predicted_id == VOCAB_SIZE_ES-1:\n",
        "            return tf.squeeze(output, axis=0) # para que el resultado sea un tensor unidimensional\n",
        "        # En otro caso ir agregando las palabras predichas\n",
        "        output = tf.concat([output, predicted_id], axis=-1)\n",
        "        \n",
        "    # por si la frase era demasiodo largo y no vio nunca una palabra de final de frase aun así convertimos\n",
        "    # lo analizado hasta MAX_LENGTH en un vector unidimensional    \n",
        "    return tf.squeeze(output, axis=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s6VeFKrE6Kdx"
      },
      "source": [
        "# Función que construirá ya la frase de salida en español\n",
        "def translate(sentence):\n",
        "    output = evaluate(sentence).numpy()\n",
        "    \n",
        "    # decodificamos los tokens que no son ni de inicio, ni final de frase\n",
        "    predicted_sentence = tokenizer_es.decode(\n",
        "        [i for i in output if i < VOCAB_SIZE_ES-2]\n",
        "    )\n",
        "    \n",
        "    print(\"Entrada: {}\".format(sentence))\n",
        "    print(\"Traducción predicha: {}\".format(predicted_sentence))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BupFjJlgDvCA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "54e7aa51-1cc2-42a5-ede2-304d1a230d08"
      },
      "source": [
        "translate(\"This is a problem we have to solve.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Entrada: This is a problem we have to solve.\n",
            "Traducción predicha: Es un problema que debemos resolver.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZdoWKbCP7Czs",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "dbe5425f-1c91-45a7-87ab-4ed0dbf13d74"
      },
      "source": [
        "translate(\"This is a really powerful tool!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Entrada: This is a really powerful tool!\n",
            "Traducción predicha: ¡Es una hermosa hermosa!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UGjBEb5WFMGt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "a7892348-eaf4-4445-d816-3c587275f436"
      },
      "source": [
        "translate(\"This is an interesting course about Natural Language Processing\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Entrada: This is an interesting course about Natural Language Processing\n",
            "Traducción predicha: Es un documento de trabajo interesante sobre el procedimiento de idioma Naígena\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L1v9g4T0FcO3"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}